{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868508c7",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/newfrogg/Automatic-Differentiation/blob/main/autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6-FUvYwvWEJF",
   "metadata": {
    "id": "6-FUvYwvWEJF"
   },
   "source": [
    "| Mục lớn                             | Mục nhỏ                                                         |           |\n",
    "|-------------------------------------|-----------------------------------------------------------------|-----------|\n",
    "| Introduction                        |                                                                 |  1 người  |\n",
    "|                                     | Differential Calculus                                           |  Tử Quân  |\n",
    "|                                     | Rules of Calculus                                               |  Tử Quân  |\n",
    "|                                     | Multivariate Chain Rule                                         |  Tử Quân  |\n",
    "|                                     | Geometry of Gradients and Gradient Descent                      |  Tử Quân  |\n",
    "| What Autodiff Isn’t                 |                                                                 |  3 người  |\n",
    "|                                     | Autodiff is not finite differences (numerical differentiation)  |  An Đông  |\n",
    "|                                     | Autodiff is not symbolic differentiation                        |  An Đông  |\n",
    "|                                     | What autodiff is ?                                              |  An Đông  |\n",
    "|                                     | Types of Autodiff (explain forward, backward)                   |  An Đông  |\n",
    "|                                     | Backpropagation Algorithm                                       |  Gia Hinh |\n",
    "|                                     | Gradient-Based Optimization                                     |  Gia Hinh |\n",
    "|                                     | give an example and describe the autodiff step-by-step workflow |  Gia Hinh |\n",
    "|                                     | Summary. Compare betwwen each type advancement of each types    |  Gia Hinh |\n",
    "| Visualization. Code, Implementation |                                                                 |  1 người  |\n",
    "|                                     | Code                                                            | Bảo Lương |\n",
    "| Exercise                            |                                                                 |  2 người  |\n",
    "|                                     | Q1-4                                                            |   Triết   |\n",
    "|                                     | Q5-8                                                            | Minh Quân |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRVgmL2uBTRS",
   "metadata": {
    "id": "LRVgmL2uBTRS"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Differential Calculus\n",
    "## Rules of Calculus\n",
    "## Geometry of Gradients and Gradient Descent\n",
    "\n",
    "## Multivariate Chain Rule\n",
    "## The Backpropagation Algorithm\n",
    "\n",
    "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html\n",
    "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aQgVWYQ1I9AG",
   "metadata": {
    "id": "aQgVWYQ1I9AG"
   },
   "source": [
    "# What Autodiff Isn’t\n",
    "\n",
    "## Autodiff is not finite differences (numerical differentiation)\n",
    "\n",
    "## Autodiff is not symbolic differentiation\n",
    "\n",
    "## What autodiff is ? Types of Autodiff (explain forward, backward)\n",
    "f(x1,x2) = [sin(x1/x2) + x1/x2 - e^x2] * [x1/x2 + e^x2]\n",
    "\n",
    "## Types of Autodiff (explain forward, backward)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g87wIJiyWcSe",
   "metadata": {
    "id": "g87wIJiyWcSe"
   },
   "source": [
    "## Backpropagation Algorithm\n",
    "## Gradient-Based Optimization\n",
    "## give an example and describe the autodiff step-by-step workflow\n",
    "## Summary. Compare betwwen each type advancement of each types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9o5Q0ZG0KVQ2",
   "metadata": {
    "id": "9o5Q0ZG0KVQ2"
   },
   "source": [
    "# Visualization\n",
    "\n",
    "Code, implement AutoDiff, explain with an example. show backward and forward\n",
    "Ex: f(x1, x2) = ln(x1) + x1x2 - sin(x2)\n",
    "\n",
    "https://d2l.ai/chapter_preliminaries/autograd.html\n",
    "\n",
    "https://homepages.inf.ed.ac.uk/htang2/mlg2022/tutorial-3.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hk4f988CNvh7",
   "metadata": {
    "id": "hk4f988CNvh7",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exercises (1-4)\n",
    "\n",
    "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
    "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
    "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
    "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KievrNAtN8MC",
   "metadata": {
    "id": "KievrNAtN8MC"
   },
   "source": [
    "# Exercises (5-8)\n",
    "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
    "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
    "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
    "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a0faa-d09b-4fe7-aa0c-38c9ebc2e495",
   "metadata": {
    "id": "KievrNAtN8MC"
   },
   "source": [
    "# **5. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.**\n",
    "# Dependency Graph for \\( f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1} \\)\n",
    "\n",
    "The function is given by:\n",
    "\\[ f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1} \\]\n",
    "Since \\(\\log x^2 = 2 \\log |x|\\), we rewrite it as:\n",
    "\\[ f(x) = (2 \\log |x| \\cdot \\sin x) + \\frac{1}{x} \\]\n",
    "\n",
    "We construct a dependency graph to trace the computation from the input \\( x \\) to the output \\( f(x) \\). Each node represents a variable or operation, and edges represent dependencies.\n",
    "\n",
    "## Dependency Graph\n",
    "\n",
    "The computational steps are:\n",
    "1. Input: \\( x \\).\n",
    "2. Compute \\( |x| \\).\n",
    "3. Compute \\( \\log |x| \\).\n",
    "4. Compute \\( 2 \\log |x| \\).\n",
    "5. Compute \\( \\sin x \\).\n",
    "6. Compute the product \\( 2 \\log |x| \\cdot \\sin x \\).\n",
    "7. Compute \\( x^{-1} = \\frac{1}{x} \\).\n",
    "8. Compute \\( f(x) = (2 \\log |x| \\cdot \\sin x) + x^{-1} \\).\n",
    "\n",
    "### Graph Structure\n",
    "- **Nodes**:\n",
    "  - \\( x \\): Input.\n",
    "  - \\( |x| \\): Absolute value of \\( x \\).\n",
    "  - \\( \\log |x| \\): Logarithm of \\( |x| \\).\n",
    "  - \\( 2 \\log |x| \\): Scaled logarithm.\n",
    "  - \\( \\sin x \\): Sine of \\( x \\).\n",
    "  - \\( \\text{product} \\): \\( 2 \\log |x| \\cdot \\sin x \\).\n",
    "  - \\( x^{-1} \\): Inverse of \\( x \\).\n",
    "  - \\( f(x) \\): Final output.\n",
    "- **Edges**:\n",
    "  - \\( x \\to |x| \\)\n",
    "  - \\( |x| \\to \\log |x| \\)\n",
    "  - \\( \\log |x| \\to 2 \\log |x| \\)\n",
    "  - \\( x \\to \\sin x \\)\n",
    "  - \\( 2 \\log |x|, \\sin x \\to \\text{product} \\)\n",
    "  - \\( x \\to x^{-1} \\)\n",
    "  - \\( \\text{product}, x^{-1} \\to f(x) \\)\n",
    "\n",
    "The graph is visualized below using Python's `graphviz` library.\n",
    "\n",
    "## Python Code for Visualization\n",
    "\n",
    "### Install Dependencies\n",
    "Ensure `graphviz` is installed. Install it using pip:\n",
    "```bash\n",
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee535e6-37e1-4b7f-9430-c8573cdd0a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a directed graph\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dot \u001b[38;5;241m=\u001b[39m Digraph(comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDependency Graph for f(x)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a directed graph\n",
    "dot = Digraph(comment='Dependency Graph for f(x)')\n",
    "\n",
    "# Add nodes\n",
    "dot.node('x', 'x')\n",
    "dot.node('abs_x', '|x|')\n",
    "dot.node('log_abs_x', 'log|x|')\n",
    "dot.node('scaled_log', '2*log|x|')\n",
    "dot.node('sin_x', 'sin(x)')\n",
    "dot.node('product', '2*log|x| * sin(x)')\n",
    "dot.node('x_inv', 'x^(-1)')\n",
    "dot.node('f', 'f(x)')\n",
    "\n",
    "# Add edges\n",
    "dot.edge('x', 'abs_x')\n",
    "dot.edge('abs_x', 'log_abs_x')\n",
    "dot.edge('log_abs_x', 'scaled_log')\n",
    "dot.edge('x', 'sin_x')\n",
    "dot.edge('scaled_log', 'product')\n",
    "dot.edge('sin_x', 'product')\n",
    "dot.edge('x', 'x_inv')\n",
    "dot.edge('product', 'f')\n",
    "dot.edge('x_inv', 'f')\n",
    "\n",
    "# Render the graph\n",
    "dot.render('dependency_graph', format='png', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952d0e4-a965-4c14-9d64-759c70bb9237",
   "metadata": {
    "id": "KievrNAtN8MC"
   },
   "source": [
    "# **6. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.**\n",
    "\n",
    "The function is given by:\n",
    "\n",
    "\\[ f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1} \\]\n",
    "\n",
    "Since \\(\\log x^2 = 2 \\log |x|\\), we can rewrite the function as:\n",
    "\n",
    "\\[ f(x) = (2 \\log |x| \\cdot \\sin x) + \\frac{1}{x} \\]\n",
    "\n",
    "We construct a dependency graph to trace the computation from the input \\( x \\) to the output \\( f(x) \\). Each node represents a variable or operation, and edges represent dependencies.\n",
    "\n",
    "## Dependency Graph\n",
    "\n",
    "The computational steps are:\n",
    "1. Input: \\( x \\).\n",
    "2. Compute \\( |x| \\).\n",
    "3. Compute \\( \\log |x| \\).\n",
    "4. Compute \\( 2 \\log |x| \\).\n",
    "5. Compute \\( \\sin x \\).\n",
    "6. Compute the product \\( 2 \\log |x| \\cdot \\sin x \\).\n",
    "7. Compute \\( x^{-1} = \\frac{1}{x} \\).\n",
    "8. Compute \\( f(x) = (2 \\log |x| \\cdot \\sin x) + x^{-1} \\).\n",
    "\n",
    "The dependency graph is visualized below using Python's `graphviz` library.\n",
    "\n",
    "## Python Code\n",
    "\n",
    "### Install Dependencies\n",
    "Ensure `numpy` and `graphviz` are installed. You can install them using pip:\n",
    "\n",
    "```bash\n",
    "pip install numpy graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fd90a6-d847-4864-a951-757418b7b614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(1.0) = 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_f(x):\n",
    "    if x == 0:\n",
    "        raise ValueError(\"Function undefined at x=0\")\n",
    "    # Step 1: Input x\n",
    "    # Step 2: Compute |x|\n",
    "    abs_x = np.abs(x)\n",
    "    # Step 3: Compute log|x|\n",
    "    log_abs_x = np.log(abs_x)\n",
    "    # Step 4: Compute 2*log|x|\n",
    "    scaled_log = 2 * log_abs_x\n",
    "    # Step 5: Compute sin(x)\n",
    "    sin_x = np.sin(x)\n",
    "    # Step 6: Compute product\n",
    "    product = scaled_log * sin_x\n",
    "    # Step 7: Compute x^(-1)\n",
    "    x_inv = 1 / x\n",
    "    # Step 8: Compute f(x)\n",
    "    result = product + x_inv\n",
    "    return result\n",
    "\n",
    "# Example computation\n",
    "try:\n",
    "    x = 1.0\n",
    "    print(f\"f({x}) = {compute_f(x)}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ba584a-9554-4fc3-9f73-65d9d41e9201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a directed graph\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dot \u001b[38;5;241m=\u001b[39m Digraph(comment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDependency Graph for f(x)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a directed graph\n",
    "dot = Digraph(comment='Dependency Graph for f(x)')\n",
    "\n",
    "# Add nodes\n",
    "dot.node('x', 'x')\n",
    "dot.node('abs_x', '|x|')\n",
    "dot.node('log_abs_x', 'log|x|')\n",
    "dot.node('scaled_log', '2*log|x|')\n",
    "dot.node('sin_x', 'sin(x)')\n",
    "dot.node('product', '2*log|x| * sin(x)')\n",
    "dot.node('x_inv', 'x^(-1)')\n",
    "dot.node('f', 'f(x)')\n",
    "\n",
    "# Add edges\n",
    "dot.edge('x', 'abs_x')\n",
    "dot.edge('abs_x', 'log_abs_x')\n",
    "dot.edge('log_abs_x', 'scaled_log')\n",
    "dot.edge('x', 'sin_x')\n",
    "dot.edge('scaled_log', 'product')\n",
    "dot.edge('sin_x', 'product')\n",
    "dot.edge('x', 'x_inv')\n",
    "dot.edge('product', 'f')\n",
    "dot.edge('x_inv', 'f')\n",
    "\n",
    "# Render the graph\n",
    "dot.render('dependency_graph', format='png', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a346b-a555-45e3-acb4-0ee77a22501c",
   "metadata": {
    "id": "KievrNAtN8MC"
   },
   "source": [
    "# **7. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.**\n",
    "\n",
    "The function is:\n",
    "\\[ f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1} \\]\n",
    "Rewritten using \\(\\log x^2 = 2 \\log |x|\\):\n",
    "\\[ f(x) = (2 \\log |x| \\cdot \\sin x) + \\frac{1}{x} \\]\n",
    "\n",
    "We compute the derivative \\( f'(x) \\) using:\n",
    "1. **Forward Differentiation**: Propagating derivatives from \\( x \\) to \\( f(x) \\).\n",
    "2. **Backward Differentiation**: Propagating sensitivities from \\( f(x) \\) to \\( x \\).\n",
    "\n",
    "Both methods follow the dependency graph:\n",
    "- Nodes: \\( x \\), \\( |x| \\), \\( \\log |x| \\), \\( 2 \\log |x| \\), \\( \\sin x \\), \\( \\text{product} = 2 \\log |x| \\cdot \\sin x \\), \\( x^{-1} \\), \\( f(x) \\).\n",
    "- Edges define dependencies as shown in the previous section.\n",
    "\n",
    "## Forward Differentiation\n",
    "In forward mode, we compute \\( \\dot{v} = \\frac{dv}{dx} \\) for each node \\( v \\), starting with \\( \\dot{x} = 1 \\).\n",
    "\n",
    "### Steps\n",
    "1. \\( x \\): \\( \\dot{x} = 1 \\).\n",
    "2. \\( |x| \\): \\( \\dot{|x|} = \\text{sgn}(x) \\).\n",
    "3. \\( \\log |x| \\): \\( \\dot{\\log |x|} = \\frac{1}{x} \\).\n",
    "4. \\( 2 \\log |x| \\): \\( \\dot{2 \\log |x|} = \\frac{2}{x} \\).\n",
    "5. \\( \\sin x \\): \\( \\dot{\\sin x} = \\cos x \\).\n",
    "6. \\( \\text{product} = 2 \\log |x| \\cdot \\sin x \\): \\( \\dot{\\text{product}} = \\frac{2}{x} \\sin x + 2 \\log |x| \\cos x \\).\n",
    "7. \\( x^{-1} \\): \\( \\dot{x^{-1}} = -\\frac{1}{x^2} \\).\n",
    "8. \\( f(x) = \\text{product} + x^{-1} \\): \\( \\dot{f} = \\frac{2 \\sin x}{x} + 2 \\log |x| \\cos x - \\frac{1}{x^2} \\).\n",
    "\n",
    "## Backward Differentiation\n",
    "In backward mode, we compute \\( \\bar{v} = \\frac{\\partial f}{\\partial v} \\), starting with \\( \\bar{f} = 1 \\), and propagate sensitivities backward.\n",
    "\n",
    "### Steps\n",
    "1. \\( f(x) \\): \\( \\bar{f} = 1 \\). For \\( f = \\text{product} + x^{-1} \\), \\( \\bar{\\text{product}} = 1 \\), \\( \\bar{x^{-1}} = 1 \\).\n",
    "2. \\( \\text{product} = 2 \\log |x| \\cdot \\sin x \\): \\( \\bar{2 \\log |x|} = \\sin x \\), \\( \\bar{\\sin x} = 2 \\log |x| \\).\n",
    "3. \\( x^{-1} \\): Contributes \\( \\bar{x}_q = -\\frac{1}{x^2} \\).\n",
    "4. \\( 2 \\log |x| \\): \\( \\bar{\\log |x|} = 2 \\sin x \\).\n",
    "5. \\( \\sin x \\): Contributes \\( \\bar{x}_v = 2 \\log |x| \\cos x \\).\n",
    "6. \\( \\log |x| \\): \\( \\bar{|x|} = \\frac{2 \\sin x}{|x|} \\).\n",
    "7. \\( |x| \\): Contributes \\( \\bar{x}_{|x|} = \\frac{2 \\sin x}{x} \\).\n",
    "8. \\( x \\): Total \\( \\bar{x} = \\frac{2 \\sin x}{x} + 2 \\log |x| \\cos x - \\frac{1}{x^2} \\).\n",
    "\n",
    "## Result\n",
    "Both methods yield:\n",
    "\\[ f'(x) = \\frac{2 \\sin x}{x} + 2 \\log |x| \\cos x - \\frac{1}{x^2} \\]\n",
    "\n",
    "## Python Code\n",
    "The following code computes \\( f'(x) \\) using both forward and backward differentiation for a given \\( x \\).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def forward_differentiation(x):\n",
    "    if x == 0:\n",
    "        raise ValueError(\"Derivative undefined at x=0\")\n",
    "    # Forward pass: compute values\n",
    "    abs_x = np.abs(x)\n",
    "    log_abs_x = np.log(abs_x)\n",
    "    scaled_log = 2 * log_abs_x\n",
    "    sin_x = np.sin(x)\n",
    "    product = scaled_log * sin_x\n",
    "    x_inv = 1 / x\n",
    "    f = product + x_inv\n",
    "    \n",
    "    # Forward differentiation: compute derivatives\n",
    "    dot_x = 1\n",
    "    dot_abs_x = np.sign(x) * dot_x\n",
    "    dot_log_abs_x = (1 / abs_x) * dot_abs_x  # Simplifies to 1/x\n",
    "    dot_scaled_log = 2 * dot_log_abs_x\n",
    "    dot_sin_x = np.cos(x) * dot_x\n",
    "    dot_product = dot_scaled_log * sin_x + scaled_log * dot_sin_x\n",
    "    dot_x_inv = (-1 / x**2) * dot_x\n",
    "    dot_f = dot_product + dot_x_inv\n",
    "    \n",
    "    return dot_f\n",
    "\n",
    "def backward_differentiation(x):\n",
    "    if x == 0:\n",
    "        raise ValueError(\"Derivative undefined at x=0\")\n",
    "    # Forward pass: compute values (needed for sensitivities)\n",
    "    abs_x = np.abs(x)\n",
    "    log_abs_x = np.log(abs_x)\n",
    "    scaled_log = 2 * log_abs_x\n",
    "    sin_x = np.sin(x)\n",
    "    product = scaled_log * sin_x\n",
    "    x_inv = 1 / x\n",
    "    f = product + x_inv\n",
    "    \n",
    "    # Backward pass: compute sensitivities\n",
    "    bar_f = 1\n",
    "    bar_product = bar_f * 1\n",
    "    bar_x_inv = bar_f * 1\n",
    "    bar_scaled_log = bar_product * sin_x\n",
    "    bar_sin_x = bar_product * scaled_log\n",
    "    bar_log_abs_x = bar_scaled_log * 2\n",
    "    bar_abs_x = bar_log_abs_x * (1 / abs_x)\n",
    "    bar_x_from_abs = bar_abs_x * np.sign(x)  # Simplifies to (2*sin(x))/x\n",
    "    bar_x_from_sin = bar_sin_x * np.cos(x)\n",
    "    bar_x_from_x_inv = bar_x_inv * (-1 / x**2)\n",
    "    bar_x = bar_x_from_abs + bar_x_from_sin + bar_x_from_x_inv\n",
    "    \n",
    "    return bar_x\n",
    "\n",
    "# Example computation\n",
    "try:\n",
    "    x = 1.0\n",
    "    forward_result = forward_differentiation(x)\n",
    "    backward_result = backward_differentiation(x)\n",
    "    print(f\"Forward differentiation at x={x}: f'(x) = {forward_result}\")\n",
    "    print(f\"Backward differentiation at x={x}: f'(x) = {backward_result}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97605a-9363-4c63-ba6b-bc8019e32b04",
   "metadata": {
    "id": "KievrNAtN8MC"
   },
   "source": [
    "# **8. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved.**\n",
    "\n",
    "# When to Use Forward vs. Backward Differentiation\n",
    "\n",
    "**Forward differentiation** (forward-mode automatic differentiation) and **backward differentiation** (reverse-mode automatic differentiation) are methods to compute gradients of a function. The choice between them depends on the computational context, including the amount of intermediate data, parallelization opportunities, and the dimensions of inputs and outputs (i.e., the size of matrices and vectors involved). Below, we analyze these factors to determine when each method is preferable.\n",
    "\n",
    "## Overview of Forward and Backward Differentiation\n",
    "\n",
    "- **Forward Differentiation**:\n",
    "  - Propagates derivatives from inputs to outputs.\n",
    "  - For a function \\( f: \\mathbb{R}^n \\to \\mathbb{R}^m \\), it computes the derivative of each output with respect to each input by propagating a \"seed\" derivative (e.g., \\( \\frac{dx_i}{dx_i} = 1 \\)) through the computational graph.\n",
    "  - Computes the Jacobian matrix \\( J \\) column by column.\n",
    "\n",
    "- **Backward Differentiation**:\n",
    "  - Propagates sensitivities (adjoints) from outputs to inputs.\n",
    "  - Computes the derivative by starting with the sensitivity of the output (e.g., \\( \\frac{\\partial f}{\\partial f} = 1 \\)) and working backward through the computational graph.\n",
    "  - Computes the Jacobian matrix \\( J \\) row by row (or its transpose’s effect).\n",
    "\n",
    "For a function \\( f(x_1, \\dots, x_n) = (f_1, \\dots, f_m) \\), the Jacobian is an \\( m \\times n \\) matrix:\n",
    "\\[ J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\]\n",
    "\n",
    "- Forward mode computes each column (derivatives with respect to one input \\( x_i \\)).\n",
    "- Backward mode computes each row (derivatives of one output \\( f_j \\)).\n",
    "\n",
    "## Key Factors for Choosing Between Forward and Backward Differentiation\n",
    "\n",
    "### 1. Amount of Intermediate Data Needed\n",
    "- **Forward Differentiation**:\n",
    "  - Requires storing the derivative of each intermediate variable with respect to each input.\n",
    "  - For \\( n \\) inputs, you need to track \\( n \\) derivative values per node in the computational graph.\n",
    "  - Memory usage scales with the number of inputs (\\( O(n) \\) per node), which can be significant for large \\( n \\).\n",
    "  - Intermediate values (e.g., \\( \\sin x \\)) and their derivatives are computed and can be discarded after use, but all derivatives for a single input are computed in one pass.\n",
    "\n",
    "- **Backward Differentiation**:\n",
    "  - Requires storing the sensitivity (adjoint) of each intermediate variable with respect to the output.\n",
    "  - For \\( m \\) outputs, you need to track \\( m \\) adjoint values per node, but typically only for one output at a time in scalar cases.\n",
    "  - Memory usage scales with the number of outputs (\\( O(m) \\) per node), but the forward pass values (e.g., \\( \\sin x \\)) must be stored or recomputed to calculate adjoints.\n",
    "  - For large graphs, storing the forward pass or recomputing it can increase memory or computation costs.\n",
    "\n",
    "- **When to Choose**:\n",
    "  - **Forward**: Prefer when \\( n \\) (number of inputs) is small, as memory scales with \\( n \\). Ideal for functions with few inputs and many outputs (\\( n \\ll m \\)).\n",
    "  - **Backward**: Prefer when \\( m \\) (number of outputs) is small, as memory scales with \\( m \\). Ideal for functions with many inputs and few outputs (\\( m \\ll n \\)), common in machine learning (e.g., scalar loss functions).\n",
    "\n",
    "### 2. Ability to Parallelize Steps\n",
    "- **Forward Differentiation**:\n",
    "  - Each input’s derivative propagation is independent. For \\( n \\) inputs, you can run \\( n \\) forward passes in parallel, computing \\( \\frac{\\partial f}{\\partial x_i} \\) for each \\( x_i \\).\n",
    "  - Parallelization is straightforward across inputs, as each pass does not depend on others.\n",
    "  - However, within a single pass, computations follow the dependency graph’s topological order, which may limit parallelism if the graph has many sequential dependencies.\n",
    "\n",
    "- **Backward Differentiation**:\n",
    "  - Each output’s adjoint propagation is independent. For \\( m \\) outputs, you can run \\( m \\) backward passes in parallel, computing \\( \\frac{\\partial f_j}{\\partial x} \\) for each \\( f_j \\).\n",
    "  - Parallelization is effective across outputs, but for scalar functions (\\( m = 1 \\)), there’s only one backward pass, limiting this advantage.\n",
    "  - The backward pass requires the forward pass values, so you may need to sequentialize the forward computation or store all intermediate values, which can constrain parallelism.\n",
    "\n",
    "- **When to Choose**:\n",
    "  - **Forward**: Prefer when you have many inputs (\\( n \\)) and parallel hardware (e.g., GPUs) to compute derivatives for each input simultaneously. Best for \\( n \\gg m \\).\n",
    "  - **Backward**: Prefer when you have many outputs (\\( m \\)) and can parallelize across outputs, or when \\( m \\) is small (e.g., \\( m = 1 \\)), and parallelization within the backward pass (e.g., across nodes) is feasible. Common in neural networks where \\( m = 1 \\).\n",
    "\n",
    "### 3. Size of Matrices and Vectors Involved\n",
    "- **Forward Differentiation**:\n",
    "  - To compute the full Jacobian \\( J \\), forward mode requires \\( n \\) passes (one per input).\n",
    "  - Each pass computes a column of \\( J \\), i.e., \\( \\frac{\\partial f}{\\partial x_i} \\), an \\( m \\)-dimensional vector.\n",
    "  - Computational cost is \\( O(n \\cdot C) \\), where \\( C \\) is the cost of evaluating the function’s computational graph.\n",
    "  - Efficient when \\( n \\) is small or when only a few input derivatives are needed (e.g., directional derivatives).\n",
    "\n",
    "- **Backward Differentiation**:\n",
    "  - To compute the full Jacobian \\( J \\), backward mode requires \\( m \\) passes (one per output).\n",
    "  - Each pass computes a row of \\( J \\), i.e., \\( \\frac{\\partial f_j}{\\partial x} \\), an \\( n \\)-dimensional vector.\n",
    "  - Computational cost is \\( O(m \\cdot C) \\), where \\( C \\) is the cost of the graph evaluation (plus overhead for storing/recomputing forward pass values).\n",
    "  - Efficient when \\( m \\) is small, as fewer passes are needed.\n",
    "\n",
    "- **When to Choose**:\n",
    "  - **Forward**: Use when \\( n \\ll m \\), as it requires fewer passes (proportional to \\( n \\)). For example, in simulations with few parameters (\\( n \\)) and many outputs (\\( m \\)), like computational fluid dynamics with multiple observables.\n",
    "  - **Backward**: Use when \\( m \\ll n \\), as it requires fewer passes (proportional to \\( m \\)). Common in machine learning, where the loss is a scalar (\\( m = 1 \\)) and there are many parameters (\\( n \\)), like neural network weights.\n",
    "\n",
    "## Practical Examples\n",
    "- **Forward Differentiation**:\n",
    "  - **Use Case**: Sensitivity analysis in engineering (e.g., how a few design parameters affect many performance metrics).\n",
    "  - **Why**: Few inputs (\\( n \\)) mean fewer passes, and parallelizing across inputs is efficient.\n",
    "  - **Example**: Optimizing a mechanical system with 5 parameters affecting 100 outputs.\n",
    "\n",
    "- **Backward Differentiation**:\n",
    "  - **Use Case**: Training neural networks, where the loss function is scalar (\\( m = 1 \\)) and there are millions of parameters (\\( n \\)).\n",
    "  - **Why**: Only one backward pass is needed to compute gradients for all inputs, making it highly efficient.\n",
    "  - **Example**: Backpropagation in deep learning.\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Factor                     | Forward Differentiation                     | Backward Differentiation                    |\n",
    "|----------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| **Intermediate Data**      | Scales with \\( n \\) (inputs)                | Scales with \\( m \\) (outputs), plus forward pass storage |\n",
    "| **Parallelization**        | Across inputs (\\( n \\))                    | Across outputs (\\( m \\))                    |\n",
    "| **Cost**                   | \\( O(n \\cdot C) \\)                         | \\( O(m \\cdot C) \\)                         |\n",
    "| **Best For**               | \\( n \\ll m \\) (few inputs, many outputs)   | \\( m \\ll n \\) (many inputs, few outputs)   |\n",
    "\n",
    "## Conclusion\n",
    "- **Choose Forward Differentiation** when:\n",
    "  - The number of inputs (\\( n \\)) is small compared to outputs (\\( m \\)).\n",
    "  - You need derivatives with respect to a subset of inputs.\n",
    "  - Parallel hardware can compute input derivatives simultaneously.\n",
    "- **Choose Backward Differentiation** when:\n",
    "  - The number of outputs (\\( m \\)) is small compared to inputs (\\( n \\)), especially \\( m = 1 \\).\n",
    "  - You’re computing gradients for all inputs, as in optimization problems like neural network training.\n",
    "  - Memory for storing forward pass values is manageable, or recomputation is feasible.\n",
    "\n",
    "By considering the input-output dimensions, computational resources, and memory constraints, you can select the differentiation mode that minimizes computation time and resource usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144c129",
   "metadata": {
    "id": "4144c129",
    "origin_pos": 1
   },
   "source": [
    "***Reference: origin notebook of d2l as below***\n",
    "\n",
    "# Automatic Differentiation\n",
    ":label:`sec_autograd`\n",
    "\n",
    "Recall from :numref:`sec_calculus`\n",
    "that calculating derivatives is the crucial step\n",
    "in all the optimization algorithms\n",
    "that we will use to train deep networks.\n",
    "While the calculations are straightforward,\n",
    "working them out by hand can be tedious and error-prone,\n",
    "and these issues only grow\n",
    "as our models become more complex.\n",
    "\n",
    "Fortunately all modern deep learning frameworks\n",
    "take this work off our plates\n",
    "by offering *automatic differentiation*\n",
    "(often shortened to *autograd*).\n",
    "As we pass data through each successive function,\n",
    "the framework builds a *computational graph*\n",
    "that tracks how each value depends on others.\n",
    "To calculate derivatives,\n",
    "automatic differentiation\n",
    "works backwards through this graph\n",
    "applying the chain rule.\n",
    "The computational algorithm for applying the chain rule\n",
    "in this fashion is called *backpropagation*.\n",
    "\n",
    "While autograd libraries have become\n",
    "a hot concern over the past decade,\n",
    "they have a long history.\n",
    "In fact the earliest references to autograd\n",
    "date back over half of a century :cite:`Wengert.1964`.\n",
    "The core ideas behind modern backpropagation\n",
    "date to a PhD thesis from 1980 :cite:`Speelpenning.1980`\n",
    "and were further developed in the late 1980s :cite:`Griewank.1989`.\n",
    "While backpropagation has become the default method\n",
    "for computing gradients, it is not the only option.\n",
    "For instance, the Julia programming language employs\n",
    "forward propagation :cite:`Revels.Lubin.Papamarkou.2016`.\n",
    "Before exploring methods,\n",
    "let's first master the autograd package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130439cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:08.286501Z",
     "iopub.status.busy": "2023-08-18T19:26:08.285693Z",
     "iopub.status.idle": "2023-08-18T19:26:10.052257Z",
     "shell.execute_reply": "2023-08-18T19:26:10.050994Z"
    },
    "id": "130439cd",
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab3850",
   "metadata": {
    "id": "e2ab3850",
    "origin_pos": 6
   },
   "source": [
    "## A Simple Function\n",
    "\n",
    "Let's assume that we are interested\n",
    "in (**differentiating the function\n",
    "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "with respect to the column vector $\\mathbf{x}$.**)\n",
    "To start, we assign `x` an initial value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253cfab",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.056833Z",
     "iopub.status.busy": "2023-08-18T19:26:10.055871Z",
     "iopub.status.idle": "2023-08-18T19:26:10.084858Z",
     "shell.execute_reply": "2023-08-18T19:26:10.083727Z"
    },
    "id": "4253cfab",
    "origin_pos": 8,
    "outputId": "a4817393-fc1c-4795-875e-4d2861054425",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75614b0",
   "metadata": {
    "id": "e75614b0",
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**Before we calculate the gradient\n",
    "of $y$ with respect to $\\mathbf{x}$,\n",
    "we need a place to store it.**]\n",
    "In general, we avoid allocating new memory\n",
    "every time we take a derivative\n",
    "because deep learning requires\n",
    "successively computing derivatives\n",
    "with respect to the same parameters\n",
    "a great many times,\n",
    "and we might risk running out of memory.\n",
    "Note that the gradient of a scalar-valued function\n",
    "with respect to a vector $\\mathbf{x}$\n",
    "is vector-valued with\n",
    "the same shape as $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a001d1e",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.088716Z",
     "iopub.status.busy": "2023-08-18T19:26:10.087816Z",
     "iopub.status.idle": "2023-08-18T19:26:10.092878Z",
     "shell.execute_reply": "2023-08-18T19:26:10.091740Z"
    },
    "id": "2a001d1e",
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "x.grad  # The gradient is None by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74bc02",
   "metadata": {
    "id": "2e74bc02",
    "origin_pos": 15
   },
   "source": [
    "(**We now calculate our function of `x` and assign the result to `y`.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bd777",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.096336Z",
     "iopub.status.busy": "2023-08-18T19:26:10.095772Z",
     "iopub.status.idle": "2023-08-18T19:26:10.105236Z",
     "shell.execute_reply": "2023-08-18T19:26:10.104075Z"
    },
    "id": "6e3bd777",
    "origin_pos": 17,
    "outputId": "d6372de1-8216-4343-88fa-ca454f727629",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3067490",
   "metadata": {
    "id": "c3067490",
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**We can now take the gradient of `y`\n",
    "with respect to `x`**] by calling\n",
    "its `backward` method.\n",
    "Next, we can access the gradient\n",
    "via `x`'s `grad` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b134ae",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.108600Z",
     "iopub.status.busy": "2023-08-18T19:26:10.108011Z",
     "iopub.status.idle": "2023-08-18T19:26:10.160854Z",
     "shell.execute_reply": "2023-08-18T19:26:10.159702Z"
    },
    "id": "21b134ae",
    "origin_pos": 25,
    "outputId": "3c3b96c9-3955-4c18-f6ed-fe92feaf5957",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1390b",
   "metadata": {
    "id": "17d1390b",
    "origin_pos": 28
   },
   "source": [
    "(**We already know that the gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.**)\n",
    "We can now verify that the automatic gradient computation\n",
    "and the expected result are identical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5030e37d",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.164665Z",
     "iopub.status.busy": "2023-08-18T19:26:10.163930Z",
     "iopub.status.idle": "2023-08-18T19:26:10.171033Z",
     "shell.execute_reply": "2023-08-18T19:26:10.169923Z"
    },
    "id": "5030e37d",
    "origin_pos": 30,
    "outputId": "da15a8ec-929d-48c0-d80f-b3a536de4f8c",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da440e48",
   "metadata": {
    "id": "da440e48",
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**Now let's calculate\n",
    "another function of `x`\n",
    "and take its gradient.**]\n",
    "Note that PyTorch does not automatically\n",
    "reset the gradient buffer\n",
    "when we record a new gradient.\n",
    "Instead, the new gradient\n",
    "is added to the already-stored gradient.\n",
    "This behavior comes in handy\n",
    "when we want to optimize the sum\n",
    "of multiple objective functions.\n",
    "To reset the gradient buffer,\n",
    "we can call `x.grad.zero_()` as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5cf4b",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.174691Z",
     "iopub.status.busy": "2023-08-18T19:26:10.173957Z",
     "iopub.status.idle": "2023-08-18T19:26:10.181847Z",
     "shell.execute_reply": "2023-08-18T19:26:10.180759Z"
    },
    "id": "add5cf4b",
    "origin_pos": 37,
    "outputId": "c3ec729a-b1fb-481f-e7f5-655516e2b346",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()  # Reset the gradient\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd4c0c",
   "metadata": {
    "id": "8bdd4c0c",
    "origin_pos": 40
   },
   "source": [
    "## Backward for Non-Scalar Variables\n",
    "\n",
    "When `y` is a vector,\n",
    "the most natural representation\n",
    "of the derivative of  `y`\n",
    "with respect to a vector `x`\n",
    "is a matrix called the *Jacobian*\n",
    "that contains the partial derivatives\n",
    "of each component of `y`\n",
    "with respect to each component of `x`.\n",
    "Likewise, for higher-order `y` and `x`,\n",
    "the result of differentiation could be an even higher-order tensor.\n",
    "\n",
    "While Jacobians do show up in some\n",
    "advanced machine learning techniques,\n",
    "more commonly we want to sum up\n",
    "the gradients of each component of `y`\n",
    "with respect to the full vector `x`,\n",
    "yielding a vector of the same shape as `x`.\n",
    "For example, we often have a vector\n",
    "representing the value of our loss function\n",
    "calculated separately for each example among\n",
    "a *batch* of training examples.\n",
    "Here, we just want to (**sum up the gradients\n",
    "computed individually for each example**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda7124",
   "metadata": {
    "id": "9dda7124",
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Because deep learning frameworks vary\n",
    "in how they interpret gradients of\n",
    "non-scalar tensors,\n",
    "PyTorch takes some steps to avoid confusion.\n",
    "Invoking `backward` on a non-scalar elicits an error\n",
    "unless we tell PyTorch how to reduce the object to a scalar.\n",
    "More formally, we need to provide some vector $\\mathbf{v}$\n",
    "such that `backward` will compute\n",
    "$\\mathbf{v}^\\top \\partial_{\\mathbf{x}} \\mathbf{y}$\n",
    "rather than $\\partial_{\\mathbf{x}} \\mathbf{y}$.\n",
    "This next part may be confusing,\n",
    "but for reasons that will become clear later,\n",
    "this argument (representing $\\mathbf{v}$) is named `gradient`.\n",
    "For a more detailed description, see Yang Zhang's\n",
    "[Medium post](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa40bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.185096Z",
     "iopub.status.busy": "2023-08-18T19:26:10.184685Z",
     "iopub.status.idle": "2023-08-18T19:26:10.192537Z",
     "shell.execute_reply": "2023-08-18T19:26:10.191435Z"
    },
    "id": "1baa40bd",
    "origin_pos": 45,
    "outputId": "dc4b39c4-6948-4526-f167-7118248b07d2",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd2c9d",
   "metadata": {
    "id": "ffbd2c9d",
    "origin_pos": 48
   },
   "source": [
    "## Detaching Computation\n",
    "\n",
    "Sometimes, we wish to [**move some calculations\n",
    "outside of the recorded computational graph.**]\n",
    "For example, say that we use the input\n",
    "to create some auxiliary intermediate terms\n",
    "for which we do not want to compute a gradient.\n",
    "In this case, we need to *detach*\n",
    "the respective computational graph\n",
    "from the final result.\n",
    "The following toy example makes this clearer:\n",
    "suppose we have `z = x * y` and `y = x * x`\n",
    "but we want to focus on the *direct* influence of `x` on `z`\n",
    "rather than the influence conveyed via `y`.\n",
    "In this case, we can create a new variable `u`\n",
    "that takes the same value as `y`\n",
    "but whose *provenance* (how it was created)\n",
    "has been wiped out.\n",
    "Thus `u` has no ancestors in the graph\n",
    "and gradients do not flow through `u` to `x`.\n",
    "For example, taking the gradient of `z = x * u`\n",
    "will yield the result `u`,\n",
    "(not `3 * x * x` as you might have\n",
    "expected since `z = x * x * x`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ac041",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.196001Z",
     "iopub.status.busy": "2023-08-18T19:26:10.195456Z",
     "iopub.status.idle": "2023-08-18T19:26:10.203246Z",
     "shell.execute_reply": "2023-08-18T19:26:10.202155Z"
    },
    "id": "107ac041",
    "origin_pos": 50,
    "outputId": "e07fe5dc-f226-4cf2-8042-4c499f7ed519",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0378e1f",
   "metadata": {
    "id": "e0378e1f",
    "origin_pos": 53
   },
   "source": [
    "Note that while this procedure\n",
    "detaches `y`'s ancestors\n",
    "from the graph leading to `z`,\n",
    "the computational graph leading to `y`\n",
    "persists and thus we can calculate\n",
    "the gradient of `y` with respect to `x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c674b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.206880Z",
     "iopub.status.busy": "2023-08-18T19:26:10.206001Z",
     "iopub.status.idle": "2023-08-18T19:26:10.213592Z",
     "shell.execute_reply": "2023-08-18T19:26:10.212476Z"
    },
    "id": "cb8c674b",
    "origin_pos": 55,
    "outputId": "772ee87b-f492-48b8-e329-e0ef9f98ca18",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f056ce",
   "metadata": {
    "id": "76f056ce",
    "origin_pos": 58
   },
   "source": [
    "## Gradients and Python Control Flow\n",
    "\n",
    "So far we reviewed cases where the path from input to output\n",
    "was well defined via a function such as `z = x * x * x`.\n",
    "Programming offers us a lot more freedom in how we compute results.\n",
    "For instance, we can make them depend on auxiliary variables\n",
    "or condition choices on intermediate results.\n",
    "One benefit of using automatic differentiation\n",
    "is that [**even if**] building the computational graph of\n",
    "(**a function required passing through a maze of Python control flow**)\n",
    "(e.g., conditionals, loops, and arbitrary function calls),\n",
    "(**we can still calculate the gradient of the resulting variable.**)\n",
    "To illustrate this, consider the following code snippet where\n",
    "the number of iterations of the `while` loop\n",
    "and the evaluation of the `if` statement\n",
    "both depend on the value of the input `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83327c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.218214Z",
     "iopub.status.busy": "2023-08-18T19:26:10.217554Z",
     "iopub.status.idle": "2023-08-18T19:26:10.222956Z",
     "shell.execute_reply": "2023-08-18T19:26:10.221858Z"
    },
    "id": "a83327c2",
    "origin_pos": 60,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f6785",
   "metadata": {
    "id": "189f6785",
    "origin_pos": 63
   },
   "source": [
    "Below, we call this function, passing in a random value, as input.\n",
    "Since the input is a random variable,\n",
    "we do not know what form\n",
    "the computational graph will take.\n",
    "However, whenever we execute `f(a)`\n",
    "on a specific input, we realize\n",
    "a specific computational graph\n",
    "and can subsequently run `backward`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef0264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.227364Z",
     "iopub.status.busy": "2023-08-18T19:26:10.226919Z",
     "iopub.status.idle": "2023-08-18T19:26:10.232880Z",
     "shell.execute_reply": "2023-08-18T19:26:10.231773Z"
    },
    "id": "c5ef0264",
    "origin_pos": 65,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51065133",
   "metadata": {
    "id": "51065133",
    "origin_pos": 68
   },
   "source": [
    "Even though our function `f` is, for demonstration purposes, a bit contrived,\n",
    "its dependence on the input is quite simple:\n",
    "it is a *linear* function of `a`\n",
    "with piecewise defined scale.\n",
    "As such, `f(a) / a` is a vector of constant entries\n",
    "and, moreover, `f(a) / a` needs to match\n",
    "the gradient of `f(a)` with respect to `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14ef91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.237298Z",
     "iopub.status.busy": "2023-08-18T19:26:10.236886Z",
     "iopub.status.idle": "2023-08-18T19:26:10.243577Z",
     "shell.execute_reply": "2023-08-18T19:26:10.242480Z"
    },
    "id": "ab14ef91",
    "origin_pos": 70,
    "outputId": "4d85cfd2-c7b3-460e-bf7e-76fdf88e0f50",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a992f28c",
   "metadata": {
    "id": "a992f28c",
    "origin_pos": 73
   },
   "source": [
    "Dynamic control flow is very common in deep learning.\n",
    "For instance, when processing text, the computational graph\n",
    "depends on the length of the input.\n",
    "In these cases, automatic differentiation\n",
    "becomes vital for statistical modeling\n",
    "since it is impossible to compute the gradient *a priori*.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "You have now gotten a taste of the power of automatic differentiation.\n",
    "The development of libraries for calculating derivatives\n",
    "both automatically and efficiently\n",
    "has been a massive productivity booster\n",
    "for deep learning practitioners,\n",
    "liberating them so they can focus on less menial.\n",
    "Moreover, autograd lets us design massive models\n",
    "for which pen and paper gradient computations\n",
    "would be prohibitively time consuming.\n",
    "Interestingly, while we use autograd to *optimize* models\n",
    "(in a statistical sense)\n",
    "the *optimization* of autograd libraries themselves\n",
    "(in a computational sense)\n",
    "is a rich subject\n",
    "of vital interest to framework designers.\n",
    "Here, tools from compilers and graph manipulation\n",
    "are leveraged to compute results\n",
    "in the most expedient and memory-efficient manner.\n",
    "\n",
    "For now, try to remember these basics: (i) attach gradients to those variables with respect to which we desire derivatives; (ii) record the computation of the target value; (iii) execute the backpropagation function; and  (iv) access the resulting gradient.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
    "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
    "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
    "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
    "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
    "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
    "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
    "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ab97d",
   "metadata": {
    "id": "4c0ab97d",
    "origin_pos": 75,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/35)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
