{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newfrogg/Automatic-Differentiation/blob/GiaHinh/autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Mục lớn                             | Mục nhỏ                                                         |           |\n",
        "|-------------------------------------|-----------------------------------------------------------------|-----------|\n",
        "| Introduction                        |                                                                 |  1 người  |\n",
        "|                                     | Differential Calculus                                           |  Tử Quân  |\n",
        "|                                     | Rules of Calculus                                               |  Tử Quân  |\n",
        "|                                     | Multivariate Chain Rule                                         |  Tử Quân  |\n",
        "|                                     | Geometry of Gradients and Gradient Descent                      |  Tử Quân  |\n",
        "| What Autodiff Isn’t                 |                                                                 |  3 người  |\n",
        "|                                     | Autodiff is not finite differences (numerical differentiation)  |  An Đông  |\n",
        "|                                     | Autodiff is not symbolic differentiation                        |  An Đông  |\n",
        "|                                     | What autodiff is ?                                              |  An Đông  |\n",
        "|                                     | Types of Autodiff (explain forward, backward)                   |  An Đông  |\n",
        "|                                     | Backpropagation Algorithm                                       |  Gia Hinh |\n",
        "|                                     | give an example and describe the autodiff step-by-step workflow |  Gia Hinh |\n",
        "|                                     | The Power of Automatic Differentiation, Applications                                     |  Gia Hinh |\n",
        "|                                     | Summary. Compare betwwen each type advancement of each types    |  Gia Hinh |\n",
        "| Visualization. Code, Implementation |                                                                 |  1 người  |\n",
        "|                                     | Code                                                            | Bảo Lương |\n",
        "| Exercise                            |                                                                 |  2 người  |\n",
        "|                                     | Q1-4                                                            |   Triết   |\n",
        "|                                     | Q5-8                                                            | Minh Quân |"
      ],
      "metadata": {
        "id": "6-FUvYwvWEJF"
      },
      "id": "6-FUvYwvWEJF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Differential Calculus\n",
        "## Rules of Calculus\n",
        "## Geometry of Gradients and Gradient Descent\n",
        "\n",
        "## Multivariate Chain Rule\n",
        "## The Backpropagation Algorithm\n",
        "\n",
        "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html\n",
        "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"
      ],
      "metadata": {
        "id": "LRVgmL2uBTRS"
      },
      "id": "LRVgmL2uBTRS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Autodiff Isn’t\n",
        "\n",
        "## Autodiff is not finite differences (numerical differentiation)\n",
        "\n",
        "## Autodiff is not symbolic differentiation\n",
        "\n",
        "## What autodiff is ? Types of Autodiff (explain forward, backward)\n",
        "f(x1,x2) = [sin(x1/x2) + x1/x2 - e^x2] * [x1/x2 + e^x2]\n",
        "\n",
        "## Types of Autodiff (explain forward, backward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aQgVWYQ1I9AG"
      },
      "id": "aQgVWYQ1I9AG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation Algorithm\n",
        "Backpropagation Algorithm is the central in traning neural networks.\n",
        "* It is an algorithm for computing gradients\n",
        "* It is a special case of **reverse-mode automatic differentiation** (AD) applied to a computational graph with <font color='red'>a scalar-output</font>.\n",
        "* Note: Backpropagation refers to the <font color='red'> whole process of training an artificial neural network </font> using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. **In contrast, reverse-mode auto diff is simply a technique used to compute gradients efficiently and it happens to be used by backpropagation.**\n",
        "\n",
        "Let take an example. We have a function, where:\n",
        "\n",
        "\\begin{split}\\begin{aligned}\n",
        "f(u, v) & = (u+v)^{2} \\\\\n",
        "u(a, b) & = (a+b)^{2}, \\qquad v(a, b) = (a-b)^{2}, \\\\\n",
        "a(w, x, y, z) & = (w+x+y+z)^{2}, \\qquad b(w, x, y, z) = (w+x-y-z)^2.\n",
        "\\end{aligned}\\end{split}\n",
        "\n",
        "For illustration, this diagram describes the nodes represent variables and edges show functional dependence.\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://d2l.ai/_images/chain-net1.svg\" width=\"400\" height=\"300\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "If we want to calculate $\\frac{\\partial f}{\\partial w}$, applying the multi-variate chain rule to get that:\n",
        "\\begin{split}\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial w} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial w} + \\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial w}, \\\\\n",
        "\\frac{\\partial u}{\\partial w} & = \\frac{\\partial u}{\\partial a}\\frac{\\partial a}{\\partial w}+\\frac{\\partial u}{\\partial b}\\frac{\\partial b}{\\partial w}, \\\\\n",
        "\\frac{\\partial v}{\\partial w} & = \\frac{\\partial v}{\\partial a}\\frac{\\partial a}{\\partial w}+\\frac{\\partial v}{\\partial b}\\frac{\\partial b}{\\partial w}.\n",
        "\\end{aligned}\\end{split}\n",
        "\n",
        "Next, try using this decomposition to compute $\\frac{\\partial f}{\\partial w}$. Notice that all we need here are the various single step partials:\n",
        "\\begin{split}\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial u} = 2(u+v), & \\quad\\frac{\\partial f}{\\partial v} = 2(u+v), \\\\\n",
        "\\frac{\\partial u}{\\partial a} = 2(a+b), & \\quad\\frac{\\partial u}{\\partial b} = 2(a+b), \\\\\n",
        "\\frac{\\partial v}{\\partial a} = 2(a-b), & \\quad\\frac{\\partial v}{\\partial b} = -2(a-b), \\\\\n",
        "\\frac{\\partial a}{\\partial w} = 2(w+x+y+z), & \\quad\\frac{\\partial b}{\\partial w} = 2(w+x-y-z).\n",
        "\\end{aligned}\\end{split}\n",
        "\n",
        "If we write this out into code this becomes a fairly manageable expression.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g87wIJiyWcSe"
      },
      "id": "g87wIJiyWcSe"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==2.0.0 torchvision==0.15.1\n",
        "# !pip install d2l==1.0.3\n",
        "## Import necessary libraries\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "from IPython import display\n",
        "from mpl_toolkits import mplot3d\n",
        "# from d2l import torch as d2l\n",
        "# Compute the value of the function from inputs to outputs\n",
        "w, x, y, z = -1, 0, -2, 1\n",
        "a, b = (w + x + y + z)**2, (w + x - y - z)**2\n",
        "u, v = (a + b)**2, (a - b)**2\n",
        "f = (u + v)**2\n",
        "print(f'    f at {w}, {x}, {y}, {z} is {f}')\n",
        "\n",
        "# Compute the single step partials\n",
        "df_du, df_dv = 2*(u + v), 2*(u + v)\n",
        "du_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)\n",
        "da_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)\n",
        "\n",
        "# Compute the final result from inputs to outputs\n",
        "du_dw, dv_dw = du_da*da_dw + du_db*db_dw, dv_da*da_dw + dv_db*db_dw\n",
        "df_dw = df_du*du_dw + df_dv*dv_dw\n",
        "print(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thYCunxjeySi",
        "outputId": "8cf670cd-25e9-4258-b7ba-164f77953e31"
      },
      "id": "thYCunxjeySi",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    f at -1, 0, -2, 1 is 1024\n",
            "df/dw at -1, 0, -2, 1 is -4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, note that this still does not make it easy to compute something like\n",
        "$\\frac{\\partial f}{\\partial x}$\n",
        "\n",
        "The reason for that is the way we chose to apply the chain rule. If we look at what we did above, we always kept $\\partial w$ in the denominator when we would. In this way, we chose to apply the chain rule seeing how changed every other variable. If that is what we wanted, this would be a good idea.\n",
        "\n",
        "However, think back to our motivation from deep learning: we want to see how very parameter changes the loss. In essence, we want to apply the chain rule keeping $\\partial f$ in the numerator whenever we can!\n",
        "\n",
        "To be more explicit, note that we can write\n",
        "\n",
        "\\begin{split}\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial w} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial w} + \\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial w}, \\\\\n",
        "\\frac{\\partial f}{\\partial a} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial a}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial a}, \\\\\n",
        "\\frac{\\partial f}{\\partial b} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial b}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial b}.\n",
        "\\end{aligned}\\end{split}\n",
        "\n",
        "Note that this application of the chain rule has us explicitly compute $\\frac{\\partial f}{\\partial u}, \\frac{\\partial f}{\\partial v}, \\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}, \\; \\textrm{and} \\; \\frac{\\partial f}{\\partial w}$\n",
        "\n",
        "\\begin{split}\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial x} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial x} + \\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial x}, \\\\\n",
        "\\frac{\\partial f}{\\partial y} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial y}+\\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial y}, \\\\\n",
        "\\frac{\\partial f}{\\partial z} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial z}+\\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial z}.\n",
        "\\end{aligned}\\end{split}\n",
        "\n",
        "and then keeping track of how $f$ changes when we change any node in the entire network. Let’s implement it.\n",
        "\n"
      ],
      "metadata": {
        "id": "C6xf-q5re4Cb"
      },
      "id": "C6xf-q5re4Cb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the value of the function from inputs to outputs\n",
        "w, x, y, z = -1, 0, -2, 1\n",
        "a, b = (w + x + y + z)**2, (w + x - y - z)**2\n",
        "u, v = (a + b)**2, (a - b)**2\n",
        "f = (u + v)**2\n",
        "print(f'f at {w}, {x}, {y}, {z} is {f}')\n",
        "\n",
        "# Compute the derivative using the decomposition above\n",
        "# First compute the single step partials\n",
        "df_du, df_dv = 2*(u + v), 2*(u + v)\n",
        "du_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)\n",
        "da_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)\n",
        "da_dx, db_dx = 2*(w + x + y + z), 2*(w + x - y - z)\n",
        "da_dy, db_dy = 2*(w + x + y + z), -2*(w + x - y - z)\n",
        "da_dz, db_dz = 2*(w + x + y + z), -2*(w + x - y - z)\n",
        "\n",
        "# Now compute how f changes when we change any value from output to input\n",
        "df_da, df_db = df_du*du_da + df_dv*dv_da, df_du*du_db + df_dv*dv_db\n",
        "df_dw, df_dx = df_da*da_dw + df_db*db_dw, df_da*da_dx + df_db*db_dx\n",
        "df_dy, df_dz = df_da*da_dy + df_db*db_dy, df_da*da_dz + df_db*db_dz\n",
        "\n",
        "print(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')\n",
        "print(f'df/dx at {w}, {x}, {y}, {z} is {df_dx}')\n",
        "print(f'df/dy at {w}, {x}, {y}, {z} is {df_dy}')\n",
        "print(f'df/dz at {w}, {x}, {y}, {z} is {df_dz}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVFYzZw_gC89",
        "outputId": "bed8f1ff-9bf6-4164-faa7-b846f97fdf19"
      },
      "id": "JVFYzZw_gC89",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f at -1, 0, -2, 1 is 1024\n",
            "df/dw at -1, 0, -2, 1 is -4096\n",
            "df/dx at -1, 0, -2, 1 is -4096\n",
            "df/dy at -1, 0, -2, 1 is -4096\n",
            "df/dz at -1, 0, -2, 1 is -4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fact that we compute derivatives from back towards the inputs rather than from the inputs forward to the outputs (as we did in the first code snippet above) is what gives this algorithm its name: backpropagation. Note that there are two steps:\n",
        "1. Compute the value of the function, and the single step partials from front to back. While not done above, this can be combined into a single **forward pass**.\n",
        "2. Compute the gradient of $f$ from back to front. We call this the **backwards pass**.\n",
        "\n",
        "<font color='red'>This is precisely what every deep learning algorithm implements to allow the computation of the gradient of the loss with respect to every weight in the network at one pass</font>\n",
        "\n",
        "\n",
        "\n",
        "To see how to encapsulated this, let’s take a quick look at this example."
      ],
      "metadata": {
        "id": "3SlglgdNgkrM"
      },
      "id": "3SlglgdNgkrM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize as ndarrays, then attach gradients\n",
        "w = torch.tensor([-1.], requires_grad=True)\n",
        "x = torch.tensor([0.], requires_grad=True)\n",
        "y = torch.tensor([-2.], requires_grad=True)\n",
        "z = torch.tensor([1.], requires_grad=True)\n",
        "# Do the computation like usual, tracking gradients\n",
        "a, b = (w + x + y + z)**2, (w + x - y - z)**2\n",
        "u, v = (a + b)**2, (a - b)**2\n",
        "f = (u + v)**2\n",
        "\n",
        "# Execute backward pass\n",
        "f.backward()\n",
        "\n",
        "print(f'df/dw at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {w.grad.data.item()}')\n",
        "print(f'df/dx at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {x.grad.data.item()}')\n",
        "print(f'df/dy at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {y.grad.data.item()}')\n",
        "print(f'df/dz at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {z.grad.data.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C98smF7Og5Pd",
        "outputId": "13e1e3e5-bc8c-4922-ae59-45ed7aa4d47c"
      },
      "id": "C98smF7Og5Pd",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df/dw at -1.0, 0.0, -2.0, 1.0 is -4096.0\n",
            "df/dx at -1.0, 0.0, -2.0, 1.0 is -4096.0\n",
            "df/dy at -1.0, 0.0, -2.0, 1.0 is -4096.0\n",
            "df/dz at -1.0, 0.0, -2.0, 1.0 is -4096.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of what we did above can be done automatically by calling **f.backwards()**."
      ],
      "metadata": {
        "id": "EKWY_uu7g86b"
      },
      "id": "EKWY_uu7g86b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation of Loss\n",
        "* The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function or error\n",
        "function.\n",
        "* Loss is the starting point of the backpropagation\n",
        "* Backpropagation is the method to minimize the loss function by adjusting network’s weights and biases. The level of adjustment is determined by the gradients of the loss function\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7s0bA9xPaOnv"
      },
      "id": "7s0bA9xPaOnv"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the network structure\n",
        "input_neurons = 2\n",
        "hidden_neurons = 2\n",
        "output_neurons = 1\n",
        "\n",
        "# Weights and biases from the example\n",
        "weights_input_hidden = np.array([[0.15, 0.2], [0.25, 0.3]])  # w11, w12, w21, w22\n",
        "weights_hidden_output = np.array([0.4, 0.45])  # w_h1, w_h2\n",
        "bias_hidden = 0.35  # b_h\n",
        "bias_output = 0.6   # b_y\n",
        "\n",
        "# Set up the figure\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Positions for neurons in straight rows\n",
        "input_pos = np.array([[0, 0.75], [0, 0.25]])  # x1, x2\n",
        "hidden_pos = np.array([[0.5, 0.75], [0.5, 0.25]])  # h1, h2\n",
        "output_pos = np.array([[1, 0.5]])  # y\n",
        "\n",
        "# Draw neurons\n",
        "for pos in input_pos:\n",
        "    ax.add_patch(plt.Circle(pos, 0.05, color='blue', fill=True))\n",
        "for pos in hidden_pos:\n",
        "    ax.add_patch(plt.Circle(pos, 0.05, color='green', fill=True))\n",
        "for pos in output_pos:\n",
        "    ax.add_patch(plt.Circle(pos, 0.05, color='red', fill=True))\n",
        "\n",
        "# Draw connections and label weights with offset to avoid overlap\n",
        "for i in range(input_neurons):\n",
        "    for j in range(hidden_neurons):\n",
        "        ax.plot([input_pos[i][0], hidden_pos[j][0]], [input_pos[i][1], hidden_pos[j][1]], 'k-')\n",
        "        if i != j:\n",
        "            mid_x = (input_pos[i][0] + hidden_pos[j][0]) / 4\n",
        "            mid_y = (input_pos[i][1] + hidden_pos[j][1]) / (3 / (j + 1))\n",
        "            ax.text(mid_x , mid_y , f'w_{i+1}{j+1}={weights_input_hidden[i][j]}', fontsize=12, ha='center')\n",
        "            continue\n",
        "        # Calculate midpoint for label placement\n",
        "        mid_x = (input_pos[i][0] + hidden_pos[j][0]) / 2\n",
        "        mid_y = (input_pos[i][1] + hidden_pos[j][1]) / 2 + 0.02\n",
        "        ax.text(mid_x , mid_y , f'w_{i+1}{j+1}={weights_input_hidden[i][j]}', fontsize=12, ha='center')\n",
        "\n",
        "for i in range(hidden_neurons):\n",
        "    ax.plot([hidden_pos[i][0], output_pos[0][0]], [hidden_pos[i][1], output_pos[0][1]], 'k-')\n",
        "    # Label weights\n",
        "    mid_x = (hidden_pos[i][0] + output_pos[0][0]) / 2\n",
        "    mid_y = (hidden_pos[i][1] + output_pos[0][1]) / 2\n",
        "    ax.text(mid_x, mid_y, f'w_h{i+1}={weights_hidden_output[i]}', fontsize=12, ha='center')\n",
        "\n",
        "# Draw biases\n",
        "ax.text(hidden_pos[0][0] + 0.15, hidden_pos[0][1] + 0.1, f'b_h={bias_hidden}', fontsize=12, color='green')\n",
        "ax.text(output_pos[0][0] + 0.15, output_pos[0][1] + 0.1, f'b_y={bias_output}', fontsize=12, color='red')\n",
        "\n",
        "# Label neurons\n",
        "ax.text(input_pos[0][0] - 0.1, input_pos[0][1], 'x₁', fontsize=10)\n",
        "ax.text(input_pos[1][0] - 0.1, input_pos[1][1], 'x₂', fontsize=10)\n",
        "ax.text(hidden_pos[0][0], hidden_pos[0][1] + 0.1, 'h₁', fontsize=10)\n",
        "ax.text(hidden_pos[1][0], hidden_pos[1][1] + 0.1, 'h₂', fontsize=10)\n",
        "ax.text(output_pos[0][0] + 0.1, output_pos[0][1], 'y', fontsize=10)\n",
        "\n",
        "# Label layers\n",
        "ax.text(0, 1, 'Input Layer', fontsize=12, ha='center')\n",
        "ax.text(0.5, 1, 'Hidden Layer\\n(Sigmoid)', fontsize=12, ha='center')\n",
        "ax.text(1, 1, 'Output Layer\\n(Sigmoid)', fontsize=12, ha='center')\n",
        "\n",
        "# Set limits and remove axes\n",
        "ax.set_xlim(-0.2, 1.3)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.axis('off')\n",
        "\n",
        "# Display the plot in Colab\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K95IDuScdwr-",
        "outputId": "d7da8c8b-65de-481d-fb4b-f60a32666332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "id": "K95IDuScdwr-",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoYAAAIECAYAAACE8u+qAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAArA1JREFUeJzs3Xd8jWcbwPHfyYksI0ZixY5Rs4hRIRJqBDVqi5EYRc2qrZSisdVWWltoiRotYvSNClFqVKhds/ZeiYxz7vePI6dOk0gi42Rc3/dzPnWecz/3cz158yRX7qlRSimEEEIIIUSmZ2HuAIQQQgghRNogiaEQQgghhAAkMRRCCCGEEK9JYiiEEEIIIQBJDIUQQgghxGuSGAohhBBCCEASQyGEEEII8ZokhkIIIYQQApDEUAghhBBCvCaJoRCCYsWK4ePjE2+5lStXotFouHr1arLVKYQQIu2QxFCIDCY6eTt69Gisn3t4eFChQoVUjir17Nu3D41Gg7+/v7lDEenUX3/9RZcuXXBycsLa2pqCBQvSuXNn/vrrryTV6+vry5YtW5InyHgEBwczYcIEnjx5kqDyPj4+ZMuWLWWDEumCJIZCCM6fP893331n7jCEMLuffvqJqlWr8uuvv9K9e3cWLVpEz549CQwMpGrVqmzevPmd607txPCrr75KcGIoRDRLcwcghDA/a2trc4eQaSmlePXqFba2tuYOJdP7+++/6dq1KyVKlGD//v04OjoaPxs8eDBubm507dqVkJAQSpQoYcZIxatXr7CyssLCQtq3kpt8RYUQsY4H/Ouvv6hfvz62trYUKlSIyZMno9frY5yrlGLy5MkUKlQIOzs76tWrF2eX25MnT/jss88oXLgw1tbWlCxZkmnTppnUe/XqVTQaDTNnzmTp0qU4OztjbW1N9erV+eOPP5LtnmfOnImrqyt58uTB1tYWFxeXGN3P7u7uvP/++7GeX6ZMGRo3bmx8r9frmTNnDuXLl8fGxoZ8+fLRp08fHj9+bHJesWLF+Oijj9i1axfVqlXD1taWJUuWJNt9iXc3Y8YMQkNDWbp0qUlSCODg4MCSJUt4+fIl06dPNx738fGhWLFiMeqaMGECGo3G+F6j0fDy5UtWrVqFRqNBo9EYn7nosufOnaN9+/bkyJGDPHnyMHjwYF69emWsI/rZWLlyZYzraTQaJkyYYKxv+PDhABQvXtx4vYSMDX6ba9eu0a9fP8qUKYOtrS158uShXbt2JvVevnwZjUbDN998E+P84OBgNBoN69evNx67efMmPXr0IF++fFhbW1O+fHmWL19ucl708JAffviBsWPH4uTkhJ2dHc+ePUvS/YjYSYuhEBnU06dPefDgQYzjkZGR8Z57584d6tWrR1RUFKNGjSJr1qwsXbo01latL7/8ksmTJ9O0aVOaNm3K8ePHadSoERERESblQkNDcXd35+bNm/Tp04ciRYoQHBzM6NGjuX37NnPmzDEpv27dOp4/f06fPn3QaDRMnz6d1q1bc/nyZbJkyZK4L0Ys5s6dS4sWLejcuTMRERH88MMPtGvXjl9++YVmzZoB0LVrVz755BNOnz5tMi7zjz/+4MKFC4wdO9Z4rE+fPqxcuZLu3bszaNAgrly5woIFCzhx4gQHDx40ifn8+fN06tSJPn368Mknn1CmTJkk349Iup9//plixYrh5uYW6+d169alWLFibN++PdF1r1mzhl69elGjRg169+4NgLOzs0mZ9u3bU6xYMaZMmcLvv//OvHnzePz4MatXr07UtVq3bs2FCxdYv34933zzDQ4ODgAxkt3E+uOPPwgODqZjx44UKlSIq1evsnjxYjw8PDhz5gx2dnaUKFGC2rVr4+fnx5AhQ0zO9/PzI3v27LRs2RKAu3fv8sEHH6DRaBgwYACOjo7s3LmTnj178uzZMz777DOT8ydNmoSVlRXDhg0jPDwcKyurJN2PiIMSQmQoK1asUMBbX+XLlzc5p2jRosrb29v4/rPPPlOAOnz4sPHYvXv3lL29vQLUlStXjMesrKxUs2bNlF6vN5YdM2aMAkzqnDRpksqaNau6cOGCybVHjRqltFqtun79ulJKqStXrihA5cmTRz169MhYbuvWrQpQP//881vvPzAwUAFq48aNby0XGhpq8j4iIkJVqFBB1a9f33jsyZMnysbGRo0cOdKk7KBBg1TWrFnVixcvlFJKBQUFKUD5+fmZlAsICIhxvGjRogpQAQEBb41PpK4nT54oQLVs2fKt5Vq0aKEA9ezZM6WUUt7e3qpo0aIxyo0fP17991ds1qxZTZ6J/5Zt0aKFyfF+/fopQJ08eVIp9e+zsWLFihh1AGr8+PHG9zNmzDB5VuPj7e2tsmbN+tYy/31mlFLq0KFDClCrV682HluyZIkC1NmzZ43HIiIilIODg8n99+zZUxUoUEA9ePDApM6OHTsqe3t74/Win+kSJUrEGoNIXtKVLEQGtXDhQvbs2RPjValSpXjP3bFjBx988AE1atQwHnN0dKRz584m5fbu3UtERAQDBw406Tb771/6ABs3bsTNzY1cuXLx4MED46tBgwbodDr2799vUr5Dhw7kypXL+D66Fefy5csJuv/4vNn6+fjxY54+fYqbmxvHjx83Hre3t6dly5asX78epRQAOp2OH3/8kVatWpE1a1bjvdnb29OwYUOTe3NxcSFbtmwEBgaaXLt48eIm3dDC/J4/fw5A9uzZ31ou+vOU6Mbs37+/yfuBAwcChucxLXjzmYmMjOThw4eULFmSnDlzmjw37du3x8bGBj8/P+OxXbt28eDBA7p06QIYhqBs2rSJ5s2bo5QyeW4aN27M06dPTeoE8Pb2lrG4qUC6koXIoGrUqEG1atViHI9OzN7m2rVr1KxZM8bx/3Z5Xrt2DYBSpUqZHHd0dDRJ6gAuXrxISEhInN1Z9+7dM3lfpEiRGHEDMcbsvatffvmFyZMn8+effxIeHm48/maCC9CtWzd+/PFHgoKCqFu3Lnv37uXu3bt07drVWObixYs8ffqUvHnzxnqt/95b8eLFk+UeRPKJTviiE8S4JDSBfBf/fY6cnZ2xsLBI8tjA5BIWFsaUKVNYsWIFN2/eNP6xBIahK9Fy5sxJ8+bNWbduHZMmTQIM3chOTk7Ur18fgPv37/PkyROWLl3K0qVLY72ePDfmIYmhECJV6PV6GjZsyIgRI2L9vHTp0ibvtVptrOXe/GX0roKCgmjRogV169Zl0aJFFChQgCxZsrBixQrWrVtnUrZx48bky5ePtWvXUrduXdauXUv+/Plp0KCBsYxerydv3rwmLSRv+m8yLK0eaY+9vT0FChQgJCTkreVCQkJwcnIiR44cQMw/JKLpdLokx/TfulPyWgkxcOBAVqxYwWeffUatWrWwt7dHo9HQsWPHGBPTunXrxsaNGwkODqZixYps27aNfv36GWcRR5fv0qUL3t7esV7vv70b8tykDkkMhRAxFC1alIsXL8Y4fv78+RjlwNBi9ubyHffv34/Rsufs7MyLFy9MEipz2bRpEzY2NuzatctkqZ4VK1bEKKvVavHy8mLlypVMmzaNLVu28Mknn5gkrs7Ozuzdu5fatWvLL6907KOPPuK7777jwIED1KlTJ8bnQUFBXL16lT59+hiP5cqVK9a1AqNb098UV2IX7eLFiyatYpcuXUKv1xtnPUe3mv/3eu9yrXfh7++Pt7c3s2bNMh579epVrPfv6emJo6Mjfn5+1KxZk9DQUJNWdkdHR7Jnz45Op0sTPxPEv2SMoRAihqZNm/L7779z5MgR47H79+/HaBFr0KABWbJkYf78+SYtef+dYQyGcUeHDh1i165dMT578uQJUVFRyXcD8dBqtWg0GpOWlqtXr8a5+HDXrl15/Pgxffr04cWLF8ZxUtHat2+PTqczdpu9KSoqShYZTieGDx+Ora0tffr04eHDhyafPXr0iL59+2JnZ2dcCgYMfxQ8ffrUpKXx9u3bsS6EnTVr1rd+LyxcuNDk/fz58wFo0qQJADly5MDBwSHGeNxFixbFei2ImUQmhVarjdFiP3/+/FhbLC0tLenUqRMbNmxg5cqVVKxY0aQFUKvV0qZNGzZt2sTp06djnH///v1ki1skjrQYCiFiGDFiBGvWrMHT05PBgwcbl6spWrSoyS9AR0dHhg0bxpQpU/joo49o2rQpJ06cYOfOncYlMqINHz6cbdu28dFHH+Hj44OLiwsvX77k1KlT+Pv7c/Xq1RjnJMWmTZs4d+5cjOPe3t40a9aM2bNn4+npiZeXF/fu3WPhwoWULFky1q7EKlWqUKFCBTZu3EjZsmWpWrWqyefu7u706dOHKVOm8Oeff9KoUSOyZMnCxYsX2bhxI3PnzqVt27bJdm8iZZQqVYpVq1bRuXNnKlasSM+ePSlevDhXr15l2bJlPHjwgPXr15ssM9OxY0dGjhzJxx9/zKBBgwgNDWXx4sWULl06xuQJFxcX9u7dy+zZsylYsCDFixc3Gct75coVWrRogaenJ4cOHWLt2rV4eXmZrKXZq1cvpk6dSq9evahWrRr79+/nwoULMe7FxcUFgC+++IKOHTuSJUsWmjdvbkwYYxMZGcnkyZNjHM+dOzf9+vXjo48+Ys2aNdjb21OuXDkOHTrE3r17yZMnT6z1devWjXnz5hEYGMi0adNifD516lQCAwOpWbMmn3zyCeXKlePRo0ccP36cvXv38ujRozhjFSnIjDOihRApIHq5mj/++CPWz93d3eNdrkYppUJCQpS7u7uysbFRTk5OatKkSWrZsmUxlsDQ6XTqq6++UgUKFFC2trbKw8NDnT59OtY6nz9/rkaPHq1KliyprKyslIODg3J1dVUzZ85UERERSql/l+SYMWNGjNj5z5IcsYle2iKuV1BQkFJKqWXLlqlSpUopa2tr9d5776kVK1bEusRItOnTpytA+fr6xnntpUuXKhcXF2Vra6uyZ8+uKlasqEaMGKFu3bplLFO0aFHVrFmzt96DMK+QkBDVqVMnVaBAAZUlSxaVP39+1alTJ3Xq1KlYy+/evVtVqFBBWVlZqTJlyqi1a9fG+r107tw5VbduXWVra2uynFN02TNnzqi2bduq7Nmzq1y5cqkBAwaosLAwkzpCQ0NVz549lb29vcqePbtq3769unfvXqzPxqRJk5STk5OysLCId+kab2/vOJ8ZZ2dnpZRSjx8/Vt27d1cODg4qW7ZsqnHjxurcuXOxPuvRypcvrywsLNQ///wT6+d3795V/fv3V4ULFzZ+rT/88EO1dOlSY5mELkElkodGqWQYyS2EEBnc3LlzGTJkCFevXo0xY1qIpJgwYQJfffUV9+/fT9ZW87SgSpUq5M6dm19//dXcoYgEkjGGQggRD6UUy5Ytw93dXZJCIRLo6NGj/Pnnn3Tr1s3coYhEkDGGQggRh5cvX7Jt2zYCAwM5deoUW7duNXdIQqR5p0+f5tixY8yaNYsCBQrQoUMHc4ckEkESQyGEiMP9+/fx8vIiZ86cjBkzhhYtWpg7JCHSPH9/fyZOnEiZMmVYv349NjY25g5JJIKMMRRCCCGEEICMMRRCCCGEEK9JYiiEEEIIIQBJDIUQcZg+fTrvvfdejD1Q4zNhwoQU2Y4ruWk0GiZMmBBvuf/eT2RkJIULF451twkh5LkxkOcm/Uq3ieHKlSvRaDQcPXrU3KEAEBoayoQJE9i3b1+Cyu/btw+NRoO/v3/KBibEO3j27BnTpk1j5MiRxk3vX7x4wfjx46lQoQJZs2YlT548VK5cmcGDB3Pr1i0zR5x6smTJwueff87XX3/Nq1evzB2OSEPkuYmbPDfpR7pNDNOa0NBQvvrqqwQnhkKkZcuXLycqKopOnToBhr/269aty4wZM3Bzc2P27NmMGTOGqlWrsm7dOpMtucaOHUtYWJi5Qk+wsLAwxo4d+07ndu/enQcPHrBu3bpkjkqkZ/LcvJ08N+mDLFcjEkQpxatXr7C1tTV3KCIVrFixghYtWhiXmdiyZQsnTpzAz88PLy8vk7KvXr0iIiLC+N7S0hJLy7T/oyUpS2jkzJmTRo0asXLlSnr06JGMUYn0TJ6bt5PnJn3IUC2GPj4+ZMuWjZs3b9KqVSuyZcuGo6Mjw4YNQ6fTGctdvXoVjUbDzJkz+eabbyhatCi2tra4u7tz+vRpkzo9PDzw8PCI9VrFihUz1ufo6AjAV199hUajSfA4jPjMnDkTV1dX8uTJg62tLS4uLjG6n93d3U02WX9TmTJlaNy4sfG9Xq9nzpw5lC9fHhsbG/Lly0efPn14/PixyXnFihXjo48+YteuXVSrVg1bW1uWLFmS5PsRad+VK1cICQmhQYMGxmN///03ALVr145R3sbGhhw5chjfxzZWKiwsjEGDBuHg4ED27Nlp0aIFN2/ejPGcRJ974cIFunTpgr29PY6OjowbNw6lFDdu3KBly5bkyJGD/PnzM2vWrBjx3Lt3j549e5IvXz5sbGx4//33WbVqVYxysT2jBw4coHr16tjY2ODs7PzW7/mGDRty4MABHj16FGcZkXnIcyPPTUaRoRJDAJ1OR+PGjcmTJw8zZ87E3d2dWbNmsXTp0hhlV69ezbx58+jfvz+jR4/m9OnT1K9fn7t37ybqmo6OjixevBiAjz/+mDVr1rBmzRpat26d5PuZO3cuVapUYeLEifj6+mJpaUm7du3Yvn27sUzXrl0JCQmJkdT+8ccfxh8U0fr06cPw4cOpXbs2c+fOpXv37vj5+dG4cWMiIyNNzj9//jydOnWiYcOGzJ07l8qVKyf5fkTaFxwcDEDVqlWNx4oWLQoYnpl3WfrUx8eH+fPn07RpU6ZNm4atrS3NmjWLs3yHDh3Q6/VMnTqVmjVrMnnyZObMmUPDhg1xcnJi2rRplCxZkmHDhrF//37jeWFhYXh4eLBmzRo6d+7MjBkzsLe3x8fHh7lz5741xlOnTtGoUSPu3bvHhAkT6N69O+PHj2fz5s2xlndxcUEpZfx6icxNnht5bjIMlU6tWLFCAeqPP/4wHvP29laAmjhxoknZKlWqKBcXF+P7K1euKEDZ2tqqf/75x3j88OHDClBDhgwxHnN3d1fu7u4xru/t7a2KFi1qfH///n0FqPHjxyco/sDAQAWojRs3vrVcaGioyfuIiAhVoUIFVb9+feOxJ0+eKBsbGzVy5EiTsoMGDVJZs2ZVL168UEopFRQUpADl5+dnUi4gICDG8aJFiypABQQEJOh+RMYxduxYBajnz58bj4WGhqoyZcooQBUtWlT5+PioZcuWqbt378Y4f/z48erNHy3Hjh1TgPrss89Myvn4+MR4ZqLP7d27t/FYVFSUKlSokNJoNGrq1KnG448fP1a2trbK29vbeGzOnDkKUGvXrjUei4iIULVq1VLZsmVTz549Mx7/77VbtWqlbGxs1LVr14zHzpw5o7RarYrtR+WtW7cUoKZNmxbjM5H5yHMjz01GkeFaDAH69u1r8t7NzY3Lly/HKNeqVSucnJyM72vUqEHNmjXZsWNHiseYUG+O6Xv8+DFPnz7Fzc2N48ePG4/b29vTsmVL1q9fb/yrVKfT8eOPP9KqVSuyZs0KwMaNG7G3t6dhw4Y8ePDA+HJxcSFbtmwEBgaaXLt48eIm3dAic3j48CGWlpZky5bNeMzW1pbDhw8zfPhwwLAqQM+ePSlQoAADBw4kPDw8zvoCAgIA6Nevn8nxgQMHxnlOr169jP/WarVUq1YNpRQ9e/Y0Hs+ZMydlypQxebZ37NhB/vz5jYP/wTAbctCgQbx48YLffvst1uvpdDp27dpFq1atKFKkiPF42bJl43wGcuXKBcCDBw/ivA+RechzI89NRpHhEkMbGxvjeL9ouXLlijGGDqBUqVIxjpUuXZqrV6+mVHiJ9ssvv/DBBx9gY2ND7ty5jd3WT58+NSnXrVs3rl+/TlBQEAB79+7l7t27dO3a1Vjm4sWLPH36lLx58+Lo6GjyevHiBffu3TOps3jx4il/gyLdsLe3Z/r06Vy9epWrV6+ybNkyypQpw4IFC5g0aVKc5127dg0LC4sY308lS5aM85w3f8lEX9vGxgYHB4cYx998tq9du0apUqWMS4VEK1u2rPHz2Ny/f5+wsLBYfyaUKVMm1nOi/whLD2vPCfOR58aUPDdpX4ZLDLVabbLWF9c375uTWVJKUFCQcYbbokWL2LFjB3v27MHLyyvGeJXGjRuTL18+1q5dC8DatWvJnz+/yUBovV5P3rx52bNnT6yviRMnmtQpM5Azpzx58hAVFcXz58/jLFO0aFF69OjBwYMHyZkzJ35+fskaQ2zPcVzP9n+fhdQS/Yv1v790ReYkz03CyHOT9qX9ufEp6OLFizGOXbhwwTjbGAytjbF1Q//3L6iU+Otn06ZN2NjYsGvXLqytrY3HV6xYEaOsVqvFy8uLlStXMm3aNLZs2cInn3xi8kPB2dmZvXv3Urt2bUn6RJzee+89wDDLslKlSm8tmytXLpydnWNMfHpT0aJF0ev1XLlyxaRl4dKlS8kT8H+uFRISgl6vN2n9OHfunPHz2Dg6OmJraxvrz4Tz58/Hes6VK1eAf1tVROYmz40peW7SrwzXYpgYW7Zs4ebNm8b3R44c4fDhwzRp0sR4zNnZmXPnznH//n3jsZMnT3Lw4EGTuuzs7AB48uRJssWn1WrRaDQxltrZsmVLrOW7du3K48eP6dOnDy9evDCZjQzQvn17dDpdrN0XUVFRyRq7SL9q1aoFYLKr0MmTJ2MdE3Tt2jXOnDkTZ7cRYBxr9N+tsObPn58c4Zpo2rQpd+7c4ccffzQei4qKYv78+WTLlg13d/dYz9NqtTRu3JgtW7Zw/fp14/GzZ8+ya9euWM85duwYGo3G+PUSmZs8N/LcZBSZusWwZMmS1KlTh08//ZTw8HDmzJlDnjx5GDFihLFMjx49mD17No0bN6Znz57cu3ePb7/9lvLly/Ps2TNjOVtbW8qVK8ePP/5I6dKlyZ07NxUqVKBChQpvjWHTpk3Gv8re5O3tTbNmzZg9ezaenp54eXlx7949Fi5cSMmSJQkJCYlxTpUqVahQoQIbN26kbNmyJssmgGG9wz59+jBlyhT+/PNPGjVqRJYsWbh48SIbN25k7ty5tG3bNrFfRpHBlChRggoVKrB3717jIrR79uxh/PjxtGjRgg8++IBs2bJx+fJlli9fTnh4+FvX7HRxcaFNmzbMmTOHhw8f8sEHH/Dbb78Zd31Iztb23r17s2TJEnx8fDh27BjFihXD39+fgwcPMmfOHLJnzx7nuV999RUBAQG4ubnRr18/4y/G8uXLx/q87dmzh9q1a5MnT55ki1+kX/LcyHOTYZhvQnTSxLVcTdasWWOU/e8yANHL1cyYMUPNmjVLFS5cWFlbWys3Nzd18uTJGOevXbtWlShRQllZWanKlSurXbt2xViuRimlgoODlYuLi7Kysop36Zro5WriegUFBSmllFq2bJkqVaqUsra2Vu+9955asWJFjPt50/Tp0xWgfH1947z20qVLlYuLi7K1tVXZs2dXFStWVCNGjFC3bt0ylilatKhq1qxZnHWIjG327NkqW7ZsxuWSLl++rL788kv1wQcfqLx58ypLS0vl6OiomjVrpv73v/+ZnBvb9+fLly9V//79Ve7cuVW2bNlUq1at1Pnz5xVgspRG9Ln37983OT+uZ9vd3V2VL1/e5Njdu3dV9+7dlYODg7KyslIVK1ZUK1asiHFubM/ob7/9ZnyGS5Qoob799ttY7+fJkyfKyspKff/997F/AUWmJM+NPDcZQbpNDJPizcQwo5kzZ47SaDQma0oJkVhPnjxRuXPnTtEf4CdOnIixdlp68c0336gCBQrEWGdUZG7y3LydPDfpQ6YeY5jRKKVYtmwZ7u7uMZYtECIx7O3tGTFiBDNmzECv1ye5vrCwsBjH5syZg4WFBXXr1k1y/akpMjKS2bNnM3bsWJnEJUzIcxM3eW7Sj0w9xjCjePnyJdu2bSMwMJBTp06xdetWc4ckMoCRI0cycuTIZKlr+vTpHDt2jHr16mFpacnOnTvZuXMnvXv3pnDhwslyjdSSJUsWk4H2QrxJnpvYyXOTfkhimAHcv38fLy8vcubMyZgxY2jRooW5QxLChKurK3v27GHSpEm8ePGCIkWKMGHCBL744gtzhyZEmiXPjTAHjVJmWuVSCCGEEEKkKTLGUAghhBBCAJIYCiGEEEKI1yQxFEIIIYQQgCSGQgghhBDiNUkMhRBCCCEEIImhEEIIIYR4TRJDIYQQQggBSGIohBBCCCFek8RQCCGEEEIAkhgKIYQQQojXJDEUQgghhBCAJIZCCCGEEOI1SQyFEEIIIQQgiaEQQgghhHhNEkMhhBBCCAFIYiiEEEIIIV6TxFAIIYQQQgCSGAohhBBCiNckMRRCCCGEEIAkhkIIIYQQ4jVJDIUQQgghBCCJoRBCCCGEeE0SQyGEEEIIAUhiKIQQQgghXpPEUAghhBBCAJIYCiGEEEKI1yQxFEIIIYQQgCSGQgghhBDiNUkMhRBCCCEEIImhEEIIIYR4TRJDIYQQQggBSGIohBBCCCFek8RQCCGEEEIAkhgKIYQQQojXJDEUQgghhBCAJIZCCCGEEOI1SQyFEEIIIQQgiaEQQgghhHhNEkMhhBBCCAFIYiiEEEIIIV6TxFAIIYQQQgCSGAohhBBCiNckMRRCCCGEEIAkhkIIIYQQ4jVJDIUQQgghBCCJoRBCCCGEeE0SQyGEEEIIAUhiKIQQQgghXpPEUAghhBBCAJIYCiGEEEKI1yQxFEKIdGDCvglovtLwIPRBkurx2eJDNt9syRSVECKjsTR3AEIIITKeZceXMfPQTK48vkJh+8IMqjGIgTUHxnveX/f+YsJvEzh26xh3XtzBLosd5RzLMdx1OM3LNDcp67PFh1UnV8Woo0yeMpwbcC7Z7kWIzEQSQyGEEMlqydEl9N3elzZl2/D5B58TdD2IQQGDCI0MZWSdkW8999rTazwPf473+94UzF6Q0MhQNp3dRIsfWrDkoyX0dultUt5aa833Lb43OWZvbZ/s9yREZiGJoRAiXfHw8KBy5crMmTPH3KGIWIRFhvHF/76gWalm+Lf3B+ATl0/QKz2T9k+it0tvctnmivP8pqWa0rRUU5NjA2oMwGWpC7MPzY6RGFpaWNKlUpfkvxEhMikZYyiEEOnIg9AHtN/YnhxTcpBneh4G7xzMq6hXia7n5rObtPqhFdl8s+E4w5Fhu4eh0+uSHF/g1UAehj2kX/V+Jsf7V+/Py8iXbL+4PdF1ai20FLYvzJNXT2L9XKfX8Sz82buEK4T4D2kxFEKIdKT9xvYUy1mMKR9O4febvzPvyDwev3rM6o9XJ7gOndLReG1jajrVZGajmey9vJdZh2bhnMuZT6t/aiz3OOwxOhV/smiXxQ67LHYAnLh9AoBqBauZlHEp6IKFxoITt08kqIXvZcRLwqLCePrqKdvOb2PnxZ10qNAhRrnQyFByTM1BaGQouWxy0alCJ6Y1nEY2K5lgI8S7kMRQCJHu6PV6RowYwffff4+VlRV9+/ZlwoQJ5g4rVRTPVZytHbcC0J/+5LDKwaKjixjmOoxK+SolqI5XUa/oUL4D49zHAdC3Wl+qLqnKshPLTBLDKkuqcO3ptXjrG+8+ngkeEwC4/eI2Wo2WvFnzmpSx0lqRxzYPt17cSlCMQ3cPZcmxJQBYaCxoXbY1C5osMClTIFsBRtQeQdUCVdErPQGXAlh0dBEn755kn88+LC3kV5wQiSVPjRAi3Vm1ahWff/45hw8f5tChQ/j4+FC7dm3y5s2Ll5cX586d49dff8XDw8PcoSa7/tX7m7wfWHMgi44uYsfFHQlODMGQDL7JrYgba0LWmBzza+1HWFRYvHWVyFXC+O+wyDCstFaxlrOxtCEsMv76AD774DPalmvLree32PDXBnR6HRG6CJMyUxpMMXnfsUJHSucpzRf/+wL/M/50rNAxQdcSQvxLEkMhRLpTqVIlxo8fD0CpUqVYsGABv/76K8OHD2fr1q306tXLzBGmnFK5S5m8d87ljIXGgqtPria4DhtLGxyzOpocy2Wbi8evHpscq12kdqLjs81iGyOBi/Yq6hW2WWwTVM97Du/xnsN7AHR7vxuN1jSi+frmHO51GI1GE+d5Qz4YwrjAcey9vFcSQyHegSSGQoh0p1Il05axAgUKcO/ePfLkyUOePHnMFJV5vC1JiotWo01Qufsv7ydojGE2q2zGMX0FshVAp3Tce3nPpDs5QhfBw7CHFMxWMNHxArQt15Y+v/ThwsMLlHEoE2c52yy25LHNw6OwR+90HSEyO0kMhRDpTpYsWUzeazQa9Hq9maJJXRcfXaR4ruLG95ceXUKv9BTLWSzZr1X9u+qJHmNYOX9lAI7eOmqy7MzRW0fRK73x88SK7oJ+Gv70reWehz/nQegDHO0c31pOCBE7SQyFECIdWfjHQho5NzK+n394PgBNSjZJ9mu9yxjD+sXrk9s2N4uPLjZJDBcfXYxdFjualW5mPPYg9AEPQh9QxL6IcVbzf1saASJ1kawOWY2tpS3lHMsBhm7pSF0k2a2zm5SdtH8SCoVnSc/E37AQQhJDIYRIT648vkKL9S3wLOnJoX8OsTZkLV4VvXg///vJfq13HWM4qd4k+u/oT7uN7Wjs3Jig60GsDVnL1/W/JrdtbmPZBUcW8NVvXxHoHYhHMQ8A+vzSh2fhz6hbpC5OOZy48+IOfqf8OPfgHLMazTJ2Wd95cYcqS6rQqUIn41jEXX/vYsfFHXiW9KTley2T/gUQIhOSxFAIkWHcvHkTd3d3bt68SefOnenUqRMzZ840d1jJ6se2P/Llvi8ZtXcUlhaWDKg+gBmNZpg7LBP9qvcji0UWZh2axbbz2yicozDfNP6GwTUHx3tuh/IdWHZiGYuPLuZh2EOyW2XHpaAL0xpMo0WZFsZyOW1y8lHpj9hzeQ+rTq5Cp9dRMndJfOv7Msx1GBYa2b9BiHehUUopcwchhBBCCCHMT/6kEkIIIYQQgHQlCyFEhvD01dN4J4rkz5Y/laIRQqRX0pUshBAZgM8WH1adXPXWMmq8/LgXQrydJIZCCJEBnLl/hlvP374PcYMSDVIpGiFEeiWJoRBCCCGEAGTyiRBCCCGEeE0SQyGEEEIIAUhiKIQQQgghXpPEUAghhBBCAJIYCiGEEEKI1yQxFEIIIYQQgCSGQgghhBDiNUkMhRBCCCEEIImhEEIIIYR4TRJDIYQQQggBSGIohBBCCCFek8RQCCGEEEIAkhgKIYQQQojXJDEUQgghhBCAJIZCCCGEEOI1S3MHkBlERMBff8GTJxAWBlZWYGcHZctCrlzmjk6ItOtx2GPOPjhLaGQoEboIbC1tyWmTk/J5y2OltTJ3eEIIkeFIYpgCoqIgIAB++w0OHIDjxw3JYWycnaFuXXB1hY8/hjx5UjdWIdKSh6EP2XxuM8E3gtl/bT9/P/471nJWWiuq5q9KnSJ1cC/mTpOSTdBaaFM5WiGEyHg0Sill7iAyirAwWLkSpk6F69chSxaIjIz/PEtLQzJpYwN9+sCQIVC0aIqHK0Sace3JNWYfms3SY0t5pXuFpYUlUfqoeM/LYpGFSH0kReyLMKr2KHwq+2CbxTYVIhZCiIxJxhgmA70evvkGnJygf3+4ccNwPCFJIRiSQoBXr2DBAihRArp1gwcPUibe9OzFixeMHz8eT09PcufOjUajYeXKlbGWPXLkCP369cPFxYUsWbKg0WiSPR69Xs/06dMpXrw4NjY2VKpUifXr1yfo3Nu3bzNq1Cjq1atH9uzZ0Wg07Nu3L9ayHh4eaDSaGC9PT89kvJvUd//lfbr+1JUS80qw8I+FvNK9AkhQUggQqTc8ZDee3qD/jv44zXbim0PfoFf6FItZCCEyMulKTqL796FLF9i9O3nq0+kM/123zlDnxo3g5pY8dWcEDx48YOLEiRQpUoT3338/zkQKYMeOHXz//fdUqlSJEiVKcOHChWSP54svvmDq1Kl88sknVK9ena1bt+Ll5YVGo6Fjx45vPff8+fNMmzaNUqVKUbFiRQ4dOvTW8oUKFWLKlCkmxwoWLJjkezCX/df2025jOx6GPkxyIqcwdHw8fvWYz3d/TsClANa2XotjVsfkCFUIITIN6UpOgt9+g/bt4eHDfxO65GRhAUrBpEkwerThfWYXHh7O48ePyZ8/P0ePHqV69eqsWLECHx+fGGXv3r1Ljhw5sLW1ZcCAASxcuJDk/Ha/efMmxYsXp3fv3ixYsAAApRTu7u5cuXKFq1evotXGPe7t+fPnREZGkjt3bvz9/WnXrh2BgYF4eHjEKOvh4cGDBw84ffp0ssVvLjq9jikHpvBl4JdoNJoUad3TarTkscvDxnYbqVu0brLXL4QQGZWkGu9o40aoVy/lkkIwdFErBWPHQteuKXedlBASEoJGo2Hbtm3GY8eOHUOj0VC1alWTsk2aNKFmzZoJqtfa2pr8+fMnqGy+fPmwtU258WZbt24lMjKSfv36GY9pNBo+/fRT/vnnn3hbALNnz07u3LkTdc2oqChevHjxTvGmBTq9jq6buzIucBwKlWJdvjql42HoQzxWeuB/xj9FriGEEBmRJIbvYMcO6NTJ8O/UStbWr4dPPzUkiulBhQoVyJkzJ/v37zceCwoKwsLCgpMnT/Ls2TPAMEYvODiYunXN26rz4MGDBL3Cw8ON55w4cYKsWbNStmxZk7pq1Khh/Dw5XbhwgaxZs5I9e3by58/PuHHjiEzoQNY0QClF3+19+eH0D6lyPZ0yPJwd/Tuy4+KOVLmmEEKkd5IYJtKff0KbNv+25qUWpeC772D69NS7ZlJYWFhQu3ZtgoKCjMeCgoJo1aoVGo2G4OBgAGOS6GbmgZSOjo4Jer05seT27dvky5cvxqSWAgUKAHDr1q1ki8/Z2ZkvvviC9evXs3r1amrWrMnkyZPp0qVLsl0jpU07OI3vj39vHA+YGqJbJdtsaMPJOydT7bpCCJFeyeSTRHj2DJo1M8w2NlfL3ejRULkyNG5snusnhpubG2PHjuXly5dkzZqVAwcO4Ovry7Vr1wgKCsLT05OgoCA0Gg116tQxa6x79uxJULny5csb/x0WFoa1tXWMMjY2NsbPk8uyZctM3nft2pXevXvz3XffMWTIED744INku1ZK2HVpF6N/HW2WaysUkbpImq5rytn+Z8lhncMscQghRHogiWEiTJgAd+4YWgvNRaMxrHV47pxh3cO0zM3NjaioKA4dOkThwoW5d+8ebm5u/PXXX8aWxKCgIMqVK5fosXbJrUGDBok+x9bW1qRrOdqrV6+Mn6ekoUOH8t1337F37940nRi+inpF7196Y6GxMNsyMjql486LO3y17ytmNZ5llhiEECI9kK7kBPrrL5g717xJIRiuf/06zEoHv9uqVauGjY0N+/fvJygoiLx581K6dGnc3Nw4cuQI4eHhBAUFmb0bGeDOnTsJer3ZCligQAHu3LkTY6bz7du3gZRfSqZw4cIAPHr0KEWvk1Qzg2dy4+kNs68tqFd65hyew1/3/jJrHEIIkZZJYpgAShkmfqSV5WKil7C5ds3ckbydlZUVNWrUICgoyCQBdHNzIzw8HD8/P+7evWv2iSdgSPIS8vrxxx+N51SuXJnQ0FDOnj1rUtfhw4eNn6eky5cvA4bxkWnVtSfXmLx/cqqOK3wbC40F/Xb0S9Zli4QQIiORruQECAiAN+ZQpAk6naFre8UKc0fydm5ubsyePZu///6boUOHAuDg4EDZsmWZNm2asYy5vcsYw5YtWzJkyBAWLVpkso7ht99+i5OTE66ursayt2/f5unTpzg7O5MlS5ZExfbs2TOsra1NxjMqpZg8eTIAjdPwgNPx+8YbZwenBVH6KPZf20/ApQCalGpi7nCEECLNkcQwAZYt+3c/47QiKgp++AHmz4ds2cwdTdzc3Nz4+uuvuXHjhkkCWLduXZYsWUKxYsUoVKhQoupcsGABT548Mc76/fnnn/nnn38AGDhwIPb29gBcu3aNNWvWAHD06FEAYzJVtGhRunbtaqzzXcYYFipUiM8++4wZM2YQGRlJ9erV2bJlC0FBQfj5+Zksbj169GhWrVrFlStXKFasmPF4dDx//WXo3lyzZg0HDhwAYOzYsQAcP36cTp060alTJ0qWLElYWBibN2/m4MGD9O7dO8a6kGnF8/Dn/Hj6xwRvb5daLDWWLD+xXBJDIYSIhex8Eo9HjyBfvrSVFL5p5Urw9jZ3FHF7/vw5uXLlws7OjsePHxuTJT8/P7p06ULXrl1ZvXp1ouosVqwY1+LoR38z8dq3bx/16tWLtZy7u/tbt9NLKL1ez7Rp01iyZAm3b9+mVKlSjB49ms6dO5uU8/HxiTUxfNv+zdGP5pUrVxg5ciR//PEHd+7cwcLCgrJly/LJJ5/Qu3fvFNkDOjms/HMl3bd2N3cYsbK0sOTesHvkss1l7lCEECJNkcQwHosXQ//+aXNhaQsLqFPHsDWfEGlN3RV1OXjjoNknncRGg4ZFzRbRt1pfc4cihBBpShqZTpF87t+/T/78+fH19TUeCw4OxsrKil9//TXR9a1ebVgiJi3S62H/frh509yRCGHq5rObBF0PSpNJIRgSw9UnE9dSLYQQmUGGG2Po6OjI8uXLadWqFY0aNaJMmTJ07dqVAQMG8OGHHzJw4ED8/PyoVKlSvF2JOh0cP27+JWric+wYODmZO4qkefHiRbx7ADs6OpqM2xNp19FbR80dwlvp0XPs9jF0eh1aC/meEkKIaBmuxRCgadOmfPLJJ3Tu3Jm+ffuSNWtWpkyZAkCfPn1Yu3Ztguq5dAkiIlIy0qSztISQEHNHkXQzZ86Md6mYGzdumDtMkUAhd0OwtEjbf3dG6CL4+/Hf5g5DCCHSlLT9kzsJZs6cSYUKFdi4cSPHjh0zLvVRoUIFHjx4kKA60kPCpVT6iDM+3bp1i3dbvPz586dSNCKpQu6GpNlu5DeF3A2hdJ7S5g5DCCHSjAybGP7999/cunULvV7P1atXqVixYqLrCAlJe8vU/JdOZ+hKTu9KlChBiRIlzB2GSCbHbh9L84mhpYUlIXdDaFuurblDEUKINCNDJoYRERF06dKFDh06UKZMGXr16sWpU6fImzdvouq5ejVtzkb+L+lhFWnNP8/+MXcI8VJKceXJFXOHIYQQaUqGTAy/+OILnj59yrx588iWLRs7duygR48e/PLLL4mq59Wr5J548hToAgwFqidbrZGR8OJF2p09LTIXpRSRryKTt9LfgaNAN8AhearUKz3hUeHJU5kQQmQQGS4x3LdvH3PmzCEwMJAcOXIAht0k3n//fRYvXszVq1dZu3Ytjx49omTJkvzvf/+jSJEisdYVEZHcLYZfAL+8fiWv7NmTvUoh0p5FgDtQC7BKWlUKRYQujc8uE0KIVCYLXL9Fu3awaVNyJod3AZlAIUSSZQc+BCrxzmsraNDQtlxbNrTbkIyBCSFE+pbhWgyTk42NYXcRnS65aswLXAYavP7v+8BOIEeSarW0hCdPkhqbEMkn59ScSd8jORxYi+HvqVxAV+Af4FcMozK2AIeBRkDxxFdvobHA2tI6aTEKIUQGI4nhWxQrltzj9jQYfoPtxtAXdhLDoKntJKVfrEgRyJo1OeITInkUdiictIkdUcBPGJLCrBiSwtyvX2UxjDkMAm4Dq4AyQEMSNf5Qo9FQPOc7ZJRCCJGBZcgFrpNLpUoptVSNM7ADw2+8vUAP4N1muWi14OKSfJEJkRxcCrhgoXnHHy96YCtwBcgCeGFICKNlAdyAQRjmcGmA8xjGH+4AXibsMlH6KCrlq/RuMQohRAYlieFbVErR3xnVAH8MjbZ+wKh3qkWjSek4hUi8SvkqvXtiuBc4heGnUwcgru0eswHNgH5AaQwJ5RFgHnAQQ6tjAuIUQgjxL0kM36JkSbBK4szHt/MEvn/97xnA3ETXEBUliaFIeyrlq/RuYwwPAcGv/90CKJmAcxwxtCp2A/JhGJu4B1gAnAbimDxmrbXGOZdz4mMUQogMTBLDt9BqoWpVwwSUlOMN+L7+9xDgx0TXIF3JIq2pVrBa4k86Bex6/e8PgcqJPL8E0AdoiaE18QmGRvllwH8WgbfAgqoFqqK10CY+TiGEyMAkMYxHt26psfvJKKA/hqaNbkBggs6ysIC6dcEprq42IczEKYcTbkXcEt6dfBnY/PrfNYC3b5sdNwugCobxhx4YxiP+gyE53AA8MhRTKLq93+0dLyKEEBmXJIbx6NDB0HKYsjQYupHbABFAKwwzlt9Or4cePVI0MCHeWY8qPRK2X/Id4AcMYwTLYRhhkdTVAKwwJIYDMSSKAGeAhcAu0IZr6VC+QxIvIoQQGY8khvHInRtatjSsFZiytBgWbXMDngFNgGtvPcPGBtq0Sem4hHg3bcq2wUZr8/ZCjzF820cARYGPSd6fSjkwdC33xdDVrAMOgcUCC9Z+v5bIyGTeuk8IIdI5SQwToGfPlFq25r9sMKzTUR7DAm2ewMNYS1paQseOkC1basQlROJlt85OhwodsLSI46+qUAxJ4QsME0g6Yuj6TQn5MayF6AU4QMTzCAYNGkSFChXYunUrsgGUEEIYyJZ4CaAUeHhAcHBqJYg3MCyAfRNwxbB+h61JCWtruHDBsLi1EGnVtSfXKLOgDOG6cNMPIoDVGMb/5QB6AvYpH4+lhSWuBV3pGNGR8ePHc//+fQDc3d2ZNWsWLjKTSwiRyUmLYQJoNLBokWFMX+ooDAQAOTGs3dGJNxdl02hg3DhJCkXaVzRnUcbWHYvmzUGDOmAThqTQBuhCqiSFAEopFrdYzKeffsqlS5cYNWoU1tbW/Pbbb1SrVo1u3bpx48aN+CsSQogMShLDBCpfHgYPTumla95UAUO3svXr/w4AFBYWhoRw6NDUikOIpBnmOowi9kUMM5QVht1JzmMYVtsJwxbiqcBCY8HgmoMp51gOgBw5cjBlyhTOnz+Pl5cXAGvWrKF06dKMHTuW58+fp05gQgiRhkhXciI8ewZly8Ldu6DTpdZVNwHtMPxGnYhGM46AAGjUKLWuL0TS7bq0C08/T9iH4QXQHsMs5FSg1WjJly0fZ/ufJYd1jljLHDlyhKFDh3LgwAEA8uXLx8SJE+nRoweWKT/7TAgh0gRpMUyEHDlgxw7IksXQnZs62mDY4wvgS1q3XiZJoUh3GpdsTOvQ1v8mhU1JtaRQg4Ys2izs7LwzzqQQoEaNGuzfv59Nmzbh7OzM3bt36dOnD1WqVGHXrl1xnieEEBmJJIaJ9P77sGmToUs59ZLDAUTvpbxlSx+2b9+eWhcWIln88ssvbJ211fCmDoZFrFOBBg1aCy2b2m9K0L7IGo2G1q1bc+bMGb755hty5crF6dOn8fT0xNPTk9OnT6dC1EIIYT7SlfyO/P2hfXtDgpga3cqdOiksLX1Ys2Y1tra2BAYGUrNmzZS/sBBJ9Pvvv1O/fn3CwsLo1q0bkc0jWf/X+hS/roXGAqUUG9ptoG25tu9Ux6NHj5g8eTILFiwgMjISCwsLevbsycSJE8mfP38yRyyEEOYniWES/PabITl8+DBlkkMLC8NSOZMnw6hRoNNF0rx5c3bt2kWePHkIDg6mdOnSyX9hIZLJ+fPnqV27Ng8fPsTT05Nt27ahtdQyJWgK4wLHodFoErY7SiJpNVry2OVhY7uN1C1aN8n1Rc9g3rRpEwDZsmVj5MiRfP7559jZ2SW5fiGESCskMUyi+/ehSxfYvTt569VqwcEBNm4EN7d/j7948QIPDw+OHTtGsWLFOHTokLRciDTp9u3buLq6cvXqVapVq0ZgYCDZ3liRPehaEO02tuNB6AN0Knn/smpUohFrW6/FMatjstZ74MABhg4dypEjRwAoVKgQX3/9NV26dMEi9ZYsEEKIFCM/yZLI0RF27oRvvoFcuQzjDpMy9lCrNby8vOD0adOkEAwtFdu3b8fZ2ZmrV6/StGlTnj17lrSbECKZPXv2jKZNm3L16lWcnZ3Zvn27SVII4FbUjdP9TuNV0QsLjQVazbtvSq55/b9cNrn4pvE37OyyM9mTQoA6depw6NAh1q1bR5EiRfjnn3/w9vamevXq7Nu3L9mvJ4QQqU1aDJNRWBisWgVTp8K1a4Zt6xKyU0p0ORsb6NsXhgyJf/HqS5cu4erqyv3792nQoAHbt2/HysoqeW5EiCSIiIigadOm/Prrrzg6OnLo0CGcnZ3fes61J9f45vdvWHJsCa+iXmFpYUmUPv6HJ7pcUfuijKozCu/3vbHNYhvveckhLCyMuXPn4uvra1zzsEWLFkyfPp0yZcqkSgxCCJHcJDFMATodBAQYxiAeOADHjkFEROxlS5aEunWhVi1o3Rpy5074df744w88PDwIDQ2lc+fOrF69WrqzhFnp9Xq6du3KunXryJo1K/v27aNatWoJPv9h6EM2n9vMoRuH2H99P5ceXYq1nJXWCpcCLtQpUgf3ou54lvREa/HuLY5Jce/ePSZMmMDSpUvR6XRYWlrSt29fxo8fj4ODg1liEkKIdyWJYSqIiIAzZ+DJE0OropUV2NnBe+8Zup+TIiAggObNmxMVFcXw4cOZPn16ssQsxLsYPnw4M2fOxNLSkp9//hlPT88k1fc47DHnHpwjNDKUCF0EtllsyWmTk3KO5bDSpq0W8rNnzzJ8+HDjclL29vZ88cUXDBo0CGtrazNHJ4QQCSOJYQawatUqfHx8AJgzZw6DBw82b0AiU5ozZw5DhgwBDN+T3bp1M3NE5vHrr78ydOhQTp48CUCxYsWYNm0a7dq1Q5N6i58KIcQ7kX7HDMDb2xtfX18AhgwZwoYNG8wckchsfvzxR2NSOGXKlEybFAJ8+OGHHDt2jOXLl1OgQAGuXr1Khw4dqF27NocOHTJ3eEII8VaSGGYQo0aNon///iil6Nq1K4GBgXGWffHiBePHj8fT05PcuXOj0WhYuXJljHJ6vZ6VK1fSokULChcuTNasWalQoQKTJ0/m1atXyRa7Xq9n+vTpFC9eHBsbGypVqsT69QlbAPnXX3+lR48elC5dGjs7O0qUKEGvXr24fft2ssUn3i4wMNCYCA4YMICRI0eaOSLz02q1dO/enYsXLzJhwgTs7Ow4dOgQrq6udOjQgStXrpg7RCGEiJ0SGUZUVJRq06aNAlSOHDnUyZMnYy135coVBagiRYooDw8PBagVK1bEKPf8+XMFqA8++EBNnjxZLV26VHXv3l1ZWFgoDw8PpdfrkyXuUaNGKUB98sknaunSpapZs2YKUOvXr4/3XBcXF1W8eHE1YsQI9d1336nRo0er7Nmzq3z58qnbt28nS3wibn/++afKkSOHAlTbtm1VVFSUuUNKk27evKm6d++uNBqNApSVlZUaNmyYevz4sblDE0IIE5IYZjBhYWHKzc1NAapgwYLq2rVrMcq8evXKmDT98ccfcSaG4eHh6uDBgzGOf/XVVwpQe/bsSXK8//zzj8qSJYvq37+/8Zher1dubm6qUKFC8SYav/32m9LpdDGOAeqLL75IcnwiblevXlUFChRQgKpbt64KCwszd0hp3okTJ9SHH36oAAWoPHnyqHnz5qmIiAhzhyaEEEoppaQrOQ0LCQlBo9Gwbds247Fjx46h0WioWrWqSdkmTZpQs2ZNbGxs2Lp1K+XLl+fWrVt4enry6NEjk7LW1tYJ2i3FysoKV1fXGMc//vhjwDALM6m2bt1KZGQk/fr1Mx7TaDR8+umn/PPPP/GOyapbt26MJXrq1q1L7ty5kyU+EbvoLe5u375N+fLl2bJlCzY2NuYOK82rXLkye/bs4ZdffqFs2bI8fPiQQYMGUaFCBbZu3YqSuYBCCDOTxDANq1ChAjlz5mT//v3GY0FBQVhYWHDy5Enjjid6vZ7g4GDq1jXsCZsrVy527tyJk5MTZ8+epXnz5oSFhSVbXHfu3AGIsUbbgwcPEvQKDw83nnPixAmyZs1K2bJlTeqqUaOG8fPEevHiBS9evJA15FJIWFgYLVq04Ny5cxQqVIiAgAByJXXdpUxEo9HQrFkzQkJCWLx4MY6Ojly4cIFWrVpRv359jh8/bu4QhRCZmCSGaZiFhQW1a9cmKCjIeCwoKIhWrVqh0WgIDg4GMCaJbm/sn1e4cGECAgLImTMnwcHBeHl5odMlz36006dPJ0eOHDRp0sTkuKOjY4Jeb04suX37Nvny5YuxjEeBAgUAuHXrVqLjmzNnDhEREXTo0OEd7k68TVRUFJ06dSI4OJicOXMSEBBAoUKFzB1WuhS9EPalS5cYNWoU1tbWxgXBvb29+eeff8wdohAiE5LEMI1zc3Pj+PHjvHz5EoADBw7QtGlTKleubEwYg4KC0Gg01KlTx+Tc6O4pa2trtmzZwoABA5LcVeXr68vevXuZOnUqOXPmNPlsz549CXo1btzYeE5YWFisi/9Gd0smtqVz//79fPXVV7Rv35769esn/gZFnJRSDBgwwPg9tW3bNsqXL2/usNK9HDlyMGXKFM6fP4+XlxdKKVavXk3p0qUZN26ccbs9IYRIFeYd4ijic/DgQeNEj3PnzilAnT9/Xg0ZMkS5ubkppZRq27atKl++fJx1+Pv7G2dDTpo0yeSzt00++a8ffvhBaTQa1bNnzyTd05uaNWumSpQoEeP4y5cvFaBGjRqV4LrOnj2rcufOrSpXrqyePXuWbDEKg4kTJypAaTQatWnTJnOHk2EdPnxY1alTxzhBJV++fGrp0qUy41sIkSqkxTCNq1atGjY2Nuzfv5+goCDy5s1L6dKlcXNz48iRI4SHhxMUFGTSjfxfbdq0Yd68eQCMGzeO5cuXJzqOPXv20K1bN5o1a8a3334ba5k7d+4k6PVmK2CBAgW4c+dOjJbM6HUICxYsmKD4bty4QaNGjbC3t2fHjh1kz5490fco4rZs2TK+/PJLAObPn0/r1q3NHFHGVaNGDfbv38+mTZtwdnbm7t279O7dm8qVK7Nr1y5zhyeEyOjMnZmK+NWtW1d5eHiobt26qTZt2iillLp//74C1LJlyxSg1q1bF2890esFarVa9csvvyilEtZi+Pvvv6usWbMqV1dXFRoaGmc5XrdwxPd681oLFixQgPrrr79M6vLz81OA2r9/f7z39eDBA/Xee++pvHnzqgsXLsRbXiTOL7/8orRarQLU6NGjzR1OphIeHq6++eYblStXLuPz07hxY3Xq1ClzhyaEyKBkr+R0YOzYscyePRsHBweGDh1q3Au5XLly6HQ6Lly4wI0bN+KdBKCUwsfHh9WrV2NnZ0dgYCAWFhZUr16dFStWGPdbftPZs2dxc3Mjf/78BAUFvXX26d69exN0P+XLlzdOLvnnn38oUaIEvXv3ZsGCBcY43d3duXz5MteuXUOr1QKGVsSnT5/i7OxMlixZAHj58iX169fn7NmzBAYG4uLikqAYRMIcPnyYevXqERYWhre3NytWrJD9fs3g0aNHTJ48mQULFhAZGYmFhQU9e/Zk4sSJCVp6SgghEkoSw3Rg165deHp6AoZ1DKPXMOzbty9LliyhWLFiCd5iKzIykubNm7Nr1y7s7Oxo2bIl69evp3Xr1lSpUgWAgQMHYm9vz/Pnzylfvjw3b97E19cXJycnk7qcnZ2pVatWku9vxIgRzJgxg969e1O9enW2bNnC9u3b8fPzw8vLy1jOx8eHVatWceXKFYoVKwZAq1at2Lp1Kz169KBevXom9WbLlo1WrVolOb7M6sKFC7i6uhrXLNy2bZsxIRfmcenSJUaOHMlPP/0EGL7HR44cyeeff46dnZ2ZoxNCZAjmbK4UCfPs2TOl1WpV9uzZTQagr127VgGqa9euiarv+fPnysrKKs6u3itXriil/t06L66Xt7d3styfTqdTvr6+qmjRosrKykqVL19erV27NkY5b29vk/iUUqpo0aJxxle0aNFkiS8zun37tipWrJgCVLVq1dTz58/NHZJ4w/79+1X16tWN3+uFChVSq1atirELkBBCJJa0GGZSd+/epXbt2vz9999UrVqVffv2yYQNAcCzZ8/w8PDgxIkTODs7ExwcTN68ec0dlvgPvV7PDz/8wOjRo7l+/ToAVatWZdasWXh4eJg3OCFEuiWzkjOpfPnyERAQgKOjI8ePH6dNmzZERESYOyxhZhEREbRp04YTJ07g6OjIrl27JClMoywsLPDy8uLcuXNMmTKF7Nmzc/z4cerVq0erVq24cOGCuUMUQqRD0mKYgURvBfc2jo6OxskcAH/88QceHh6EhobSuXNnVq9eHWPvYZE56PV6unbtyrp168iaNatxFw6RPty7d48JEyawdOlSdDodlpaWfPrpp3z55ZeyPaQQIsEkA8hAZs6cSYECBd76unHjhsk51atXx9/fH61Wi5+fH6NGjTJT9MLcRo4cybp167C0tMTf31+SwnQmb968LFq0iJCQEJo1a0ZUVBTz58+nZMmSzJw502SPciGEiIu0GGYgly9f5vLly28tU6dOHeN2c29auXIl3bt3Bwx7DUcviSMyhzlz5jBkyBDA8L3g7e1t5ohEUu3du5dhw4Zx8uRJAIoXL87UqVNp166dLDkkhIiTJIbCyNfXly+++AKNRsMPP/xA+/btzR2SSAU//vgjHTt2BAzfA6NHjzZzRCK56HQ6Vq9ezRdffGHcTahWrVrMmjWLWrVqMWHCBL766ivu378v3c1CCEC6ksUbRo8eTb9+/VBK0bVrVwIDA80dkkhhgYGBdOvWDYD+/fvLUIIMRqvV0r17dy5evMiECROws7Pj0KFDuLq60qFDBx4/fpzgunbv3k3Pnj2pUKECWq3WuJZocgoPD2fkyJEULFgQW1tbatasyZ49e96proYNG6LRaBgwYEAyRylExiaJoTDSaDTMmzeP1q1bExERQatWrQgJCTF3WCKFnDx5klatWhlnIs+dO1e6GDOorFmzMn78eC5evEj37t3RaDRs2LDBuNvQ06dP461j3bp1rFu3Dnt7+wTvYZ5YPj4+zJ49m86dOzN37ly0Wi1NmzblwIEDiarnp59+4tChQykSoxAZntlWUBRpVmhoqKpTp44CVMGCBdW1a9fMHZJIZlevXlUFChRQgHJzc1NhYWHmDkmkohMnTqgPP/zQuEB2rly51Pz581VERESc59y8edP4ebNmzZJ9AfnDhw8rQM2YMcN4LCwsTDk7O6tatWoluJ6wsDBVrFgxNXHiRAWo/v37J2ucQmR00mIoYrC1tWXbtm2UK1eOW7du4enpyaNHj8wdlkgm0Vvc3b59m/Lly7N169ZYJySJtCMkJASNRsO2bduMx44dO4ZGozFukRmtSZMm1KxZ8631Va5cmT179tCpUycAHj9+zMCBA7GxsSFr1qx0796d0NBQk3MKFiyYolsiRq+O0Lt3b+MxGxsbevbsyaFDh2KsqBCX6dOno9frGTZsWEqFKkSGJomhiFWuXLkICAjAycmJs2fP0rx5c8LCwswdlkiisLAwWrRowblz53BycmLnzp3kypXL3GGJeFSoUIGcOXOyf/9+47GgoCAsLCw4efIkz549AwxrUQYHB1O3bt1469RoNJQuXRqAwoULY2VlhV6vJzQ0lJUrV9KvX793ilWv1/PgwYMEvSIjI43nnThxgtKlS5MjRw6T+mrUqAHAn3/+Ge+1r1+/ztSpU5k2bRq2trbvFL8QmZ0khiJOhQsXJiAgAHt7e4KDg/Hy8kKn05k7LPGOoqKi6NSpE8HBweTMmZOAgAAKFy5s7rBEAlhYWFC7dm2CgoKMx4KCgmjVqhUajYbg4GAAY5Lo5uaWqPobNmzIvXv3GDVqFNbW1gCsWrUKb29v/vnnn0TVdf36dRwdHRP0OnjwoPG827dvU6BAgRj1RR+7detWvNceOnQoVapUMc6yF0IknqW5AxBpW4UKFdi6dSuNGjViy5YtDBgwgEWLFskkhXRGKcWAAQPYunUr1tbWbN26lQoVKpg7LJEIbm5ujB07lpcvX5I1a1YOHDiAr68v165dIygoCE9PT4KCgtBoNNSpUydRdfft2xd7e3umTJlC3759ad26NcePH2f16tVs3LiRoUOHMmLEiATtp54/f/4EzyR+//33jf8OCwszJqVvih7mEF+PRWBgIJs2beLw4cMJurYQInaSGIp4ubu74+fnR/v27fn2229xcnJi7Nix5g5LJMLkyZNZsmQJGo0GPz+/BHU1irTFzc2NqKgoDh06ROHChbl37x5ubm789ddfxpbEoKAgypUrR+7cuRNVd5EiRYz/Llq0KAMHDqR79+64uLhw7NgxJk+ezHfffcekSZPo0aPHW+uysbGhQYMGib4/W1vbWHdnefXqlfHzuERFRTFo0CC6du1K9erVE31tIcS/pCtZJEjbtm2ZO3cuAOPGjWP58uVmjkgk1LJly/jyyy8BmDdvHm3atDFzROJdVKtWDRsbG/bv309QUBB58+aldOnSuLm5ceTIEcLDwwkKCkp0NzJgsn/6mzZu3Ii/vz/Ozs7cvXuX3r17U7lyZe7fvx9nXTqdjjt37iToFRERYTyvQIECxkW43xR97G1L5KxevZrz58/Tp08frl69anwBPH/+nKtXr8aYTCOEiJ0khiLBBg4cyMiRIwHo3bs327dvN3NEIj7bt2+nT58+AIwaNUoW+03HrKysqFGjBkFBQSYJoJubG+Hh4fj5+XH37t1kbQ3WaDS0adOGM2fOMHv2bHLlysXp06c5cuQId+/e5fTp0zHOuXHjRrx7tke/osdGgmGm9IULF4wTaaJFdw1Xrlw5zjivX79OZGQktWvXpnjx4sYXGJLG4sWLs3v37mT4igiR8UlXskiUKVOmcOvWLdasWUP79u0JDAw0zhoUacvhw4dp164dOp2Obt264evra+6QRBK5ubkxe/Zs/v77b4YOHQqAg4MDZcuWZdq0acYyyc3KyoohQ4bg7e3NpEmTmDt3Lq9eveL999+nZ8+eTJw4kfz58wPvPsawbdu2zJw5k6VLlxqXmgkPD2fFihXUrFnTZKLU9evXCQ0N5b333gOgY8eOsSaOH3/8MU2bNuWTTz6JdwkfIcRr5l5IUaQ/ERERqnHjxgpQDg4O6vz58+YOSfzH+fPnVZ48eRSgGjdu/NaFi0X6ERAQYFyU+tixY8bjffr0UYAqVqxYouobP368AtT9+/dNjq9YsUIB6sqVK8ZjJ0+eVJMmTVKTJk1SJUqUUFmyZDHGYmNjoyZPnqxevnyZpPtr166dsrS0VMOHD1dLlixRrq6uytLSUv32228m5dzd3VVCfn0hC1yL8eOVAqX+8z0u4iZdySLRsmTJgr+/Py4uLjx48ABPT0/u3Llj7rDEa3fu3KFx48Y8fPiQatWq4e/vn6ILE4vU4+rqilarJXv27CatbW92K6eU48ePM27cOMaNG8fly5dN1iB89eoVY8eOpUyZMqxZswa9Xv9O11i9ejWfffYZa9asYdCgQURGRvLLL7/IZCmROZw9C56ekC0b5M4NXbvCW8bzxvD8OYwYAcWLg7U1ODlB27aQyPG1GqWUSmToQgBw9+5dateuzd9//03VqlXZt29fgpazECnn2bNneHh4cOLECZydnQkODiZv3rzmDktkYHq9nh9++IHRo0dz/fp1AFxcXJg1axbu7u5mjk5kehMmwFdfGRIsBwdzRxO3f/6BKlXA3h4GDYIXL2DmTChSBI4cASurt5//9Cm4uxvq6d0bSpY03HNQEKxZA4nZyMDcTZYifbt48aJydHRUgGrYsKEKDw83d0iZVnh4uGrQoIEClKOjo7p06ZK5QxKZSGhoqJoyZYrKnj27sYu5ZcuWMtREmFd66Ur+9FOlbG2Vunbt32N79hhiX7IkYefnzKnU5ctJDkW6kkWSlCxZku3bt2NnZ8eePXvo2bPnO3cjiXen1+vp0aMHe/fuJWvWrOzYsQNnZ2dzhyXM5MWLF/EuFZPcuxjZ2toyatQoLl26xKeffopWq2Xr1q2UL1+eQYMG8fDhw2S9nhCJ8uABtG8POXJAnjwweDC8XiMzXitWgEYDJ07E/MzXF7RauHkzafFt2gQffWRoIYzWoAGULg0bNrz93CdPDDH27m3oRo6IgFjWBE0oSQxFklWvXh1/f3+0Wi1r165l9OjR5g4p0xk1ahR+fn5YWlri7+9PtWrVzB2SMKOZM2fGu1TMjRs3UuTaefPmZdGiRYSEhNCsWTOioqKYP38+zs7OzJw5M9ZFrIVIce3bGxLBKVOgaVOYN8+QSCVE27Zgawt+fjE/8/MDDw/DeD4wjOd78CD+1+PH/9Zx8ybcuwex/dyuUSP2hPRNBw4Y7q1kSUOsdnaGeGvXhgTsMR5DktschXgteiYjoObOnWvucDKNOXPmGL/uK1euNHc4Ig34+++/1Z49e976CgsLS5VY9uzZoypVqmT8Hi1evLjasGGD0uv1qXJ9kclFdyW3aGF6vF8/w/GTJxNWT6dOShUsqJRO9++x48cNdaxYEfN68b2KFv33nD/+MBxbvTrmdYcPN3z26lXcsc2ebSiTJ49SNWoo5een1KJFSuXLp1SuXErdupWwe3xN1jEUycbHx4dbt27xxRdf8Nlnn5E/f37at29v7rAytA0bNjBkyBAAfH198fb2NnNEIi0oUaIEJUqUMHcYADRo0IDjx4+zatUqxo4dy5UrV2jfvj2urq7MmjWLDz74wNwhisygf3/T9wMHwqJFsGMHVKoU//ndusH69RAYCB9+aDjm52domXtzN6lu3SAhe5W/ucVj9D7gsewVzuu9wgkLi/1zMExUAUN396+/GmY1g2EyS61asHAhTJ4cf0yvSWIoktXo0aO5efMmixYtomvXruTNmxcPDw9zh5UhBQYG0rVrV5RS9O/fn1GjRpk7JCFipdVq6dGjB+3bt2fmzJnMmDGD4OBgatWqRYcOHZgyZYpxpxIhUkSpUqbvnZ3BwgJeb50Yr4YNoUABQzL44Yeg1xsSxZYt4c3VOEqUMLwSIzpJjG2YRfQ4yLfsFW78rHnzf5NCgA8+MIw5fGOHoYSQMYYiWWk0GubNm0fr1q2JiIigVatWnDp1ytxhZTghISG0atWKiIgI2rRpw9y5c9FoNOYOS4i3ypYtGxMmTODChQt0794djUbDjz/+yHvvvceIESN48uSJuUMUmUVif15qteDlZZgk8uqVoeXw1i3o0sW03IsXcOdO/K831ycsUMDw31j2Cuf2bcOahnG1FgJE7yOeL1/Mz/LmNR3PmACSGIpkFz0JpU6dOjx9+hRPT0/j+mYi6a5fv06TJk149uwZbm5urF27Fq1Wa+6whEgwJycnli9fzvHjx/nwww+JiIhgxowZlCxZkgULFpgsni1Esrh40fT9pUuGVr9ixRJeR7du8OwZ/PyzoeXQ0REaNzYtM3OmIdGL71W9+r/nODkZ6jp6NOY1jxyBt+wTDoCLi+G/sc2MvnXLUHciSGIoUoStrS3btm2jXLly3Lp1C09PTx49emTusNK9R48e4enpya1btyhfvjxbt27FJnoMihDpTOXKldmzZw+//PIL7733Hg8fPmTgwIFUrFiRbdu2oWT/BZFcFi40fT9/vuG/TZokvI5KlQyv7783tBx27AiW/xmR160b7NkT/+u/M5zbtIFffoE3Vwv49Ve4cAHatfv3WGQknDtn2rpYpgy8/z5s3WqY8Rxt925DfQ0bJvweQWYli5R1/fp15eTkpABVu3ZtFRoaau6Q0q3Q0FDl6uqqAOXk5KSuX79u7pCESDYRERFq4cKFysHBwTiD2cPDw2RPaCESLXqWcMWKSjVvrtTChUp16WI45uWV+Ppmzvx3ZvHhw8kX5/XrhlnFzs5KzZunlK+vYUZxxYqmM5KvXDFc29vb9Pz//U8prVapMmUMs5THj1cqe3alSpdW6vnzRIUiiaFIcadOnVL29vYKUK1atVJRUVHmDindiYqKUq1atVKAypkzpzp16pS5QxIiRTx58kSNGjVKWVtbK0BpNBrVrVs3dePGDXOHJtKj6MTwzBml2rY1JEu5cik1YIBS77Jk0+3bhgSsdOlkD1WdPq1Uo0ZK2dkZdjHp3FmpO3dMy8SVGCpl2Cnlgw+UsrFRKndupbp2NcSbSLJXskgVv/32G40aNSIiIoJPP/2UhQsXymSJBFJK0a9fP7799lusra3ZvXs3devWNXdYQqSoa9euMWbMGNatWwcYhqcMHTqUkSNHku3NmZdCpKYHDwxjBL/8EsaNM3c0KULGGIpU4e7ujp+fHxqNhsWLF+Pr62vukNKNr7/+mm+//RaNRoOfn58khSJTKFq0KH5+fhw+fJjatWsTFhbG5MmTKVmyJN9//32yb+knRIKsXAk6HXTtau5IUoy0GIpUNX/+fAYNGgTA8uXL6d69u5kjStuWL19Oz549AcPXbsCAAWaOSIjUp5Tip59+YuTIkfz9998AVKxYkZkzZ9KoUSMzRyfStadP/11gOi7588P//gdnzhhaCevVg59+Sp34zEASQ5HqRo0axbRp09BqtWzbto2mTZuaO6Q0afv27bRs2RKdTseoUaOYMmWKuUMSwqwiIiJYuHAhkyZN4vHrtdk8PT2ZOXMm5cuXN3N0Il3y8YFVq95eRinDfsjBwYb9h9eu/Xdv5AxIEkOR6pRSeHt7s2bNGuzs7AgMDKRGjRrmDitNOXz4MPXq1SMsLIxu3bqxcuVKGZMpxGuPHj1i0qRJLFy4kMjISCwsLOjVqxcTJ04kX2yL/AoRlzNnDGv9vU2DBqkTSxohiaEwi8jISD766CN2796Ng4MDBw8epHTp0uYOK024cOECrq6uPHz4kMaNG/Pzzz+TJUsWc4clRJpz6dIlRo4cyU+vu/WyZcvGqFGj+Pzzz7F92xZiQog4SWIozOb58+fUq1ePY8eOUbx4cYKDg8mfP7+5wzKrO3fuUKtWLa5evYqLiwv79u2TGZhCxCMoKIihQ4fyxx9/AFCoUCF8fX3p3LkzFhYyx1KIxJDEUJjV3bt3cXV15fLly1StWpV9+/aR/c0NyTORZ8+e4eHhwYkTJ3B2dubgwYPSLSZEAun1etavX8/o0aO58Xr3CBcXF2bNmoW7u7uZoxMi/ZA/pYRZ5cuXj4CAABwcHDh+/Dht2rQhIiLC3GGluoiICNq0acOJEydwdHQkICBAkkIhEsHCwoLOnTtz/vx5fH19yZ49O8eOHcPDw4OPP/6YCxcumDtEIdIFSQyF2ZUqVYrt27djZ2fHnj176NmzJ3q93txhpRq9Xk+PHj3Yu3cvdnZ2bN++nZIlS5o7LCHSJVtbW0aPHs2lS5fo27cvFhYWbNmyhfLlyzN48GAePnxo7hCFSNMkMRRpQo0aNfD390er1bJ27VpGjx5t7pBSzahRo/Dz80Or1eLv70/16tXNHZIQ6V7evHlZvHgxp06domnTpkRFRTFv3jxKlizJrFmzCA8PN3eIQqRJkhiKNKNJkyZ8//33AEyfPp158+aZOaKUN3fuXGbMmAHA999/T5MmTcwckRAZS7ly5di+fTt79uyhUqVKPHnyhGHDhlG2bFk2btyIDLMXwpQkhiJN8fHxYfLkyQB89tlnbNiwwcwRpZwNGzYwZMgQwLDtnY+Pj3kDEiIDa9CgAcePH2fZsmUUKFCAK1eu0L59e+rUqcPvv/9u7vCESDNkVrJIc5RSDBgwgEWLFmFlZcWuXbvw8PAwd1jJKjAwEE9PTyIiIujXrx8LFiyQBayFSCUvXrxg5syZzJgxg9DQUAA6dOjAlClTKF68uJmjE8K8JDEUaZJOp6Ndu3Zs3rwZe3t7goKCqFixornDShYhISG4ubnx7NkzWrduzYYNG9BqteYOS4hM5+bNm4wbN46VK1eilMLKyorBgwczZswYcubMae7whDALSQxFmhUWFkbDhg05ePAgBQsW5NChQxQpUsTcYSXJ9evXqVWrFrdu3aJOnTrs3r1bdmgQwsz+/PNPhg0bxq+//gpAnjx5mDBhAn369JFdh0SmI4mhSNMePXpEnTp1OHv2LGXLluXAgQPkzp3b3GG9kzfvpVy5chw4cIBcuXKZOywhBIYhLDt27GDYsGGcO3cOgDJlyjB9+nSaN28uQz1EpiGTT0Saljt3bgICAnBycuLs2bO0aNGCsLAwc4eVaGFhYTRv3pyzZ8/i5OREQECAJIVCpCEajYZmzZoREhLCwoULcXBw4Pz587Rs2ZIPP/yQEydOmDtEIVKFJIYizStSpAg7d+7E3t6egwcP4uXlhU6nM3dYCabT6fDy8iI4OBh7e3sCAgIoXLiwucMSQsQiS5Ys9OvXj0uXLjFy5Eisra0JDAzExcUFHx8fbt68ae4QhUhRkhiKdKFixYps3boVKysrtmzZwsCBA9PF+mPRM6y3bNmCtbU1W7dupUKFCuYOSwgRD3t7e6ZOncq5c+fo1KkTSilWrVpFqVKl+PLLL3nx4oW5QxQiRUhiKNINd3d3/Pz80Gg0LF68GF9fX3OHFK+vv/6ab7/9Fo1Gg5+fH+7u7uYOSQiRCMWKFWPdunX8/vvv1K5dm7CwMCZNmkSpUqX4/vvv01XvhRAJIZNPRLozf/58Bg0aBMDy5cvp3r27mSOK3fLly+nZsydgiHnAgAFmjkgIkRRKKX766SdGjhzJ33//DRh6M2bOnEmjRo3MHB0QFQWnTsGhQ/D773DtGrx8afgsa1YoWhQ++ABq1YKKFcHS0rzxijRJEkORLo0aNYpp06ah1WrZtm0bTZs2NXdIJrZv307Lli3R6XSMGjWKKVOmmDskIUQyCQ8PZ9GiRUycOJEnT54A4OnpycyZMylfvnzqB/T77zBtGgQEwKtXoNGAVmtIFN9kaQk6HSgFNjbg6QkjRxqSRSFek8RQpEtKKby9vVmzZg12dnYEBgZSo0YNc4cFwJEjR6hXrx6hoaF069aNlStXylIXQmRADx8+ZNKkSSxcuJCoqCgsLCz45JNP+Oqrr8iXL1/KXlyvhx07YMoUCA42JH3/TQTjE32OqyuMGQNNmoCFjDDL7CQxFOlWZGQkH330Ebt378bBwYHg4GBKlSpl1pguXLhA7dq1efDgAY0bN+bnn3+WBXKFyOAuXrzIyJEj2bx5MwDZsmVj9OjRDBkyJGUWsL9/H7p0gd27DS2DSR3nGF1Ho0awdi04OiZPnCJdksRQpGvPnz+nXr16HDt2jOLFi3Po0KGU/0s9Dnfu3MHV1ZUrV67g4uLCvn37yJYtm1liEUKkvv379zN06FCOHj0KQOHChfH19cXLywuL5GqJ278f2rWDhw+TnhD+l6Ul5M4NGzdC3brJW7dIN6TNWKRr2bNnZ/v27ZQoUYIrV67QtGlTnj9/nupxPH/+nGbNmnHlyhWcnZ3Zvn27JIVCZDJ169bl8OHDrF27lsKFC3Pjxg26du1KjRo1+O2335J+gRkzwMMjZZJCMHQrP3hguMaMGclfv0gXJDEU6V6+fPkICAjAwcGB48eP07ZtWyIiIlLt+hEREbRp04bjx4/j6OhIQECA2VothRDmZWFhQefOnTl//jy+vr5kz56dY8eO4eHhwccff8yFCxfereJp02DECMPEkZRcIkevN1xjxAiYPj3lriPSLEkMRYZQqlQptm/fjp2dHbt376ZXr16psgC2Xq+nZ8+e7NmzBzs7O7Zv307JkiVT/LpCiLTN1taW0aNHc+nSJfr27YuFhQVbtmyhfPnyDB48mIcPHya8suXLYdSolAs2LiNHGq4tMhVJDEWGUaNGDfz9/dFqtaxZs4bRo0en+DVHjx7N2rVr0Wq1+Pv7U7169RS/phAi/cibNy+LFy/m1KlTNG3alKioKObNm0fJkiWZNWsW4eHhb6/gt9+gd+/UCTY2ffoYYhCZhkw+ERnOypUrjYtez5s3j4EDB6bIdebNm8fgwYMBWLFiBT4+PilyHSFExrF3716GDh1KSEgIACVKlGDq1Km0bds25rJWL19CqVJw966hi9ccLCwgXz64dAns7MwTg0hV0mIoMhwfHx8mT54MwODBg9m4cWOyX2PDhg189tlngGHbO0kKhRAJ0aBBA44fP86yZcsoUKAAly9fpn379tSpU4fff//dtLCvr3mTQjBc++5dQywiU5AWQ5EhKaUYMGAAixYtwsrKit27dyfbPsX79u2jcePGRERE0K9fPxYsWCALWAshEu3FixfMmDGDGTNmEBYWBkCHDh2YOnUqxSIjoVy5xC9anVIsLeHMGUMLpsjQJDEUGZZOp6Ndu3Zs3rwZe3t7goKCqFixYpLqPHXqFG5ubjx9+pTWrVuzYcMGtFptMkUshMiMbt68ydixY1m1ahVKKaytrRlcsCBjrl/HPiVnICeGpSU0aAA7d5o7EpHCJDEUGVpYWBgNGzbk4MGDODk5cejQIQoXLvxOdV2/fp1atWpx69Yt6tSpw+7du1NmVwMhRKb0559/MnToUP73v/8B4ABMAHoDaWb/pHPnoEwZc0chUpCMMRQZmq2tLdu2baNs2bLcvHkTT09PHj16lOh6Hj16hKenJ7du3aJcuXJs27ZNkkIhRLKqXLkye/fu5ee2bXkPeAAMACoCPwNmb8XRamH1anNHIVKYtBiKTOH69eu4urpy8+bNRLf2JWeroxBCvJVOB4UKEXnnDt8B4zEkiAD1gFlAFbMFBxQoAP/8Y5itLDIk+X9WZApFihRh586d2Nvbc+DAAby8vNAlYOyOTqfDy8uLgwcPYm9vT0BAgCSFQoiUs28f3LlDFqAfcAkYCVgDgYAL4APcNFd8t28bYhQZliSGItOoWLEiW7ZswcrKii1btjBw4MC37o4SPbM5+pytW7dSoUKFVIxYCJHp7NhhmOjxmj0wFTgHdMLQnbwKKAV8CbxI7fgsLWH79hSpevXq1eTJkyfGot+tWrWia9euKXJNEZMkhiJT8fDwYO3atWg0GhYvXozvW9bm+vrrr/n222/RaDT4+fkl23I3QggRpz//jHWJmmLAOuB3oDYQBkzCkCAuA1Jt7nJUFJw8mSJVt2vXDp1Ox7Zt24zH7t27x/bt2+nRo0eKXFPEJImhyHTatWvHnDlzABg7diwrVqyIUWb58uWMGzcOgLlz59K2bdvUDFEIkVn9+edbP64JBAH+gDNwB+gFVAX2pHBoRvHE+K5sbW3x8vIy+Zm8du1aihQpgoeHR4pcU8QkiaHIlAYNGsSIESMA+OSTT9ixY4fxs+3bt9P79d6kI0eOTLEt9YQQwsTdu5CAVRM0QBvgL2A2kBMIARoBTV4fT1EPHxpiTQGffPIJu3fv5uZNwyjKlStX4uPjI5sIpCKZlSwyLb1ej7e3N2vXrsXOzo7AwEAA6tWrR2hoKF27dmXVqlXyA0kIkTp+/dWwiHQiPcTQrbwQiMLQ4vMJ8BWQLznje9Ovv0L9+ilStYuLC23btqVRo0bUqFGDq1evyqS/VGQZfxEhMiYLCwuWLVvGvXv32L17N56engCEhobSqFEjli1bJkmhECL1PH36TqflAeYA/THMYN4MLAH8gGFAM6BasgT4hneMNSF69erFnDlzuHnzJg0aNJCkMJVJV7LI1KysrPD396dixYo8fvyYx48fU6lSJfz9/cmSJc3sNSCEyAxevUrS6aWAn4DfMCxr8wLDzikfAEeTGFoMSYz1bby8vPjnn3/47rvvZNKJGUhiKMR/yOgKIUR6l577Ouzt7WnTpg3ZsmWjVatW5g4n05HEUGRqERERtGnThlOnTpErVy5y5crFqVOnaNu2LREREeYOTwiRmdjYJOn0i8DHgDuGFsLsGMYZ/k4KdCUnMdb43Lx5k86dO2NtbZ2i1xExSWIoMi29Xk/Pnj3Zs2cPdnZ2BAQEEBAQgJ2dHbt376ZXr17SeiiESD329u902kNgMFAO2ILhF3tfDInil6RAUgjvHGt8Hj9+zObNm9m3bx/9+/dPkWuIt5PJJyLTGj16NGvXrkWr1eLv70+NGjUA2LhxIy1atGDNmjUULFiQqVOnmjlSIUSmkMidlcIxzESeBDx5fawJMAMon5xxxSaFdoGqUqUKjx8/Ztq0aZQpUyZFriHeTparEZnSvHnzGDx4MAArVqzAx8fH5PMVK1YYBz3PmzdP1jIUQqSOPHniXctQAZswzEC+/PpYJWAm0DBFg3stTx548CA1riTMQLqSRaazYcMGPvvsMwAmT54cIykE6N69O5MmTQJg8ODBbNy4MRUjFEJkWpUrv/Xjw4Ab0A5DUlgAw5Z4x0mlpBDijVGkb5IYikxl3759dO3aFaUU/fr1Y8yYMXGW/eKLL+jbty9KKbp06cJvv/2WipEKITKlypXBMuYor6tAJwxLzxwEbDGMH7wA9AC0qRWfpSW8/35qXU2YgSSGItM4deoUrVq1IiIigo8//ph58+a9dQFrjUbDggULjOe0bNmSU6dOpWLEQohMp2lTiIoyvn2Kocv4PeAHDMvQ+GCYWPIVkC2144uKgmbNUvuqIhXJGEORKVy/fp1atWpx69Yt6tSpw+7du7G1tU3QuWFhYTRs2JCDBw/i5OTEoUOHZCV+IUTK0OmgUCEi79xhKYYFqqNH89UHZgGVzRQaAAUKwD//gIW0K2VU8v+syPAePXqEp6cnt27doly5cmzbti3BSSGAra0t27Zto2zZsty8eRNPT08eJWCjeyGESCxlYcHPdepQERiAISl8D/gZ2IuZk0KtFrp3l6Qwg5P/d0WGFhYWRosWLTh79ixOTk4EBASQK1euRNeTO3duAgICcHJy4syZM7Rs2ZKwsLAUiFgIkVmdOHGCDz/8kBb+/pwHHDAsRxMCfEQa2M1Ep4Nu3cwdhUhhkhiKDEun0+Hl5cXBgwext7cnICAgSV3ARYoUYefOndjb23PgwAE6d+6MTqdLxoiFEJnRzZs38fHxwcXFhcDAQKytrRlZvDiXtFr6AWli13ZLS/D0BFlbMMOTxFBkSEopBg4cyJYtW7CysmLr1q1USIYFWStWrGisc/PmzQwaNEh2RxFCvJMXL17w5ZdfUqpUKVatWoVSio4dO3Lu3Dmm7tqF/Vsmx5nF/PnmjkCkAkkMRYbk6+vL4sWL0Wg0+Pn54e7unmx1e3h4sHbtWjQaDYsWLWLKlCnJVrcQIuPT6XQsW7aMUqVKMWnSJMLCwnB1deX3339n/fr1FCtWDEqVghEj0sZ4PgsLGDkSSpY0dyQiFcisZJHhpNauJfHtniKEEP+1Z88ehg0bRkhICAAlSpRg2rRptGnTJubyWS9fGhLEu3dBrzdDtBiSwvz54eJFsLMzTwwiVaWBP0WESD47duzgk08+AWDkyJEpupXdoEGDGDFiBAC9evVi586dKXYtIUT69tdff9G0aVMaNWpESEgIOXPmZNasWZw5c4a2bdvGvqZq1qywfr15Ww0tLAwxSFKYaUiLocgwjhw5Qr169QgNDaVr166sWrXqrQtYJwe9Xo+3tzdr167Fzs6Offv2Ub169RS9phAi/bh79y7jx4/nu+++Q6/XY2lpSf/+/Rk3bhx58uRJWCXLl0PPnikbaFyWLYPXPTAic5DEUGQIFy9exNXVlQcPHtCoUSN++eUXsmRJnbl8ERERNG/enN27d+Po6EhwcDAlZSyOEJlaWFgY33zzDVOnTuX58+cAfPzxx0ybNo1SpUolvsJp02DUqGSOMgHXfN0rIjIPSQxFunf37l1q1arFlStXjMs9ZM+ePVVjeP78OR4eHhw/fpwSJUoQHBxMvnz5UjUGIYT56fV61q1bx5gxY7hx4wYALi4uzJ49m7p16yat8hkzDJNANJqUG3NoYQFKwfTpMGxYylxDpGmSGIp0LS0lZHfu3MHV1dWYoO7bt49s2VJ9J1MhhJns37+foUOHcvToUQAKFy7MlClT6NSpExbJNU5w/35o1w4ePTLZUzlZaLWQJw9s3AhJTWJFuiWTT0S6FRERQdu2bTl+/DgODg4EBASYtZUuf/787Nq1CwcHB44dO0bbtm2JjIw0WzxCiNRx8eJFPv74Y9zd3Tl69CjZs2fH19eX8+fP07lz5+RLCsGQsJ0+DfXrG95rtUmvM7qODz801C1JYaYmiaFIl5RS9OrVi927d2NnZ8f27dvfbdxOMitVqhTbt2/Hzs6OXbt20atXL1kAW4gM6uHDhwwePJhy5cqxZcsWLCws6Nu3LxcvXmT06NGJ2pM9URwdISAAfvkFPvjAcMzSMvH1RJ/zwQewfbuhTkfH5ItTpEuSGIp0afTo0axZswatVou/vz81atQwd0hGNWrUYOPGjWi1WlavXs2YMWPMHZIQIhmFh4cze/ZsSpYsybx584iKiqJJkyaEhISwePHi1Om50GigWTM4cAAOHYLmzcHG5t/PYksULS0Nn4GhbPPmhnMPHICmTf/9TGRqMsZQpDvz589n0KBBQNpeWPrNhbbnz5/PgAEDzByRECIplFJs2rSJkSNHcvnyZcCwTeasWbNo2LChmaPDMObw9GlDsnfoEFy/Di9eGD7Llg2KFIFatQyvChXerZVRZHiSGIp0ZePGjXTo0AGlFJMnT+aLL74wd0hvNXnyZMaNG4dGo2HDhg20bdvW3CEJId7B4cOHGTp0KAcPHgQMY4onT56Mj48P2uQY5ydEGiGJoUg39u3bR+PGjYmIiKBfv34sWLAgxRewTiqlFP369ePbb7/FysqK3bt3J+u+zUKIlHX16lVGjx7NDz/8AICtrS3Dhw9n+PDhsuqAyJAkMRTpwqlTp3Bzc+Pp06d8/PHHxjF86YFOp6Nt27Zs2bIFe3t7goKCqFixornDEkK8xdOnT/H19WXu3LmEh4ej0Wjw9vZm8uTJODk5mTs8IVKMJIYizbt+/Tq1atXi1q1b1K5dmz179qTcbL8UEhYWRsOGDTl48CBOTk4cOnSIwoULmzssIcR/REZGsnTpUiZMmMCDBw8AqF+/PjNnzqRKlSpmjk6IlCeJoUjTHj16RJ06dTh79ixly5blwIED5M6d29xhvZM376VcuXIEBQWl23sRIqNRSrF9+3aGDx/OuXPnAChTpgwzZszgo48+SvPDVoRILrJcjUizwsLCaNGiBWfPnsXJyYmAgIB0nUjlzp2bgIAAChYsyJkzZ2jZsiVhYWHmDkuITO/PP/+kQYMGNG/enHPnzuHg4MCCBQs4deoUzZs3l6RQZCqSGIo0SafT4eXlxcGDB7G3t2fnzp0UKVLE3GElWZEiRQgICCBHjhwcOHCAzp07o9PpzB2WEJnSzZs36d69O1WrVuV///sfVlZWjBgxgkuXLtG/f3+yZMli7hCFSHWSGIo0RynFwIED2bJlC1ZWVmzZsiVDTdaoWLEiW7duxcrKis2bNzNo0CDZHUWIVPTixQvGjx9P6dKlWblyJUopOnbsyPnz55k2bRr29vbmDlEIs5HEUKQ5vr6+LF68GI1Gw9q1a/Hw8DB3SMnOw8ODNWvWoNFoWLRoEVOmTDF3SEJkeDqdjmXLllG6dGkmTpxIaGgorq6u/P7776xfv55ixYqZO0QhzE4mn4g05c3dQubOnWvc4SSjmjt3Lp999hmQtndxESK927NnD8OGDSMkJASAEiVKMG3aNNq0aSNjCIV4gySGIs3YsWMHLVq0QKfTMWLECKZNm2bukFLFiBEjmDFjBlqtlp9//pkmTZqYOyQhMoy//vqL4cOHs3PnTgBy5szJuHHj6N+/P9bW1maOToi0RxJDkSYcOXKEevXqERoaSpcuXVi1ahUWFpljpINer6dbt274+flhZ2fHvn37qF69urnDEiJdu3v3LuPHj+e7775Dr9djaWlJ//79GTduHHny5DF3eEKkWZIYCrO7ePEirq6uPHjwgEaNGvHzzz9jZWVl7rBSVUREBB999BF79uzB0dGR4OBgSpYsae6whEh3wsLC+Oabb5g6dSrPnz8H4OOPP2batGmUKlXKzNEJkfZJYijM6u7du9SqVYsrV65QtWpV9u3bR/bs2c0dllk8f/4cd3d3Tpw4QYkSJQgODiZfvnzmDkuIdEGv17Nu3TrGjBnDjRs3AHBxcWH27NnUrVvXzNEJkX5IYijM5vnz53h4eHD8+HGKFy9OcHAw+fPnN3dYZnXnzh1cXV25cuUKLi4u7Nu3j2zZspk7LCHStKCgID7//HOOHj0KQOHChfH19cXLyyvTDEkRIrnIEyPMIiIigrZt23L8+HEcHBzYtWtXpk8KAfLnz09AQAAODg4cO3aMtm3bEhkZae6whEiTLl68SOvWralbty5Hjx4lW7ZsfP3115w/f54uXbpIUijEO5CnRqQ6pRS9evVi9+7d2NnZsX37dhn784bSpUvzyy+/YGdnx65du+jVq5csgC3EGx49esRnn31GuXLl2Lx5MxYWFvTp04dLly4xZswYbG1tzR2iEOmWJIYi1Y0ePZo1a9ag1WrZuHEjNWrUMHdIaU7NmjXZsGEDWq2W1atXM2bMGHOHJITZhYeHM3v2bJydnZk7dy5RUVE0adKEkJAQvv32WxmTK0QykDGGIlXNnz/fuGi1LOgcvzcX/J4/fz4DBgwwc0RCpD6lFJs2bWLkyJFcvnwZMGwtOWvWLBo2bGjm6ITIWKTFUKSajRs3MnjwYAAmT54sSWECdO/enUmTJgEwaNAg/P39zRyREKnr8OHDuLm50a5dOy5fvkz+/Pn5/vvvOXHihCSFQqQAaTEUqeK3336jUaNGRERE0K9fPxYsWCDbUCWQUop+/frx7bffYm1tze7du2X5DZHhXbt2jdGjR7N+/XoAbG1tGTZsGCNGjJCZ+kKkIGkxFCnu1KlTtGzZkoiICD7++GPmzZsnSWEiaDQaFixYQKtWrQgPD6dFixacPn3a3GGJdGTChAloNBoePHhg7lDi9fTpU0aNGkWZMmVYv349Go0Gb29vLly4wMSJEyUpFCKFSWIoUtSNGzdo0qQJT58+pXbt2vj5+aHVas0dVrqj1WpZt24dtWvX5unTp3h6ehoX8RUiuYSGhrJw4UIaNWpEgQIFyJ49O1WqVGHx4sXodLpku054eDgjR46kYMGC2NraUrNmTXbu3MnChQspWbIk06ZNIzw8nHr16nHs2DFWrlxJoUKFYq2rYcOGaDSaWMffajSaWF9Tp05NtnsRIqOxNHcAIuN69OgRnp6e3Lx5k7Jly7Jt2zZZRiIJbG1t2bZtG3Xq1OHs2bN4enpy4MABcuXKZe7QRAZx+fJlBg4cyIcffsjnn39Ojhw52LVrF/369eP3339n1apVyXIdHx8f/P39+eyzzyhZsiRz5syhadOmxs/LlCnDjBkz+Oijj97au/DTTz9x6NCht16rYcOGdOvWzeRYlSpVknYDQmRkSogUEBoaqurUqaMA5eTkpK5du2bukDKMa9euqYIFCypAubm5qbCwMHOHJNK48ePHK0Ddv3//reXu37+vTp8+HeN49+7dFaAuXryY5FgOHz6sADVjxgx14sQJVb9+fQUoQFlaWqoFCxaoiIiIeOsJCwtTxYoVUxMnTlSA6t+/f4wycR0XQsRNupJFstPpdHTu3JkDBw5gb2/Pzp07KVKkiLnDyjCKFClCQEAAOXLkICgoiM6dOydrN58wn5CQEDQaDdu2bTMeO3bsGBqNhqpVq5qUbdKkCTVr1kxU/U+ePMHHx4ecOXNib29P9+7dCQ0NNX7u4OBA+fLlY5z38ccfA3D27NlEXS82/v7+aLVaTpw4QdWqVfnf//6HlZUVdevWJSoqihYtWpAlS5Z465k+fTp6vZ5hw4bFWzYsLIxXr14lOXYhMgNJDEWyUkoxaNAgNm/ejJWVFVu2bKFixYrmDivDqVixIlu3bsXKyoqffvqJwYMHy+4oGUCFChXImTMn+/fvNx4LCgrCwsKCkydP8uzZMwD0ej3BwcGJnp3evn17nj9/zpQpU2jfvj0rV67kq6++ive8O3fuAIbEMZper+fBgwcJekVv6/jixQv8/f3R6/WsW7cOpRQdO3bk/PnzfPnllwD8+eef8cZz/fp1pk6dyrRp0+IdnrJy5UqyZs2Kra0t5cqVY926dfHWL0SmZuYWS5HBfP311wpQGo1GbdiwwdzhZHg//vij0mg0ClC+vr7mDkckg2bNmqkaNWoY37du3Vq1bt1aabVatXPnTqWUUsePH1eA2rp1a4LqjO5K7tGjh8nxjz/+WOXJk+et54aHh6ty5cqp4sWLq8jISOPxK1euGLuA43vt3btXff/996pAgQLGY66urur333831vfXX38pQH377bfx3k/btm2Vq6ur8T1xdBm7urqqOXPmqK1bt6rFixerChUqKEAtWrQo3msIkVnJ5BORbFauXMkXX3wBwJw5c2jXrp2ZI8r42rdvz+3bt/nss88YM2YMBQsWxNvb29xhiSRwc3Nj7NixvHz5kqxZs3LgwAF8fX25du0aQUFBeHp6EhQUhEajoU6dOomqu2/fvjGutXnzZp49e0aOHDliPWfAgAGcOXOG7du3Y2n576+M/Pnzs2fPnniveezYMQYNGsSZM2cAsLS0pGLFihw4cMBkYomNjQ1g6PZ9m8DAQDZt2sThw4fjvfbBgwdN3vfo0QMXFxfGjBmDj4+PTIYTIhaSGIpksXPnTnr16gXAiBEjjNveiZQ3ePBgbt68yYwZM+jZsyf58uXD09PT3GGJd+Tm5kZUVBSHDh2icOHC3Lt3Dzc3N/766y+CgoIAQ/dyuXLlyJ07d6Lq/u9Y3+gZ7Y8fP441MZwxYwbfffcdkyZNMpk1DIZErkGDBnFe68yZMwwfPpwdO3YAkDNnTsaOHcvy5cvJlStXjNnG0WMA35asRUVFMWjQILp27Ur16tXfcqexs7KyYsCAAfTt25djx44lOrEWIjOQMYYiyf744w/atm2LTqejS5cuTJkyxdwhZTpTp041TkJp27Ytf/zxh7lDEu+oWrVq2NjYsH//foKCgsibNy+lS5fGzc2NI0eOEB4eTlBQEG5ubomuO641RFUs41NXrlzJyJEj6du3L2PHjo3xuU6n486dOzFep0+fxtvbm4oVK7Jjxw60Wi39+/fn0qVLDB06lIIFC3L79u0Y9UUfK1iwYJzxr169mvPnz9OnTx+uXr1qfAE8f/6cq1evmkymiU3hwoUBw3JaQoiYpMVQJMmlS5do1qwZoaGhNGrUiGXLlmFhIX9vpDYLCwuWL1/OvXv32LNnD82aNSM4OJiSJUuaOzSRSFZWVtSoUYOgoCCKFCliTADd3NwIDw/Hz8+Pu3fvpui2iFu3bqVXr160bt2ahQsXxlrmxo0bFC9ePN66ov9YyZMnDwCVK1cmMDAwRvd1dNdw5cqV46zr+vXrREZGUrt27RifrV69mtWrV7N582ZatWoVZx2XL18GwNHRMd7YhciM5De4eGd3796lcePG3L9/n6pVq+Lv74+VlZW5w8q0rKys2LRpE1WqVOH+/ft4enpy7949c4cl3oGbmxuHDx8mMDDQmBg6ODhQtmxZpk2bZiyTEvbv30/Hjh2pW7cufn5+cf6hFz3GcNeuXYwaNYq8efMaPytVqhQzZ85kz5497Nmzh/fff9/4WXTvwtKlS43HwsPDWbFiBTVr1jS26IEhETx37pzxfceOHdm8eXOMF0DTpk3ZvHmzcQmf+/fvx4j5+fPnzJkzBwcHB1xcXN7xKyRExiYthuKdvHjxgmbNmnH58mWKFy/O9u3byZ49u7nDyvSyZ8/Ojh07cHV15e+//6ZZs2YEBgbK/rLpjJubG19//TU3btwwSQDr1q3LkiVLKFasWJxbxCXFtWvXaNGiBRqNhrZt27Jx40aTzytVqkSlSpUAwxhDa2trhg4dahy6UKhQIaZMmYKXl1ecCWXNmjVp164do0eP5t69e5QsWZJVq1Zx9epVli1bZlK2W7du/Pbbb8au7vfee4/33nsv1nqLFy9u0lK4cOFCtmzZQvPmzSlSpAi3b99m+fLlXL9+nTVr1sgfsULEQRJDkWgRERG0adOGY8eO4eDgwK5du8ifP7+5wxKv5c+fn4CAAGrXrs3Ro0dp27YtP//8c4IWDRZpg6urK1qtFjs7O5PWNjc3N5YsWZJirYVXrlzh6dOnAPTv3z/G5+PHj6dSpUpcvHiRkSNHGlvrsmXLxujRoxkyZEiCZvquXr2acePGsWbNGh4/fkylSpX45ZdfkrV7vHbt2gQHB/P999/z8OFDsmbNSo0aNVi+fDn169dPtusIkdFoVGyjjoWIg1IKb29v1qxZg52dHYGBgdSoUcPcYYlYHD58mPr16xMaGkq3bt1YuXLlW/edTS88PDyoXLkyc+bMMXcomc6jR4+YOHEiCxcuJCoqCgsLC3r16sXEiRPJly+fucMTQiQDGWMoEmX06NGsWbMGrVbLxo0bJSlMw2rWrMmGDRvQarWsXr2aMWPGmDskkU6Fh4cze/ZsnJ2dmTt3LlFRUXh6enLy5EmWLFkiSaEQGYh0JYsEmz9/vnHg+3fffRdjXTOR9jRr1oylS5fSs2dPpk6dipOTEwMGDDB3WCKZvHjxghcvXry1jKOjY5zL1MRHKcWmTZsYOXKkcTZvxYoVmTlzJo0aNXqnOoUQaZu0GIoE2bhxI4MHDwZg0qRJdO/e3cwRiYTq0aMHEydOBGDQoEH4+/ubOaKk0+v1jBgxgty5c5M/f34mTJhg7pDMYubMmRQoUOCtrxs3brxT3YcPH8bNzY127dpx+fJl8ufPz3fffceJEyckKRQiA5MxhiJev/32G40aNSIiIoK+ffuyaNGiDDFWLTNRSvHpp5+yZMkSrK2t2b17d4qug5eSPDw8OHHiBJ9//jleXl4cOnQIHx8fdu3axeXLl1mxYgWRkZHUqFEjw3+vXr582diSF5c6deoYt5tLiGvXrjF69GjWr18PGHYiGTZsGCNGjJDZ7UJkApIYirc6deoUbm5uPH36lFatWuHv7//O3VLCvHQ6HW3atGHr1q3Y29tz4MABKlSoYO6wEs3DwwOdTmfcHg6gRo0a1K9fn08//dS47ZurqyuTJk1667Zt4l9Pnz5lypQpzJkzh/DwcDQaDd26dWPy5MkpsjSOECJtkq7kDO6PP/5gwIABlC9fnqxZs1KkSBHat2/PhQsXYpQ9cuQI/fr1w8XFhSxZsqDRaGjSpAlPnz6ldu3arFu3LslJoV6vZ/r06RQvXhwbGxsqVapkbJmIz6+//kqPHj0oXbo0dnZ2lChRgl69esW6vZaHhwcajSbGKzPvIazValm/fj2urq7/b+/Ow6Is9z+Ov4dBRHFPTE1FzeUoZh7SzAWxTilKLilqhalpeTI7x4wst0qPhluldczyV5lrlluS+9ETxSCWRqZlckhTMhL3JWUfnt8fM0yRhgMMzIiflxfXBc88y3cU8eP9PPf35sKFC4SGhhb5NqO75fXSy1OnTh1OnjxJQECA48/aMAytwuOEnJwcFixYQJMmTZg1axaZmZncfffdJCQksHjxYoVCkRuMJp+UcbNmzWLnzp0MGDCA1q1bk5qayvz58wkKCuKLL77IN2K0efNm3n33XVq3bk1AQACHDx8mJSWFFi1a8MknnzjVn+xaJk2axMyZM3n88cdp164d0dHRPPzww5hMJh588MECj33++ec5e/YsAwYMoGnTpvz444/Mnz+fjRs38s0331zRSzGv2e7vFbQO642gQoUKbNiwgU6dOpGYmEhoaChxcXFUr17d3aUVyh97MppMJnJzcx1fr1y5EqvVyt13313apV03DMNg06ZNjBs3zrG6SPPmzZkzZw73339/mb4FLyIFMKRM27lzp5GZmZlvW1JSklG+fHkjIiIi3/bU1FQjLS3NSEtLM+rUqWMARt26dY3k5GSX1PLzzz8b5cqVM0aPHu3YlpubawQHBxv16tUzcnJyCjz+888/N6xW6xXbAGPSpEn5toeEhBiBgYEuqbssOnr0qFG3bl0DMIKDg4309HR3l+S0kJAQY8yYMfm29enTxxg6dKhhGIaRmJho1K9f3zh48GDpF3ed2Lt3r3HPPfcYgAEYNWvWNObPn29kZWW5uzQRcTPdZ/EQ+/fvx2Qy8cknnzi2JSQkYDKZCAoKyrdvjx49HOuBXkvHjh2vWPqpadOmBAYGcvDgwXzbb775Znx8fIiIiHDcnt26davjma3iio6OJjs7myeffNKxzWQyMWrUKH7++Wd27dpV4PFdunS54tZgly5dqFGjxhXvJU9OTs4123nciAICAtiyZQtVqlTBYrEQERGB1Wp1d1nFdubMGfr168e77777p0un3chSUlJ49NFHCQoK4tNPP8XHx4fnnnuOQ4cOMXr0aK2OIyJ6xtBTtGrVimrVqhEbG+vYZrFY8PLyYt++fVy8eBGwPaMXHx9frBmlhmFw4sQJatasecX2f/7zn3z88ceOAHbbbbdd9RynT5926iMzM9NxzN69e/Hz86NFixb5zpXXJHvv3r2Ffi95fdz++F4AkpKS8PPzo3LlytSuXZsXXniB7OzsQl+jrGrdujXr16/Hx8eHdevWMWbMGMeatNerKVOmcPLkSaKioujatatjybYb3eXLl5kyZQrNmjVj8eLFGIbBoEGDSExMZNasWVStWtXdJYqIp3DvgKX8XlhYmHHnnXc6vu7Xr5/Rr18/w2w2G1u2bDEMwzC+/vprAzCio6OLfJ1ly5YZgPHee+/l2/7yyy8bgGEymYzu3bsbBX17YL8Fda2P999/P9/7a9y48RXnunz5sgEY48ePL/R7mTZtmgEY//3vf/NtHz58uDFlyhRj7dq1xtKlS43evXsbgDFw4MBCX6Os+/DDDx1/XlFRUe4uR1woJyfHeO+99xyPhgBGhw4djF27drm7NBHxUJp84kGCg4OZPHkyly9fxs/Pj7i4OKKiokhOTsZisRAaGorFYsFkMtG5c+ciXSMxMZHRo0fToUMHhg4d6ti+ePFiJk2aBMDcuXP54Ycf2LZt25+eZ/v27U5dLzAw0PF5eno65cuXv2KfvB5r6enpTp0zT2xsLFOnTmXgwIHcc889+V5777338n39yCOPMHLkSN555x3Gjh3LXXfdVahrlWWDBg3i+PHjjB07lokTJ1K3bt183xtyfdqxYweRkZHs378fgEaNGjFr1izCw8M1sURE/py7k6n8ZufOnQZgbN++3UhMTDQA43//+58xduxYIzg42DAMwwgPDy/ypIrjx48bjRs3NurXr2+kpKQ4tm/evNkwm80GYIwbN84wDMMYPXp0gSOGReHKEcODBw8aNWrUMNq0aWNcvHjRqWPyfk+nTZvm9HVuJM8++6wB5BuhluvPgQMHjJ49ezpGCKtWrWq88sorRkZGhrtLE5HrgEYMPUjbtm3x9fUlNjaWBg0aUKtWLZo1a0ZwcDALFiwgMzMTi8XCAw88UOhzX7hwgR49enD+/HksFoujbcuePXsIDw/HarUSERHBzJkznTpfamqqU/tVrVrV0eamTp06xMTEYBhGvhGLvIkuzraSOXbsGN26daNq1aps3ryZypUrO3Vc/fr1ATh79qxT+99oZs2axfHjx1mxYgXh4eHExMTQrl07d5clTjp58iQvvfQS77zzDlarFW9vb5588klefPFFbrrpJneXJyLXCU0+8SA+Pj7ceeedWCwWLBYLwcHBgO0Wc2ZmJitWrODEiROFnniSkZFBr169SEpKYuPGjbRs2RKAQ4cOERYWRlpaGvfddx+LFi1yuiHwtdZnzfv46KOPHMe0adOGtLS0K2YQf/nll47Xr+XMmTN069aNzMxMtm3bRp06dZz8XcCxdJi/v7/Tx9xIvLy8WLRoEffeey+XL18mLCyMQ4cOubssuYb09HRmzJhBkyZNePvtt7FarfTt25cDBw7w+uuvKxSKSKFoxNDDBAcH89prr3H48GEiIyMBqFmzJi1atGDWrFmOfZxltVoZNGgQu3btIjo6mg4dOgBw4sQJunfvzqlTpwgKCmLt2rVXtLUpSFGeMezTpw9jx45lwYIFzJ8/H7DNhH777be55ZZb6Nixo2Pf48ePc+HCBW699VZHC43Lly/Ts2dPUlJSiImJoWnTple95sWLFylfvny+5xkNw2D69OkAdO/e3en3eaPJm6EcEhLC3r17CQ0NJT4+nlq1arm7NPmD3NxcVq5cycSJE/npp58ACAoK4rXXXiMkJMTN1YnI9UrB0MMEBwfz8ssvc+zYsXwBsEuXLixcuJCGDRsWaomqyMhIPvnkE3r16sXZs2dZvnw5GRkZvPzyyxw9epRGjRqxadMmKleuTHJyMsuWLQPgq6++AnCEqYCAAB555BHHeYuy/my9evV4+umnmTNnDtnZ2bRr147169djsVhYsWJFvuX2JkyYwJIlSzhy5AgNGzYEICIigt27dzN8+HAOHjyYb+SxUqVK9O3bF4Cvv/6ahx56iIceeogmTZqQnp7Oxx9/zM6dOxk5cuQVfSElv8qVK7N582Y6duzI4cOHCQsLIyYmhkqVKrm7NLGzWCxERkayZ88ewPZ3KyoqioiICC0DKCLF4+ZnHOUPLl68aJjNZqNy5cr5VgJZvny5ARiPPPJIoc4XEhJSYDuZpKQkx74xMTF/ul9ISIhL3p/VajWioqKMgIAAw8fHxwgMDDSWL19+xX5Dhw41AOPIkSOObQEBAX9aX0BAgGO/H3/80RgwYIDRsGFDw9fX16hYsaJxxx13GG+//baRm5vrkvdxI/jf//5n1KxZ0wCM7t27a1UMD5CUlGQ88MADju/7SpUqGdOnTzcuX77s7tJEpIwwGcZ13tFWnGYYBsOGDWPp0qVUrFiRmJgYR3Npkav58ssvueeee0hLS2PIkCEsXrxYrU7c4OzZs0ybNo0333yT7OxsvLy8eOyxx5g6deoVa4SLiBSH7jncQCZOnMjSpUsxm82sXr1aoVCuqX379qxatQqz2czSpUsdvS6ldGRlZTF37lyaNGnCvHnzyM7OJjQ0lH379rFw4UKFQhFxOY0YXqfyloIriL+/v+O5vfnz5/OPf/wDgEWLFvHoo4+WeI1SdixatIgRI0YA8O9//5unnnrKzRWVbYZhsG7dOp5//nkOHz4M2JanfOWVV+jWrZubqxORskzB8Do1ZcoUpk6dWuA+eRM31qxZw8CBAzEMg2nTpjF58uRSqlLKkmnTpvHiiy9iMplYvXo1/fv3d3dJZdLu3buJjIwkLi4OgNq1azNt2jQeffTRfBO0RERKgoLhderHH3909OX7M507d2b37t2Ovn9PPPEECxYs0DNiUiSGYTBq1CgWLlxI+fLl2b59e6FaJ0nBkpOTmTBhAitXrgSgQoUKPPvsszz33HOaES4ipUbBsAz77rvv6Ny5MxcuXKBv376sWbNGIw5SLFarlf79+xMdHU21atWIi4vL16tSCu/ChQvMmDGDefPmkZmZiclkYsiQIUyfPr1QralERFxBwbCMOnbsGB06dCAlJYVOnTqxfft2x9J0IsWRnp7OvffeS3x8PPXq1SM+Pt6x3KA4Lycnh3feeYeXXnqJU6dOAdC1a1deffVV9doUEbdRMCyDzp07R+fOnfn+++9p0aIFcXFx1KhRw91lSRly9uxZOnXqRGJiIoGBgVgsFqpXr+7usq4LhmGwadMmxo0bR2JiIgDNmjVjzpw59OrVS496iIhbqV1NGZORkUGfPn34/vvvqVu3Llu3blUoFJerUaMGW7dupW7duhw4cIC+ffuSkZHh7rI83jfffMO9995Lr169SExM5KabbuLf//433333Hb1791YoFBG3UzAsQ6xWKxEREVgsFqpUqcLWrVtp0KCBu8uSMiogIIAtW7ZQpUoVYmNjGTx4MFar1d1leaSUlBQeffRRgoKC+PTTT/Hx8WHcuHEcOnSIp556yrEeuIiIuykYlhGGYTBmzBjWrVuHj48P0dHR3Hbbbe4uS8q41q1bs379enx8fFi7di1PP/00ejrlN5cvX2bKlCk0a9aMxYsXYxgGgwYNIjExkdmzZ1OtWjV3lygiko+CYRkxc+ZM3nzzTUwmE8uWLaNr167uLkluEHfffTdLly4FbI3UZ82a5eaK3M9qtbJo0SKaNm3K1KlTSUtLo0OHDuzatYsPP/yQRo0aubtEEZGr0uSTMmDJkiUMGzYMgHnz5jFmzBj3FiQ3pHnz5jF27FjA9j05ZMgQN1fkHjt27CAyMpL9+/cD0KhRI2bNmkV4eLieIRQRj6dgWAqysuDAATh/HtLTwccHKlaEFi2guBM5t2zZQq9evbBarYwbN47Zs2e7pGaRohg3bhyvvPIK3t7ebNiwgdDQ0GKd71z6OQ6ePkhadhpZ1iwqeFegmm81AmsF4mP2cVHVrvH9998zbtw4Nm/eDEDVqlWZPHky//jHPyhfvrybqxMRcY6CYQnIyYGtW+HzzyEuDr7+2hYOr+bWW6FLF+jYER54AG66yfnr7Nmzh65du5KWlkZERARLly7Fy0tPB4j75ObmMmTIEFasWIGfnx8xMTG0a9fO6ePPpJ3h48SPiT8WT2xyLIfPHb7qfj5mH4JqB9G5QWdCGobQo0kPzF7uad5+8uRJXnrpJd555x2sVive3t6MGjWKF198kZo1a7qlJhGRolIwdKH0dFi8GGbOhJ9+gnLlIDv72sd5e9vCpK8v/P3vMHYsBAQUfMyhQ4fo2LEjp06d4r777mPjxo34+HjWCIrcmLKysggLC2PHjh34+/sTHx9PkyZNCjwm+Xwyr+16jf9L+D8yrBl4e3mTk5tzzWuV8ypHdm42Dao2YHyn8QxrM4wK5UqnkXt6ejrz5s1jxowZ/PrrrwD06dOH2bNn06xZs1KpQUTE1TS85AK5uTB3LtxyC4weDceO2bY7EwrBFgoBMjJg/nxo3BiGDIHTp6++/4kTJ+jevTunTp3ir3/9K2vXrr2hQ+GePXt46qmnCAwMxM/PjwYNGjBw4ECSkpLy7Zebm8vixYvp3bs39evXx8/Pj1atWjF9+nSX9uDLzc1l9uzZNGrUCF9fX1q3bu1Y//ZaYmNjHfX5+vpSu3ZtQkND2blzp8vqK2l5M5TbtGnDqVOnCA0N5eTJk1fd99TlUzyy7hEav9GYN/e8SYbV9ufgTCgEyM61/SU7duEYozeP5pbXbmHurrnkGrmueTNXkZubywcffMBf/vIXJk6cyK+//kpQUBCfffYZ69evVygUkeuaRgyL6dQpGDwY/vMf157XbIaaNWH1aggO/m37pUuX6Nq1KwkJCTRq1Ij4+Hhq167t2otfZ8LDw9m5cycDBgygdevWpKamMn/+fC5dusQXX3xBq1atANvvXeXKlbnrrru4//77qVWrFrt27WLJkiV06dKFTz/91CWTAyZMmMDMmTN5/PHHadeuHdHR0WzatImVK1fy4IMPFnjsu+++y8aNG2nXrh21a9fm3LlzLF++nG+//ZZNmzYV+5m90pSamkqHDh04evQobdu2JSYmhkqVKjlej02OZcDqAZxJO4PVcG3/w26Nu7G833L8/fxdet64uDieeeYZ9uzZA0C9evWIiooiIiJCj3GISJmgYFgMn38OAwfCmTNQEn19vbzAMGDaNJgwAazWbHr16sW2bduoWbMmO3fu1OgEEB8fT9u2bfONmv7www/cdttthIeHs3z5csB2i/Orr76iY8eO+Y7/17/+xUsvvcT27du59957i1VLSkoKjRo1YuTIkcyfPx+w9ZgMCQnhyJEjHD16FLO5cM/CpaWl0bhxY9q0acPWrVuLVV9pS0pKomPHjpw5c4bu3buzYcMGvMxezIibwYsxL2IymUpkdM9sMnNTxZtYPWA1XQK6FPt8hw4d4vnnn2fdunUAVKpUifHjxzN27FgqVqxY7POLiHgK/Re3iFavhrvvLrlQCLZb1IYBkyfD4MEGI0Y8xrZt26hYsSIbN2687kLh/v37MZlMfPLJJ45tCQkJmEwmgoKC8u3bo0cP2rdv79R5O3bseMWt9KZNmxIYGMjBgwcd23x8fK4IhQAPPPAAQL59iyo6Oprs7GyefPJJxzaTycSoUaP4+eef2bVrV6HPWbFiRfz9/Tl//nyx6yttzZo1Y9OmTVSoUIFt27YxYsQIBq8bzAsxL2BglNgtX6th5UzaGbou7sqa79cU+Txnz55l7NixtGzZknXr1uHl5cXIkSP54YcfmDRpkkKhiJQ5CoZFsHkzPPSQ7fPSWgFs5cqJLFu2FLPZzKpVq5wOTZ6kVatWVKtWjdjYWMc2i8WCl5cX+/bt4+LFi4DtGa74+Hi6dCn6SI9hGJw4ccKpWaGpqakAV+x7+vRppz4yMzMdx+zduxc/Pz9atGiR71x33nmn43VnXLx4kdOnT5OYmMjEiRP57rvv+Nvf/ubUsZ6mffv2rFq1CrPZzLJly/jw9Q9L5bp5t6cfXPMgm3/YXKhjs7KymDt3Lk2aNGHevHlkZ2cTGhrKvn37WLhw4Q3/+IaIlF0KhoX0zTfQv/9vo3mlYz4wE4C+ff+PsLCw0rqwS3l5edGpUycsFotjm8VioW/fvphMJuLj4wEcITH49w9XFtKKFStISUlh0KBB19x39uzZVKlShR49euTb7u/v79TH7yeWHD9+nJtvvvmKZxXr1KkDwC+//OJU/QMHDsTf358WLVrw6quv8ve//50XXnjBqWM90f3330+fyD62L+KAL0vnunmjkv1X9Wdf6r5r728YrF27lpYtW/LMM89w7tw5WrVqxbZt29iyZYvjeVURkbLK290FXE8uXoSwMNts49ILhWuAf9o//xfr1g1n2zbo3r20ru9awcHBTJ48mcuXL+Pn50dcXBxRUVEkJydjsVgIDQ3FYrFgMpno3Llzka6RmJjI6NGj6dChA0OHDi1w36ioKHbs2MGCBQuuWLd2+/btTl0vMDDQ8Xl6evpVmxn7+vo6XnfGzJkziYyM5NixYyxZsoSsrCxycpybqeuJth3axrqK66Ar8BmwBagMtCz5axsYZFuz6flBTw6OPkiV8lWuut/u3buJjIwkLi4OgJtvvplp06YxfPjwQj8XKiJyvVIwLIQpUyA11TZaWDpigcGAAfwdmIzJZOt1mJho63t4vQkODiYnJ4ddu3ZRv359Tp48SXBwMAcOHHCMJFosFlq2bEmNGjUKff7U1FTCwsKoWrUqa9asKfAf9I8++ojJkyczYsQIRo0adcXrRZmIUqFChXy3lvPktcOpUMG5Hntt2rRxfD548GCCgoIYNmwYa9YU/Xk5d8nIyWDkxpF4mbzIDcmFX4EEYC3gB1yjZ6crWA0rqZdSmfrZVF7t/mq+15KTk5kwYYJj5LdChQpERkby3HPPUbly5ZIvTkTEg+hWspMOHIDXXy/NUPgd0BvIBPoAbwImcnNtzbNffbXAgz1W27Zt8fX1JTY2FovFQq1atWjWrBnBwcHs3r2bzMxMLBZLkW4jX7hwgR49enD+/Hm2bt1K3bp1/3Tf7du3M2TIEMLCwnj77bevuk9qaqpTH78fBaxTpw6pqan8cbL/8ePHAQqs6c/4+PjQu3dv1q1b5/SIoyd5Jf4Vjl04ZptoYgJ6As0BK7ASuHqLQ5fLNXKZ9+U8Dpw8ANi+X8aPH0/z5s0doXDIkCEkJSUxbdo0hUIRuSEpGDrBMGDUKFv7mNJxDAgFLgAdsf3r+dvIV14Lm+Tk0qrHdXx8fLjzzjuxWCz5AmBwcDCZmZmsWLGCEydOFHriSUZGBr169SIpKYmNGzfSsuWf36P88ssveeCBB2jbti2rVq3C2/vqA+d16tRx6uOjjz5yHNOmTRvS0tKumOH85ZdfOl4vivT0dAzDcKywcb1IPp/M9NjpGPwuKJuB/kA9IANYju1bvRR4mbwYtWEUCxYsoGnTpsyaNYvMzExHb9AlS5ZQr1690ilGRMQDqY+hE7ZsgZ49S+tq54DOwPfAX4CdwJW3VL29bY2133+/tOpyncmTJ/Paa69Rs2ZNIiMjGTNmDAAtW7bEarWSlJTEsWPHnP4H2mq10q9fPzZv3kx0dDQ9C/jDOnjwIMHBwdSuXRuLxUL16tX/dN8dO3Y4df3AwEDH5JKff/6Zxo0bX7WP4Y8//khycrLj9vbx48e5cOECt956K+XKlQNs6+7WqlUr3/nPnz9P69atAfjpp5+cqslTDFs/jBXfrrj6SiZpwCLgNOAPDAdKcjU7A/gB+I/9mtja6cyZM4devXq5pLm5iMj1TsHQCeHhEB3929J1JScD6AZYgLpAPAU9gOXra1t55XeLSVwXtm3b5ljBIyEhwdHD8IknnmDhwoU0bNiQI0eOOH2+p59+mtdff51evXoxcODAK14fPHgwAL/++iuBgYGkpKQQFRXFLbfckm+/W2+9lQ4dOhT1bTk899xzzJkzh5EjR9KuXTvWr1/Ppk2bWLFiBQ8//LBjv2HDhrFkyRKOHDlCw4YNAbjjjjuoV68e7du3p1atWvz000+8//77/PLLL3z00UeEh4cXu77S8mvmr9SaU8uxzN1VnQfeBS5h+1YfDJQrgWJSgW2A/dvKp7IPr0bZZnvnhXIREdHkk2s6e7a0QqEViMAWCqtgm7ZZ8FP5GRmwdi1cY+Ktx+nYsSNms5mKFSty++23O7YHBwezcOHCQj9f+M033wCwYcMGNmzYcMXrecHwzJkzHLMvZD1+/Pgr9hs6dKhLguHMmTOpXr06CxcuZPHixTRt2pTly5fnC4V/Zvjw4Xz44YfMnTuX8+fPU716de666y4++OCDYrXvcYe1B9cWHAoBqmELg+8DycA6YACue8jlIvAp8I39azPQHqwhViJGRCgUioj8gUYMr+Gtt2D06JJuT2MA/8A2wcQH2Arcfc2jvLygc2fb0nwinqbL+13YeWync6ub/IjtWcNc4E6gB7aJKkWVhe0pjHgg274tELgXqA4mTCwIW8ATbZ8oxkVERMqeMjf55NSpU9SuXZuoqCjHtvj4eHx8fPjvf/9b6PMtXQol/+jRTPJmHcMynAmFYJshHRsLKSklWJpIEaRcTMHyk8X5Je8aA/3sn+/G1gS7KHKBr4E3gM+xhcJ6wAhsI5H2R0pNmFi6b2kRLyIiUnaVuVvJ/v7+LFq0iL59+9KtWzeaN2/OI488wlNPPUX79u0JCQkhNzeXtLQ03nrrLcdSZVdjtcLXX5d0i5olwET753OBK5+Ru5aEBPjD43JlwqVLl7h06VKB+/j7+6v5sAf66pevCn9QK2w9DrcB/8XWALtNIY4/jG1iyQn719WA+7A10f7Df+5yySXheALWXCtmL33/iIjkKXPBEKBnz548/vjjRERE0LZtW/z8/JgxYwYmk4mVK1dSt25dtmzZwjPPPONY5eBqDh2CrKySrHQrtqEMgHHAmEKfwdsb9u+H3r1dWZdneOWVV5g6dWqB+/x+4oZ4jv0n9uPt5X312cgF6YAtHMYDn2BrgN30GsecwhYIf7B/XR7oArSnwJ9wWdYsDp87TLObmhWuRhGRMqxMBkOwhYpWrVqxevVqEhISHMuU5TUYNplMeF2jMeH+/SVZ4R4gnN8mncws0lkMo6TrdJ8hQ4Zcc1m82rVrl1I1Uhj7T+x3/jbyH92LLRx+C6wChgFXGxG/hG15vQRsj+l6AW2BEGyB0sk6FQxFRH5TZoPh4cOH+eWXX8jNzeXo0aPcdtttjtcyMzN5/vnnmT59eoHn2L/fNiLn+hnJh4Aw4DK2e12LKOrjnlar7VZyWdS4cWMaN27s7jKkCBKOJxQ9GHphW+znMrZJKR9gG1jPa+eZDXyBbQJ/3oh+c2x/lWo6fxlvL2/2n9hPeMvrpwWQiEhJK5PBMCsri8GDBzNo0CCaN2/OY489xrfffutoHPzPf/6Tu+66i169ehV4nqNHS2I28klsq5qcAv6KbcFYn2Kd0d6BRcRj/Hzx5+KdwBvb47aLsfUgXIatAfYRbM8f5q2UUgdb689Ghb+EYRgcOe98v0wRkRtBmQyGkyZN4sKFC7zxxhtUqlSJzZs3M3z4cDZu3Mgbb7zB4cOH2bJlyzXPk5Hh6oknv2ILhYex9Shcg2145HKxzpqdDZculcbsaZFrMwyD7Izsa+94LV7YwuESbAsCvcFvrWeqAH8DbqPIvRVyjVwyczKLXaaISFlS5oLhZ599xrx584iJiaFKlSoALFu2jNtvv51p06YxZcoUgoKCuO+++6hatSrR0dF/eq6sLFePGE4A9to/TwZuddmZK1d22alEPFM2thDYFbiL4g60Y2CQZS3R2WUiItcdNbguwIABtpVFXPc7dBzbUnciUiRPAC6ab2TCRHjLcFYNWOWaE4qIlAFlbsTQlXx9bauLWK2uOmNtbFMpXcvbG86fd/lpRYqs2sxqhW9V4wwXrmDnZfKivHd5151QRKQMUDAsQMOGrn5uz4TTfTQKoUED8HP9aUWKrH7N+h4/scNkMtGoWhFmrYiIlGFlbkk8V2rduiRa1biW2Qx33OHuKkTyu6POHXiZPPvHS05uDq1vbu3uMkREPIpn/+R2s9bXwb8ZJtP1UafcWFrf3NrjgyGgYCgi8gee/5PbjZo0AZ9iznwsaTk5CobieVrf3LpknjF0ofLm8txa3XWdAUREygIFwwKYzRAUZJuA4sl0K1k8Tdu6bd1dQoG88CKoThBmL7O7SxER8SgeHnncb8iQklj9xDW8vKBLF7jlauvIirjRLVVuIbhBsMfeTjYwGHL7EHeXISLicTzzp7YHGTTINnLoiXJzYfhwd1chcnXD/zq86OsllzBvL28GBQ5ydxkiIh5HwfAaatSAPn1svQI9ja8v9O/v7ipErq5/i/74mn3dXcYVvE3e9Gneh+oVqru7FBERj6Ng6IQRIzyvbY23Nzz4IFSq5O5KRK6ucvnKDGo1CG8vz/pfVY6Rw/C/aqhdRORqtCSeEwwDunaF+HjPCYjly0NSkq25tYinSj6fTPP5zcm0Zrq7FMB2C7lT/U7EDI3B5Nru9SIiZYJGDJ1gMsGCBbZn+jyByQQvvKBQKJ4voFoAk7tMxoRnhDDDMFgQtkChUETkTygYOikwEMaMcX/rGi8vWyCMjHRvHSLOerbjszSo2sDtM5S9TF6MaT+Glv4t3VqHiIgn063kQrh4EVq0gBMnwGp1Tw0mE2zdCt26uef6IkWx7dA2QleEuu36ZpOZmyvdzMHRB6lSvorb6hAR8XQaMSyEKlVg82YoV84W0NxhxgyFQrn+dG/SnRl/m+GWa5swUc5cji0RWxQKRUSuQcGwkG6/Hdautd3SLc1waDLB44/Dc8+V3jVFXOn5Ts/zWNBjpfq8oQkTZi8zaweu1brIIiJOUDAsgp494cMPbZ+XVvPrhx6Ct95y30ilSHGZTCbeDnubh1o9VCrXy3umcWX/lfRs2rNUrikicr1TMCyi8HCIiYGbbiq5cJg3Kvnyy7BsmeeuwCLiLLOXmWX9ljH97umYMJXYhBSzyUzNijX5bNhnhLcML5FriIiURZp8UkynTsHgwfCf/7j2vGYz1KwJq1dDcLBrzy3iCSzJFgasHsDptNNYDdfO5urWuBvL+y3H38/fpecVESnrNGJYTP7+sGULzJ0L1avbRviKc7vXbLZ9PPwwfPedQqGUXcEBwXz35Hc8fNvDeJm8MJuKPiRusv+q7ludud3nsmXwFoVCEZEi0IihC6Wnw5IlMHMmJCfblq1zZqWUvP18feGJJ2DsWDWvlhtL8vlk5n4xl4UJC8nIycDby5uc3Gv/5cnbL6BqAOM7j2fo7UOpUK5CKVQsIlI2KRiWAKvV1mvw888hLg4SEiAr6+r7NmkCXbpAhw7Qrx/UqFG6tYp4kjNpZ/g48WN2HdtF7E+xHDp76Kr7+Zh9uKPOHXRu0JmQgBBCm4Ri9tJDuCIixaVgWAqysuD77+H8eduooo8PVKwIf/mL7faziFzdufRzJJ5OJC07jSxrFhXKVaCabzVa+rfEx+zj7vJERMocBUMRERERATT5RERERETsFAxFREREBFAwFBERERE7BUMRERERARQMRURERMROwVBEREREAAVDEREREbFTMBQRERERQMFQREREROwUDEVEREQEUDAUERERETsFQxEREREBFAxFRERExE7BUEREREQABUMRERERsVMwFBERERFAwVBERERE7BQMRURERARQMBQREREROwVDEREREQEUDEVERETETsFQRERERAAFQxERERGxUzAUEREREUDBUERERETsFAxFREREBFAwFBERERE7BUMRERERARQMRURERMROwVBEREREAAVDEREREbFTMBQRERERQMFQREREROwUDEVEREQEUDAUERERETsFQxEREREBFAxFRERExE7BUEREREQABUMRERERsVMwFBERERFAwVBERERE7BQMRURERARQMBQREREROwVDEREREQEUDEVERETETsFQRERERAAFQxERERGxUzAUEREREUDBUERERETsFAxFREREBFAwFBERERE7BUMRERERARQMRURERMROwVBEREREAAVDEREREbFTMBQRERERQMFQREREROwUDEVEREQEUDAUERERETsFQxEREREBFAxFRERExE7BUEREREQABUMRERERsVMwFBERERFAwVBERERE7BQMRURERARQMBQREREROwVDEREREQEUDEVERETETsFQRERERAAFQxERERGxUzAUEREREUDBUERERETsFAxFREREBFAwFBERERE7BUMRERERARQMRURERMROwVBEREREAAVDEREREbFTMBQRERERQMFQREREROwUDEVEREQEUDAUERERETsFQxEREREBFAxFRERExE7BUEREREQABUMRERERsVMwFBERERFAwVBERERE7BQMRURERARQMBQREREROwVDEREREQEUDEVERETETsFQRERERAAFQxERERGxUzAUEREREUDBUERERETsFAxFREREBFAwFBERERE7BUMRERERAeD/AfLdaJosOvHiAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, we simple feedforward neural network with one hidden layer. The network has:\n",
        "\n",
        "* Input layer: 2 neurons ($x_1$, $x_2$)\n",
        "* Hidden layer: 2 neurons ($h_1$, $h_2$) with sigmoid activation\n",
        "* Output layer: 1 neuron ($y$) with sigmoid activation\n",
        "* Loss function: Mean Squared Error (MSE)\n",
        "\n",
        "We’ll compute gradients and update weights for one training example with\n",
        "Input: $x = [x_1, x_2] = [0.5, 0.1]$. Target output: $t = 0.8$\n",
        "\n",
        "Weights and biases (randomly initialized):\n",
        "\n",
        "* Input to hidden: $w_{11} = 0.15$, $w_{12} = 0.2$, $w_{21} = 0.25$, $w_{22} = 0.3$\n",
        "\n",
        "* Hidden to output: $w_{h1} = 0.4$, $w_{h2} = 0.45$\n",
        "\n",
        "* Biases: $b_h = 0.35$ (hidden), $b_y = 0.6$ (output)\n",
        "\n",
        "* Learning rate: $\\eta = 0.5$\n",
        "\n",
        "* Activation function: Sigmoid, $\\sigma(z) = \\frac{1}{1 + e^{-z}}$,\n",
        "\n",
        "* Derivative: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
        "\n",
        "The network structure is:\n",
        "\n",
        "* Input: $x_1$, $x_2$\n",
        "\n",
        "* Hidden: $h_1 = \\sigma(w_{11}x_1 + w_{21}x_2 + b_h)$, $h_2 = \\sigma(w_{12}x_1 + w_{22}x_2 + b_h)$\n",
        "\n",
        "* Output: $y = \\sigma(w_{h1}h_1 + w_{h2}h_2 + b_y)$\n",
        "\n",
        "* Loss: $E = \\frac{1}{2}(t - y)^2$\n"
      ],
      "metadata": {
        "id": "rcrpL-YWcJvl"
      },
      "id": "rcrpL-YWcJvl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>Step 1: Forward Pass. Compute the activations of all neurons. </font>\n",
        "\n",
        "Hidden Layer:\n",
        "\n",
        "Net input to $h_1$:$$z_{h1} = w_{11}x_1 + w_{21}x_2 + b_h = (0.15)(0.5) + (0.25)(0.1) + 0.35 = 0.075 + 0.025 + 0.35 = 0.45$$$$h_1 = \\sigma(z_{h1}) = \\frac{1}{1 + e^{-0.45}} \\approx 0.6106$$\n",
        "\n",
        "Net input to $h_2$:$$z_{h2} = w_{12}x_1 + w_{22}x_2 + b_h = (0.2)(0.5) + (0.3)(0.1) + 0.35 = 0.1 + 0.03 + 0.35 = 0.48$$$$h_2 = \\sigma(z_{h2}) = \\frac{1}{1 + e^{-0.48}} \\approx 0.6177$$\n",
        "\n",
        "\n",
        "Output Layer:\n",
        "\n",
        "Net input to $y$:$$z_y = w_{h1}h_1 + w_{h2}h_2 + b_y = (0.4)(0.6106) + (0.45)(0.6177) + 0.6 \\approx 0.2442 + 0.2780 + 0.6 = 1.1222$$$$y = \\sigma(z_y) = \\frac{1}{1 + e^{-1.1222}} \\approx 0.7546$$\n",
        "\n",
        "Loss:$$E = \\frac{1}{2}(t - y)^2 = \\frac{1}{2}(0.8 - 0.7546)^2 = \\frac{1}{2}(0.0454)^2 \\approx 0.00103$$"
      ],
      "metadata": {
        "id": "LWGJ7rUFeslX"
      },
      "id": "LWGJ7rUFeslX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'> Step 2: Backward Pass (Compute Gradients) </font>\n",
        "\n",
        "We compute the partial derivatives of the loss $E$ with respect to each weight and bias, starting from the output layer and moving backward.\n",
        "Output Layer Gradients:\n",
        "\n",
        "Error term for output neuron:$$\\delta_y = \\frac{\\partial E}{\\partial y} \\cdot \\sigma'(z_y)$$\n",
        "\n",
        "Loss derivative: $\\frac{\\partial E}{\\partial y} = -(t - y) = -(0.8 - 0.7546) = -0.0454$\n",
        "Sigmoid derivative: $\\sigma'(z_y) = y(1 - y) = 0.7546(1 - 0.7546) \\approx 0.1851$\n",
        "$\\delta_y = -0.0454 \\cdot 0.1851 \\approx -0.0084$\n",
        "\n",
        "\n",
        "Gradients for weights to output:$$\\frac{\\partial E}{\\partial w_{h1}} = \\delta_y \\cdot h_1 = -0.0084 \\cdot 0.6106 \\approx -0.00513$$$$\\frac{\\partial E}{\\partial w_{h2}} = \\delta_y \\cdot h_2 = -0.0084 \\cdot 0.6177 \\approx -0.00519$$\n",
        "\n",
        "Gradient for output bias:$$\\frac{\\partial E}{\\partial b_y} = \\delta_y = -0.0084$$\n",
        "\n",
        "\n",
        "Hidden Layer Gradients:\n",
        "\n",
        "Error terms for hidden neurons:$$\\delta_{h1} = \\left( w_{h1} \\cdot \\delta_y \\right) \\cdot \\sigma'(z_{h1})$$\n",
        "\n",
        "$w_{h1} \\cdot \\delta_y = 0.4 \\cdot (-0.0084) \\approx -0.00336$\n",
        "Sigmoid derivative: $\\sigma'(z_{h1}) = h_1(1 - h_1) = 0.6106(1 - 0.6106) \\approx 0.2379$\n",
        "$\\delta_{h1} = -0.00336 \\cdot 0.2379 \\approx -0.0008$\n",
        "\n",
        "$$\\delta_{h2} = \\left( w_{h2} \\cdot \\delta_y \\right) \\cdot \\sigma'(z_{h2})$$\n",
        "\n",
        "$w_{h2} \\cdot \\delta_y = 0.45 \\cdot (-0.0084) \\approx -0.00378$\n",
        "Sigmoid derivative: $\\sigma'(z_{h2}) = h_2(1 - h_2) = 0.6177(1 - 0.6177) \\approx 0.2362$\n",
        "$\\delta_{h2} = -0.00378 \\cdot 0.2362 \\approx -0.00089$\n",
        "\n",
        "\n",
        "Gradients for weights to hidden layer:$$\\frac{\\partial E}{\\partial w_{11}} = \\delta_{h1} \\cdot x_1 = -0.0008 \\cdot 0.5 \\approx -0.0004$$$$\\frac{\\partial E}{\\partial w_{21}} = \\delta_{h1} \\cdot x_2 = -0.0008 \\cdot 0.1 \\approx -0.00008$$$$\\frac{\\partial E}{\\partial w_{12}} = \\delta_{h2} \\cdot x_1 = -0.00089 \\cdot 0.5 \\approx -0.000445$$$$\\frac{\\partial E}{\\partial w_{22}} = \\delta_{h2} \\cdot x_2 = -0.00089 \\cdot 0.1 \\approx -0.000089$$\n",
        "\n",
        "Gradient for hidden bias:$$\\frac{\\partial E}{\\partial b_h} = \\delta_{h1} + \\delta_{h2} = -0.0008 + (-0.00089) \\approx -0.00169$$"
      ],
      "metadata": {
        "id": "bvuHid2beyQw"
      },
      "id": "bvuHid2beyQw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'> Step 3: Update Weights and Biases </font>\n",
        "\n",
        "Using gradient descent: $w = w - \\eta \\cdot \\frac{\\partial E}{\\partial w}$.\n",
        "Output Layer:\n",
        "\n",
        "$w_{h1} = 0.4 - 0.5 \\cdot (-0.00513) = 0.4 + 0.002565 \\approx 0.4026$\n",
        "$w_{h2} = 0.45 - 0.5 \\cdot (-0.00519) = 0.45 + 0.002595 \\approx 0.4526$\n",
        "$b_y = 0.6 - 0.5 \\cdot (-0.0084) = 0.6 + 0.0042 \\approx 0.6042$\n",
        "\n",
        "Hidden Layer:\n",
        "\n",
        "$w_{11} = 0.15 - 0.5 \\cdot (-0.0004) = 0.15 + 0.0002 \\approx 0.1502$\n",
        "$w_{21} = 0.25 - 0.5 \\cdot (-0.00008) = 0.25 + 0.00004 \\approx 0.25004$\n",
        "$w_{12} = 0.2 - 0.5 \\cdot (-0.000445) = 0.2 + 0.0002225 \\approx 0.2002$\n",
        "$w_{22} = 0.3 - 0.5 \\cdot (-0.000089) = 0.3 + 0.0000445 \\approx 0.30004$\n",
        "$b_h = 0.35 - 0.5 \\cdot (-0.00169) = 0.35 + 0.000845 \\approx 0.3508$"
      ],
      "metadata": {
        "id": "_80Z96Xee3u-"
      },
      "id": "_80Z96Xee3u-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation Application & Problems\n",
        "\n",
        "#### Applications of Backpropagation\n",
        "\n",
        "### In Machine Learning (ML)\n",
        "* Backpropagation is the backbone of training artificial neural networks (ANNs) and is widely used across ML domains:\n",
        "\n",
        "    - **Image Recognition**: Optimizes weights in convolutional neural networks (CNNs) for tasks like object detection (e.g., identifying objects in images).\n",
        "    - **Natural Language Processing (NLP)**: Adjusts weights in recurrent neural networks (RNNs) and transformers for tasks like machine translation and text generation.\n",
        "    - **Speech Recognition**: Fine-tunes deep neural networks to convert audio to text by minimizing prediction errors.\n",
        "* **Control Systems**: Optimizes parameters in control algorithms for robotics or autonomous systems, adjusting to minimize error in system dynamics.\n",
        "  \n",
        "  $E = \\frac{1}{2} (y_{\\text{desired}} - y_{\\text{actual}})^2$\n",
        "  \n",
        "- **Physics Simulations**: Used to fit models to experimental data, such as optimizing parameters in computational fluid dynamics.\n",
        "- **Economics**: Adjusts weights in econometric models to predict economic trends or optimize resource allocation.\n",
        "- **Signal Processing**: Fine-tunes filters in adaptive signal processing to minimize noise in communication systems.\n",
        "\n",
        "#### Problems and Challenges with Backpropagation\n",
        "- **Vanishing/Exploding Gradients**: Throughout Backpropagation workflow, gradients are multiplied across layers, this can lead to two extreme cases:\n",
        "\n",
        "    Gradients can shrink $( \\frac{\\partial E}{\\partial w} \\to 0 )$ or grow  $( \\frac{\\partial E}{\\partial w} \\to \\infty )$\n",
        "\n",
        "<font color='red'>This is also called Backpropagation through Time Problem (BPTT)</font>.\n",
        "\n",
        "- **Overfitting**: Models may memorize training data, requiring regularization like dropout.\n",
        "\n",
        "- **Computational Complexity**: Training large networks demands significant computational resources (e.g., GPUs).\n",
        "- **Local Minima**: The algorithm may converge to suboptimal solutions, though this is less critical in high-dimensional spaces.\n",
        "\n",
        "  $E(w) = \\frac{1}{2} \\sum (t - y)^2$\n",
        "\n",
        "- **Initialization Sensitivity**: Poor weight initialization can lead to slow convergence, addressed by methods like Xavier initialization.\n",
        "\n",
        "Backpropagation’s versatility makes it a powerful tool across domains, though its challenges require tailored solutions depending on the application."
      ],
      "metadata": {
        "id": "j_5PZniPmEWm"
      },
      "id": "j_5PZniPmEWm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparision & Summary"
      ],
      "metadata": {
        "id": "rkVRd5AWs5XR"
      },
      "id": "rkVRd5AWs5XR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Comparison between other Differentiation methods\n",
        "1. Compare to Numerical Differentiation:\n",
        "\n",
        "| **Aspect**               | **Numerical Differentiation**                              | **Automatic Differentiation**                            |\n",
        "|---------------------------|-----------------------------------------------------------|--------------------------------------------------------|\n",
        "| **Accuracy**             | Approximates derivatives (e.g., $ f'(x) \\approx \\frac{f(x + h) - f(x)}{h} $). Prone to round-off and truncation errors, accuracy decreases with complex functions or noisy data. | Exact derivatives up to machine precision via computational graph. Avoids approximation errors. |\n",
        "| **Computational Efficiency** | Requires $ O(n) $ function evaluations for $ n $ parameters (e.g., central differences). Inefficient for **high-dimensional problems**. | Operates at the cost of function evaluation, $ O(n) $ for forward mode or $ O(n) $ per output in reverse mode. **Highly efficient for complex models**. |\n",
        "| **Implementation Complexity** | Simple to implement (e.g., finite difference formula). Requires tuning step size $ h $ to balance accuracy and error. | Requires computational graph (handled by frameworks like TensorFlow or PyTorch). Complex manually but integrated into ML libraries. |\n",
        "| **Applicability**        | Suitable for simple functions or black-box models where derivatives are unavailable. Struggles with high-dimensional or noisy data. | Excels in ML, optimizing deep networks with thousands of parameters. Applicable to any computable function, including those with control flow. |\n",
        "| **Memory Usage**         | Minimal memory, stores only function evaluations.         | Moderate to high memory usage (forward mode stores intermediates, reverse mode stores graph). Scales with outputs in reverse mode. |\n",
        "| **Mathematical Insight** | $ f'(x) \\approx \\frac{f(x + h) - f(x - h)}{2h} $ (error $ O(h^2) $). | Computes $ \\frac{\\partial E}{\\partial w} $ by chaining partial derivatives (e.g., $ \\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w} $), exact and efficient. |\n",
        "\n",
        "2. Compare to Symbolic Differentiation:\n",
        "\n",
        "| **Aspect**               | **Symbolic Differentiation**                              | **Automatic Differentiation**                            |\n",
        "|---------------------------|-----------------------------------------------------------|--------------------------------------------------------|\n",
        "| **Accuracy**             | Provides exact derivatives (e.g., $ \\frac{d}{dx}(x^2) = 2x $). No approximation errors, but suffers from expression swell with large expressions. | Delivers exact derivatives up to machine precision via computational graph. |\n",
        "| **Computational Efficiency** | Computationally expensive due to symbolic manipulation and simplification. Exponential growth in complexity with nested functions. | Operates at the cost of function evaluation, $ O(n) $ for forward mode or $ O(n) $ per output in reverse mode. Highly efficient for complex models. |\n",
        "| **Implementation Complexity** | Requires a symbolic math engine (e.g., SymPy), complex for large systems. Impractical for dynamic computations. | Requires computational graph (handled by frameworks like TensorFlow or PyTorch). Complex manually but integrated into ML libraries. |\n",
        "| **Applicability**        | Ideal for analytical derivations or small, static expressions. Impractical for large-scale, data-driven models. | Excels in ML, optimizing deep networks with thousands of parameters. Applicable to any computable function, including those with control flow. |\n",
        "| **Memory Usage**         | High memory usage due to storing symbolic expressions.   | Moderate to high memory usage (forward mode stores intermediates, reverse mode stores graph). Scales with outputs in reverse mode. |\n",
        "| **Mathematical Insight** | $ \\frac{d}{dx}(e^{x^2}) = 2x e^{x^2} $, exact but grows with complexity. | Computes $ \\frac{\\partial E}{\\partial w} $ by chaining partial derivatives (e.g., $ \\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w} $), exact and efficient. |\n",
        "\n",
        "3. Insights:\n",
        "- **Numerical Differentiation** is simple but inaccurate and inefficient for large systems.\n",
        "- **Symbolic Differentiation** is precise but impractical for dynamic, large-scale problems.\n",
        "- **Automatic Differentiation** strikes a balance, offering exactness and efficiency, making it the preferred choice for modern machine."
      ],
      "metadata": {
        "id": "OlHySnxGqg6g"
      },
      "id": "OlHySnxGqg6g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison between Forward & Backward mode\n",
        "| Feature                     | Forward Mode AD                          | Backward Mode AD                         |\n",
        "|-----------------------------|------------------------------------------|------------------------------------------|\n",
        "| **Definition**              | Computes derivatives alongside function evaluation, propagating derivatives forward. | Computes derivatives by backpropagating gradients from output to input. |\n",
        "| **Computation Direction**   | Forward: from inputs to outputs.         | Backward: from outputs to inputs.        |\n",
        "| **Derivative Propagation**  | Accumulates derivatives (tangents) during forward pass. | Accumulates gradients during backward pass. |\n",
        "| **Efficiency**              | Efficient for functions with few inputs and many outputs (e.g., $ f: \\mathbb{R}^n \\to \\mathbb{R}^m $, $ n \\ll m $). | Efficient for functions with many inputs and few outputs (e.g., $ f: \\mathbb{R}^n \\to \\mathbb{R}^m $, $ n \\gg m $). |\n",
        "| **Memory Usage**            | Lower memory for small input dimensions; stores intermediate tangents. | Higher memory; stores intermediate activations for backward pass. |\n",
        "| **Use Case**                | Real-time systems, sensitivity analysis for small input spaces. | Neural network training, optimization problems with scalar loss. |\n",
        "| **Implementation Complexity** | Simpler to implement for small systems.  | More complex, requires reverse computation graph. |\n",
        "| **Example Application**     | Computing Jacobians for small systems.   | Gradient computation in deep learning.   |\n",
        "| **Scalability**             | Scales poorly with large input dimensions. | Scales well with large input dimensions. |\n",
        "\n",
        "### Recommended usage for each mode\n",
        "**Forward Mode:**\n",
        "* Good when you have a small number of inputs (e.g., 2–3 parameters).\n",
        "* Ideal for Jacobian-vector products: e.g., computing directional derivatives.\n",
        "\n",
        "**Backward Mode:**\n",
        "* When you have many inputs and one output (e.g., loss function in neural nets).\n",
        "* Ideal for training machine learning models (via backpropagation)."
      ],
      "metadata": {
        "id": "NWLyE0NPtll3"
      },
      "id": "NWLyE0NPtll3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "[Automatic Differentiation in Machine Learning: a Survey](https://arxiv.org/pdf/1502.05767)\n",
        "\n",
        "[Automatic Differentiation D2l](https://d2l.ai/chapter_preliminaries/autograd.html)\n",
        "\n",
        "[Youtube](https://www.youtube.com/watch?v=wG_nF1awSSY)\n",
        "\n"
      ],
      "metadata": {
        "id": "xXaZ5N4u1HCq"
      },
      "id": "xXaZ5N4u1HCq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n",
        "\n",
        "Code, implement AutoDiff, explain with an example. show backward and forward\n",
        "Ex: f(x1, x2) = ln(x1) + x1x2 - sin(x2)\n",
        "\n",
        "https://d2l.ai/chapter_preliminaries/autograd.html\n",
        "\n",
        "https://homepages.inf.ed.ac.uk/htang2/mlg2022/tutorial-3.pdf\n"
      ],
      "metadata": {
        "id": "9o5Q0ZG0KVQ2"
      },
      "id": "9o5Q0ZG0KVQ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises (1-4)\n",
        "\n",
        "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
        "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
        "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
        "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
        "\n"
      ],
      "metadata": {
        "id": "hk4f988CNvh7"
      },
      "id": "hk4f988CNvh7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises (5-8)\n",
        "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
        "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
        "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
        "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved."
      ],
      "metadata": {
        "id": "KievrNAtN8MC"
      },
      "id": "KievrNAtN8MC"
    },
    {
      "cell_type": "markdown",
      "id": "4144c129",
      "metadata": {
        "origin_pos": 1,
        "id": "4144c129"
      },
      "source": [
        "***Reference: origin notebook of d2l as below***\n",
        "\n",
        "# Automatic Differentiation\n",
        ":label:`sec_autograd`\n",
        "\n",
        "Recall from :numref:`sec_calculus`\n",
        "that calculating derivatives is the crucial step\n",
        "in all the optimization algorithms\n",
        "that we will use to train deep networks.\n",
        "While the calculations are straightforward,\n",
        "working them out by hand can be tedious and error-prone,\n",
        "and these issues only grow\n",
        "as our models become more complex.\n",
        "\n",
        "Fortunately all modern deep learning frameworks\n",
        "take this work off our plates\n",
        "by offering *automatic differentiation*\n",
        "(often shortened to *autograd*).\n",
        "As we pass data through each successive function,\n",
        "the framework builds a *computational graph*\n",
        "that tracks how each value depends on others.\n",
        "To calculate derivatives,\n",
        "automatic differentiation\n",
        "works backwards through this graph\n",
        "applying the chain rule.\n",
        "The computational algorithm for applying the chain rule\n",
        "in this fashion is called *backpropagation*.\n",
        "\n",
        "While autograd libraries have become\n",
        "a hot concern over the past decade,\n",
        "they have a long history.\n",
        "In fact the earliest references to autograd\n",
        "date back over half of a century :cite:`Wengert.1964`.\n",
        "The core ideas behind modern backpropagation\n",
        "date to a PhD thesis from 1980 :cite:`Speelpenning.1980`\n",
        "and were further developed in the late 1980s :cite:`Griewank.1989`.\n",
        "While backpropagation has become the default method\n",
        "for computing gradients, it is not the only option.\n",
        "For instance, the Julia programming language employs\n",
        "forward propagation :cite:`Revels.Lubin.Papamarkou.2016`.\n",
        "Before exploring methods,\n",
        "let's first master the autograd package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130439cd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:08.286501Z",
          "iopub.status.busy": "2023-08-18T19:26:08.285693Z",
          "iopub.status.idle": "2023-08-18T19:26:10.052257Z",
          "shell.execute_reply": "2023-08-18T19:26:10.050994Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "130439cd"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ab3850",
      "metadata": {
        "origin_pos": 6,
        "id": "e2ab3850"
      },
      "source": [
        "## A Simple Function\n",
        "\n",
        "Let's assume that we are interested\n",
        "in (**differentiating the function\n",
        "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to the column vector $\\mathbf{x}$.**)\n",
        "To start, we assign `x` an initial value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4253cfab",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.056833Z",
          "iopub.status.busy": "2023-08-18T19:26:10.055871Z",
          "iopub.status.idle": "2023-08-18T19:26:10.084858Z",
          "shell.execute_reply": "2023-08-18T19:26:10.083727Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "4253cfab",
        "outputId": "a4817393-fc1c-4795-875e-4d2861054425"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(4.0)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75614b0",
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "e75614b0"
      },
      "source": [
        "[**Before we calculate the gradient\n",
        "of $y$ with respect to $\\mathbf{x}$,\n",
        "we need a place to store it.**]\n",
        "In general, we avoid allocating new memory\n",
        "every time we take a derivative\n",
        "because deep learning requires\n",
        "successively computing derivatives\n",
        "with respect to the same parameters\n",
        "a great many times,\n",
        "and we might risk running out of memory.\n",
        "Note that the gradient of a scalar-valued function\n",
        "with respect to a vector $\\mathbf{x}$\n",
        "is vector-valued with\n",
        "the same shape as $\\mathbf{x}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a001d1e",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.088716Z",
          "iopub.status.busy": "2023-08-18T19:26:10.087816Z",
          "iopub.status.idle": "2023-08-18T19:26:10.092878Z",
          "shell.execute_reply": "2023-08-18T19:26:10.091740Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "2a001d1e"
      },
      "outputs": [],
      "source": [
        "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
        "x.requires_grad_(True)\n",
        "x.grad  # The gradient is None by default"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e74bc02",
      "metadata": {
        "origin_pos": 15,
        "id": "2e74bc02"
      },
      "source": [
        "(**We now calculate our function of `x` and assign the result to `y`.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3bd777",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.096336Z",
          "iopub.status.busy": "2023-08-18T19:26:10.095772Z",
          "iopub.status.idle": "2023-08-18T19:26:10.105236Z",
          "shell.execute_reply": "2023-08-18T19:26:10.104075Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "6e3bd777",
        "outputId": "d6372de1-8216-4343-88fa-ca454f727629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3067490",
      "metadata": {
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "c3067490"
      },
      "source": [
        "[**We can now take the gradient of `y`\n",
        "with respect to `x`**] by calling\n",
        "its `backward` method.\n",
        "Next, we can access the gradient\n",
        "via `x`'s `grad` attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b134ae",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.108600Z",
          "iopub.status.busy": "2023-08-18T19:26:10.108011Z",
          "iopub.status.idle": "2023-08-18T19:26:10.160854Z",
          "shell.execute_reply": "2023-08-18T19:26:10.159702Z"
        },
        "origin_pos": 25,
        "tab": [
          "pytorch"
        ],
        "id": "21b134ae",
        "outputId": "3c3b96c9-3955-4c18-f6ed-fe92feaf5957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d1390b",
      "metadata": {
        "origin_pos": 28,
        "id": "17d1390b"
      },
      "source": [
        "(**We already know that the gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.**)\n",
        "We can now verify that the automatic gradient computation\n",
        "and the expected result are identical.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5030e37d",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.164665Z",
          "iopub.status.busy": "2023-08-18T19:26:10.163930Z",
          "iopub.status.idle": "2023-08-18T19:26:10.171033Z",
          "shell.execute_reply": "2023-08-18T19:26:10.169923Z"
        },
        "origin_pos": 30,
        "tab": [
          "pytorch"
        ],
        "id": "5030e37d",
        "outputId": "da15a8ec-929d-48c0-d80f-b3a536de4f8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad == 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da440e48",
      "metadata": {
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ],
        "id": "da440e48"
      },
      "source": [
        "[**Now let's calculate\n",
        "another function of `x`\n",
        "and take its gradient.**]\n",
        "Note that PyTorch does not automatically\n",
        "reset the gradient buffer\n",
        "when we record a new gradient.\n",
        "Instead, the new gradient\n",
        "is added to the already-stored gradient.\n",
        "This behavior comes in handy\n",
        "when we want to optimize the sum\n",
        "of multiple objective functions.\n",
        "To reset the gradient buffer,\n",
        "we can call `x.grad.zero_()` as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add5cf4b",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "20"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.174691Z",
          "iopub.status.busy": "2023-08-18T19:26:10.173957Z",
          "iopub.status.idle": "2023-08-18T19:26:10.181847Z",
          "shell.execute_reply": "2023-08-18T19:26:10.180759Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "add5cf4b",
        "outputId": "c3ec729a-b1fb-481f-e7f5-655516e2b346"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()  # Reset the gradient\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdd4c0c",
      "metadata": {
        "origin_pos": 40,
        "id": "8bdd4c0c"
      },
      "source": [
        "## Backward for Non-Scalar Variables\n",
        "\n",
        "When `y` is a vector,\n",
        "the most natural representation\n",
        "of the derivative of  `y`\n",
        "with respect to a vector `x`\n",
        "is a matrix called the *Jacobian*\n",
        "that contains the partial derivatives\n",
        "of each component of `y`\n",
        "with respect to each component of `x`.\n",
        "Likewise, for higher-order `y` and `x`,\n",
        "the result of differentiation could be an even higher-order tensor.\n",
        "\n",
        "While Jacobians do show up in some\n",
        "advanced machine learning techniques,\n",
        "more commonly we want to sum up\n",
        "the gradients of each component of `y`\n",
        "with respect to the full vector `x`,\n",
        "yielding a vector of the same shape as `x`.\n",
        "For example, we often have a vector\n",
        "representing the value of our loss function\n",
        "calculated separately for each example among\n",
        "a *batch* of training examples.\n",
        "Here, we just want to (**sum up the gradients\n",
        "computed individually for each example**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dda7124",
      "metadata": {
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "9dda7124"
      },
      "source": [
        "Because deep learning frameworks vary\n",
        "in how they interpret gradients of\n",
        "non-scalar tensors,\n",
        "PyTorch takes some steps to avoid confusion.\n",
        "Invoking `backward` on a non-scalar elicits an error\n",
        "unless we tell PyTorch how to reduce the object to a scalar.\n",
        "More formally, we need to provide some vector $\\mathbf{v}$\n",
        "such that `backward` will compute\n",
        "$\\mathbf{v}^\\top \\partial_{\\mathbf{x}} \\mathbf{y}$\n",
        "rather than $\\partial_{\\mathbf{x}} \\mathbf{y}$.\n",
        "This next part may be confusing,\n",
        "but for reasons that will become clear later,\n",
        "this argument (representing $\\mathbf{v}$) is named `gradient`.\n",
        "For a more detailed description, see Yang Zhang's\n",
        "[Medium post](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1baa40bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.185096Z",
          "iopub.status.busy": "2023-08-18T19:26:10.184685Z",
          "iopub.status.idle": "2023-08-18T19:26:10.192537Z",
          "shell.execute_reply": "2023-08-18T19:26:10.191435Z"
        },
        "origin_pos": 45,
        "tab": [
          "pytorch"
        ],
        "id": "1baa40bd",
        "outputId": "dc4b39c4-6948-4526-f167-7118248b07d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffbd2c9d",
      "metadata": {
        "origin_pos": 48,
        "id": "ffbd2c9d"
      },
      "source": [
        "## Detaching Computation\n",
        "\n",
        "Sometimes, we wish to [**move some calculations\n",
        "outside of the recorded computational graph.**]\n",
        "For example, say that we use the input\n",
        "to create some auxiliary intermediate terms\n",
        "for which we do not want to compute a gradient.\n",
        "In this case, we need to *detach*\n",
        "the respective computational graph\n",
        "from the final result.\n",
        "The following toy example makes this clearer:\n",
        "suppose we have `z = x * y` and `y = x * x`\n",
        "but we want to focus on the *direct* influence of `x` on `z`\n",
        "rather than the influence conveyed via `y`.\n",
        "In this case, we can create a new variable `u`\n",
        "that takes the same value as `y`\n",
        "but whose *provenance* (how it was created)\n",
        "has been wiped out.\n",
        "Thus `u` has no ancestors in the graph\n",
        "and gradients do not flow through `u` to `x`.\n",
        "For example, taking the gradient of `z = x * u`\n",
        "will yield the result `u`,\n",
        "(not `3 * x * x` as you might have\n",
        "expected since `z = x * x * x`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107ac041",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "21"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.196001Z",
          "iopub.status.busy": "2023-08-18T19:26:10.195456Z",
          "iopub.status.idle": "2023-08-18T19:26:10.203246Z",
          "shell.execute_reply": "2023-08-18T19:26:10.202155Z"
        },
        "origin_pos": 50,
        "tab": [
          "pytorch"
        ],
        "id": "107ac041",
        "outputId": "e07fe5dc-f226-4cf2-8042-4c499f7ed519"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0378e1f",
      "metadata": {
        "origin_pos": 53,
        "id": "e0378e1f"
      },
      "source": [
        "Note that while this procedure\n",
        "detaches `y`'s ancestors\n",
        "from the graph leading to `z`,\n",
        "the computational graph leading to `y`\n",
        "persists and thus we can calculate\n",
        "the gradient of `y` with respect to `x`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8c674b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.206880Z",
          "iopub.status.busy": "2023-08-18T19:26:10.206001Z",
          "iopub.status.idle": "2023-08-18T19:26:10.213592Z",
          "shell.execute_reply": "2023-08-18T19:26:10.212476Z"
        },
        "origin_pos": 55,
        "tab": [
          "pytorch"
        ],
        "id": "cb8c674b",
        "outputId": "772ee87b-f492-48b8-e329-e0ef9f98ca18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f056ce",
      "metadata": {
        "origin_pos": 58,
        "id": "76f056ce"
      },
      "source": [
        "## Gradients and Python Control Flow\n",
        "\n",
        "So far we reviewed cases where the path from input to output\n",
        "was well defined via a function such as `z = x * x * x`.\n",
        "Programming offers us a lot more freedom in how we compute results.\n",
        "For instance, we can make them depend on auxiliary variables\n",
        "or condition choices on intermediate results.\n",
        "One benefit of using automatic differentiation\n",
        "is that [**even if**] building the computational graph of\n",
        "(**a function required passing through a maze of Python control flow**)\n",
        "(e.g., conditionals, loops, and arbitrary function calls),\n",
        "(**we can still calculate the gradient of the resulting variable.**)\n",
        "To illustrate this, consider the following code snippet where\n",
        "the number of iterations of the `while` loop\n",
        "and the evaluation of the `if` statement\n",
        "both depend on the value of the input `a`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83327c2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.218214Z",
          "iopub.status.busy": "2023-08-18T19:26:10.217554Z",
          "iopub.status.idle": "2023-08-18T19:26:10.222956Z",
          "shell.execute_reply": "2023-08-18T19:26:10.221858Z"
        },
        "origin_pos": 60,
        "tab": [
          "pytorch"
        ],
        "id": "a83327c2"
      },
      "outputs": [],
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189f6785",
      "metadata": {
        "origin_pos": 63,
        "id": "189f6785"
      },
      "source": [
        "Below, we call this function, passing in a random value, as input.\n",
        "Since the input is a random variable,\n",
        "we do not know what form\n",
        "the computational graph will take.\n",
        "However, whenever we execute `f(a)`\n",
        "on a specific input, we realize\n",
        "a specific computational graph\n",
        "and can subsequently run `backward`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ef0264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.227364Z",
          "iopub.status.busy": "2023-08-18T19:26:10.226919Z",
          "iopub.status.idle": "2023-08-18T19:26:10.232880Z",
          "shell.execute_reply": "2023-08-18T19:26:10.231773Z"
        },
        "origin_pos": 65,
        "tab": [
          "pytorch"
        ],
        "id": "c5ef0264"
      },
      "outputs": [],
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51065133",
      "metadata": {
        "origin_pos": 68,
        "id": "51065133"
      },
      "source": [
        "Even though our function `f` is, for demonstration purposes, a bit contrived,\n",
        "its dependence on the input is quite simple:\n",
        "it is a *linear* function of `a`\n",
        "with piecewise defined scale.\n",
        "As such, `f(a) / a` is a vector of constant entries\n",
        "and, moreover, `f(a) / a` needs to match\n",
        "the gradient of `f(a)` with respect to `a`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab14ef91",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.237298Z",
          "iopub.status.busy": "2023-08-18T19:26:10.236886Z",
          "iopub.status.idle": "2023-08-18T19:26:10.243577Z",
          "shell.execute_reply": "2023-08-18T19:26:10.242480Z"
        },
        "origin_pos": 70,
        "tab": [
          "pytorch"
        ],
        "id": "ab14ef91",
        "outputId": "4d85cfd2-c7b3-460e-bf7e-76fdf88e0f50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.grad == d / a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a992f28c",
      "metadata": {
        "origin_pos": 73,
        "id": "a992f28c"
      },
      "source": [
        "Dynamic control flow is very common in deep learning.\n",
        "For instance, when processing text, the computational graph\n",
        "depends on the length of the input.\n",
        "In these cases, automatic differentiation\n",
        "becomes vital for statistical modeling\n",
        "since it is impossible to compute the gradient *a priori*.\n",
        "\n",
        "## Discussion\n",
        "\n",
        "You have now gotten a taste of the power of automatic differentiation.\n",
        "The development of libraries for calculating derivatives\n",
        "both automatically and efficiently\n",
        "has been a massive productivity booster\n",
        "for deep learning practitioners,\n",
        "liberating them so they can focus on less menial.\n",
        "Moreover, autograd lets us design massive models\n",
        "for which pen and paper gradient computations\n",
        "would be prohibitively time consuming.\n",
        "Interestingly, while we use autograd to *optimize* models\n",
        "(in a statistical sense)\n",
        "the *optimization* of autograd libraries themselves\n",
        "(in a computational sense)\n",
        "is a rich subject\n",
        "of vital interest to framework designers.\n",
        "Here, tools from compilers and graph manipulation\n",
        "are leveraged to compute results\n",
        "in the most expedient and memory-efficient manner.\n",
        "\n",
        "For now, try to remember these basics: (i) attach gradients to those variables with respect to which we desire derivatives; (ii) record the computation of the target value; (iii) execute the backpropagation function; and  (iv) access the resulting gradient.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
        "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
        "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
        "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
        "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
        "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
        "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
        "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0ab97d",
      "metadata": {
        "origin_pos": 75,
        "tab": [
          "pytorch"
        ],
        "id": "4c0ab97d"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/35)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}