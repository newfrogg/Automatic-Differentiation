{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newfrogg/Automatic-Differentiation/blob/main/autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6-FUvYwvWEJF",
      "metadata": {
        "id": "6-FUvYwvWEJF"
      },
      "source": [
        "| Mục lớn                             | Mục nhỏ                                                         |           |\n",
        "|-------------------------------------|-----------------------------------------------------------------|-----------|\n",
        "| Introduction                        |                                                                 |  1 người  |\n",
        "|                                     | Differential Calculus                                           |  Tử Quân  |\n",
        "|                                     | Rules of Calculus                                               |  Tử Quân  |\n",
        "|                                     | Multivariate Chain Rule                                         |  Tử Quân  |\n",
        "|                                     | Geometry of Gradients and Gradient Descent                      |  Tử Quân  |\n",
        "| What Autodiff Isn’t                 |                                                                 |  3 người  |\n",
        "|                                     | Autodiff is not finite differences (numerical differentiation)  |  An Đông  |\n",
        "|                                     | Autodiff is not symbolic differentiation                        |  An Đông  |\n",
        "|                                     | What autodiff is ?                                              |  An Đông  |\n",
        "|                                     | Types of Autodiff (explain forward, backward)                   |  An Đông  |\n",
        "|                                     | Backpropagation Algorithm                                       |  Gia Hinh |\n",
        "|                                     | Gradient-Based Optimization                                     |  Gia Hinh |\n",
        "|                                     | give an example and describe the autodiff step-by-step workflow |  Gia Hinh |\n",
        "|                                     | Summary. Compare betwwen each type advancement of each types    |  Gia Hinh |\n",
        "| Visualization. Code, Implementation |                                                                 |  1 người  |\n",
        "|                                     | Code                                                            | Bảo Lương |\n",
        "| Exercise                            |                                                                 |  2 người  |\n",
        "|                                     | Q1-4                                                            |   Triết   |\n",
        "|                                     | Q5-8                                                            | Minh Quân |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LRVgmL2uBTRS",
      "metadata": {
        "id": "LRVgmL2uBTRS"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "## Differential Calculus\n",
        "## Rules of Calculus\n",
        "## Geometry of Gradients and Gradient Descent\n",
        "\n",
        "## Multivariate Chain Rule\n",
        "## The Backpropagation Algorithm\n",
        "\n",
        "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html\n",
        "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aQgVWYQ1I9AG",
      "metadata": {
        "id": "aQgVWYQ1I9AG"
      },
      "source": [
        "<div align=\"justify\">\n",
        "\n",
        "## **What AD Is Not ?**\n",
        "Without a clear introduction, many people place automatic differentiation (AD) in the same category as numerical differentiation or symbolic differentiation. This confusion is understandable because AD delivers numerical values for derivatives while using the rules of symbolic calculus, yet it only stores the resulting numbers rather than the complete algebraic expressions. This dual character means AD operates in a space that overlaps both approaches (Griewank, 2003). To start, we will explain how AD differs from those two familiar methods and in many respects exceeds their capabilities.\n",
        "\n",
        "### **AD Is Not Numerical Differentiation**\n",
        "Numerical differentiation estimates a derivative by applying finite difference formulas to function values sampled at specific points (Burden & Faires, 2001). At its core, the method springs directly from the limit definition of the derivative. For example, for a multivariate function $f∶R^n→R$, one can approximate the gradient $(\\frac{∂f}{∂x_1 },…,\\frac{∂f}{∂x_n})$ using:\n",
        "\n",
        "$$\\frac{∂f(x)}{∂x_i} \\approx \\frac{f(x+he_i )-f(x)}{h}\\text{ (Eq.1)}$$\n",
        "\n",
        "\n",
        "where $e_i$ is the $i$-th unit vector and $h>0$ is a small step size. Its main upside is simplicity of implementation; the downsides are that computing a gradient in $n$ dimensions needs $O(n)$ separate evaluations of $f$ and that you must choose the step size $h$ with great care.\n",
        "\n",
        "Approximating derivatives numerically is fundamentally unstable and suffers from poor conditioning, except when using complex variable techniques that apply only to certain holomorphic functions (Fornberg, 1981). This instability arises from truncation errors and round off errors introduced by finite computational precision and the selection of the step size $h$. While the truncation error vanishes as $h$ approaches zero $(h→0)$, reducing $h$ increases the round off error until it becomes the primary source of inaccuracy.\n",
        "\n",
        "Various methods have been proposed to reduce the approximation error in numerical differentiation. For example, using a central difference formula:\n",
        "\n",
        "$$ \\frac{∂f(x)}{∂x_i} \\approx \\frac{f(x+he_i )-f(x-he_i)}{2h} + O(h^2 )\\text{ (Eq.2)}$$\n",
        "\n",
        "in which the leading order errors cancel out, pushing the truncation error from first order to second order in $h$. In one dimension, this central difference (Eq. 2) is just as expensive as the forward difference (Eq. 1) because both require two evaluations of $f$. But in higher dimensions the cost grows quickly: forming the full Jacobian of a map $f∶R^n→R^m$ using central differences demands $2mn$ function evaluations, forcing a trade off between accuracy and computational expense.\n",
        "\n",
        "Advanced approaches to enhance numerical differentiation, such as higher order finite difference schemes, Richardson extrapolation toward the limit (Brezinski & Zaglia, 1991), and weighted sum differential quadrature methods (Bert & Malik, 1996), all raise computational cost, never fully remove approximation errors, and remain highly vulnerable to floating point truncation. Moreover, the $O(n)$ expense of computing an $n$-dimensional gradient makes numerical differentiation impractical for machine learning, where $n$ can reach millions or billions in cutting edge deep networks (Shazeer et al., 2017). By contrast, deep learning models tolerate approximation errors reasonably well thanks to their intrinsic resilience to numerical noise (Gupta et al., 2015).\n",
        "\n",
        "### **AD Is Not Symbolic Differentiation**\n",
        "\n",
        "Symbolic differentiation refers to the automated transformation of mathematical expressions to derive their exact derivative formulas (Grabmeier and Kaltofen, 2003). This is achieved by systematically applying differentiation rules, for example:\n",
        "\n",
        "$$ \\frac{d}{dx}(f(x)+g(x)) \\approx \\frac{d}{dx}(f(x)) + \\frac{d}{dx}(g(x))\\text{ (Eq.3)}$$\n",
        "\n",
        "$$ \\frac{d}{dx}(f(x)g(x)) \\approx (\\frac{d}{dx}(f(x)))g(x) + f(x)(\\frac{d}{dx}(g(x)))\\text{ (Eq.4)}$$\n",
        "\n",
        "When mathematical formulas are stored as structured data, differentiating an expression tree by symbolic means becomes a fully mechanical task, an idea that dates back to the earliest days of calculus (Leibniz, 1685). Today, this approach is implemented in computer algebra systems like Mathematica, Maxima, and Maple, as well as in machine learning libraries such as Theano.\n",
        "\n",
        "In optimization, symbolic derivatives can reveal the underlying structure of a problem and sometimes even yield closed-form solutions for extrema( such as solving for $\\frac{d}{dx} f(x)=0$) bypassing the need for further derivative evaluations. However, symbolic derivatives often expand into expressions that grow exponentially in size compared to the original formula, making them impractical for efficient runtime computation of derivative values.\n",
        "\n",
        "Take the function $h(x)=f(x)h(x)$ and apply the product rule. Both $h(x)$ and its derivative $(\\frac{d}{dx} h(x))$ share the subexpressions $f(x)$ and $g(x)$. On the derivative’s right hand side, the terms $f(x)$ and $\\frac{d}{dx} f(x)$ appear separately. If you immediately expand $\\frac{d}{dx} f(x)$ by plugging in its symbolic derivative, you duplicate every computation common to $f(x)$ and $\\frac{d}{dx} f(x)$. Repeating this process without optimization causes the symbolic representation to grow exponentially, making evaluation extremely slow. This issue is called expression swell.\n",
        "\n",
        "When our focus is on obtaining accurate numerical derivatives rather than preserving their symbolic representations, we can greatly reduce computational effort by storing only the numerical values of intermediate sub expressions. To make this even more efficient, we interleave differentiation and simplification at each step. This concept is the foundation of automatic differentiation: at every elementary operation, perform the symbolic differentiation while simultaneously tracking and storing the numerical results alongside the primary function’s computation. This procedure defines forward mode AD, which we will discuss in the next section.\n",
        "\n",
        "## **What is Autodiff and type of Autodiff ?**\n",
        "### **What is Autodiff ?**\n",
        "\n",
        "Automatic differentiation can be viewed as an alternative way to execute a program in which the original calculations are extended to include derivative computations. Every numerical routine ultimately breaks down into a finite collection of basic operations whose derivatives are known (Verma, 2000; Griewank and Walther, 2008). By applying the chain rule to combine the derivatives of these basic operations, one obtains the derivative of the entire composite function. Common basic operations include addition, subtraction, multiplication, division, the sign operation, and transcendental functions such as exponentials, logarithms, and trigonometric functions.\n",
        "\n",
        "To implement this, we can leverage an evaluation trace. An evaluation trace is a special table that keeps track of intermediate variables as well as the operations that created them. Every row corresponds to an intermediate variable and the elementary operation that caused it. These variables, called primals, are typically denoted $v_i$ for functions $f:R^n→R^m$ and follow these rules:\n",
        "\n",
        "- Input variables: $v_{i-n}=x_i,i=1,…,n$\n",
        "- Intermediate variables: $v_i,i=1,…,l$\n",
        "- Output variables: $y_{m-i}=v_{l-i}  ,i=m-1,…,0$\n",
        "\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "$$y=f(x_1,x_2 )=x_1 x_2+x_2-ln⁡(x_1 )\\text{ (Eq.5)}$$\n",
        "$$x_1=2,x_2=4$$\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Forward Primal Trace** | **Output** |\n",
        "| -------- | ------- |\n",
        "| $v_{-1}=x_1$ | 2 |\n",
        "| $v_{0}=x_2$ | 4 |\n",
        "| $v_1=v_{-1} \\times v_0$ | 2 x (4) = 8 |\n",
        "| $v_2=ln⁡(v_{-1})$ | ln(2) = 0.693 |\n",
        "| $v_3=v_1+v_0$ | 8 + 4 = 12 |\n",
        "| $v_4=v_2-v_3$ | 12 – 0.693 = 11.307 |\n",
        "| $y=v_4$ | 11.307 |\n",
        "<p style=\"text-align:center;\">Table 1: Forward Primal Trace</p>\n",
        "</div>\n",
        "\n",
        "![image](./image/compution_graph.png \"Computation Graph\")\n",
        "<p style=\"text-align:center;\">Figure 1: Computation Graph</p>\n",
        "\n",
        "Evaluation traces are fundamental to automatic differentiation. Unlike symbolic differentiation, which is restricted to closed form expressions, AD can also handle algorithms that include branching, loops, recursion, and function calls. This flexibility stems from the fact that any numerical program execution generates a trace of concrete input, intermediate, and output values. Those values alone suffice to calculate derivatives via repeated application of the chain rule, regardless of which control flow path the code followed. In other words, AD disregards any operations, such as conditionals or loop controls, that do not directly affect numerical values.\n",
        "\n",
        "### **Forward Mode**\n",
        "\n",
        "Forward mode AD enhances the evaluation trace by linking each primary value $v_i$ to a tangent $\\dot{v_i}$. These tangents record the partial derivative of each value relative to the selected input variable.\n",
        "\n",
        "Referencing back to eq. 5, we'd have the following definition of tangents if we were interested in finding $\\frac{∂y}{∂x_2 }$:\n",
        "\n",
        "$$ \\dot{v_i} = \\frac{∂v_i}{∂x_2} $$\n",
        "\n",
        "Continuing from this definition, we can build out the forward primal and forward tangent trace to compute $\\frac{∂y}{∂x_2}$ when $x_1=3,x_2=-4,\\dot{x_1}=\\frac{∂x_1}{∂x_2}=0$, and $\\dot{x_2}=\\frac{∂x_2}{∂x_2}=1$.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| Forward Primal Trace | Output | Forward Tangent Trace                       | Output           |\n",
        "|----------------------|--------|---------------------------------------------|------------------|\n",
        "| $v_{-1} = x_1$       | 3      | $\\dot v_{-1} = \\dot x_1$                    | 0                |\n",
        "| $v_{0} = x_2$        | –4     | $\\dot v_{0} = \\dot x_2$                     | 1                |\n",
        "| $v_{1} = v_{-1} \\times v_{0}$ | $3 \\times (-4) = -12$ | $\\dot v_{1} = \\dot v_{-1}\\,v_{0} + \\dot v_{0}\\,v_{-1}$ | $0\\times(-4) + 1\\times(3) = 3$ |\n",
        "| $v_{2} = \\ln(v_{-1})$| $\\ln(3)=1.10$ | $\\dot v_{2} = \\dot v_{-1}\\times\\frac{1}{v_{-1}}$ | $0\\times\\frac1{3}=0$ |\n",
        "| $v_{3} = v_{1} + v_{0}$ | $-12 + -4 = -16$ | $\\dot v_{3} = \\dot v_{1} + \\dot v_{0}$ | $3 + 1 = 4$ |\n",
        "| $v_{4} = v_{2} - v_{3}$ | $-16 - 1.10 = 17.10$ | $\\dot v_{4} = \\dot v_{2} - \\dot v_{3}$ | $0 - 4 = -4$ |\n",
        "| $y = v_{4}$ | $-17.10$ | $\\dot y = \\dot v_{4}$ | $-4$ |\n",
        "\n",
        "<p style=\"text-align:center;\">Table 2: Forward Mode Trace</p>\n",
        "</div>\n",
        "\n",
        "Forward mode AD works by, at each elementary operation, calculating both the usual intermediate values (primals) and their derivatives (tangents) simultaneously using standard calculus rules. This mechanism not only yields derivatives but can build entire Jacobian matrices. For a function $f:R^n→R^m$, we set the input **x=a** and initialize its tangent $\\dot{x}$ to the unit vector $e_i$ for $i=1,…,n$. Running the function in forward mode then produces the partial derivatives of every output $y_j$ with respect to that single input $x_i$. Each forward pass therefore generates one column of the Jacobian, giving all $\\frac{∂y_j}{∂x_i }$.\n",
        "\n",
        "Jacobian Matrix:\n",
        "\n",
        "$$\n",
        "J =\n",
        "\\begin{pmatrix}\n",
        "\\displaystyle \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\displaystyle \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\displaystyle \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\displaystyle \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Since $f∶R^n→R^m$ has $n$ inputs and each forward mode pass produces one column of the Jacobian, assembling the full $m×n$ Jacobian requires $O(n)$ evaluations. Remember that the Jacobian collects every partial derivative of each output with respect to every input—the very gradients used in optimization.\n",
        "\n",
        "Forward mode AD naturally extends to compute the Jacobian–vector product (JVP). Given J ϵ $R^{m×n}$ and a direction **r** ϵ $R^n$, the JVP **J∙r** is an $m$-dimensional vector describing how the outputs shift when the inputs are perturbed along **r**.\n",
        "\n",
        "The power of forward mode AD lies in the fact that you never have to form the entire Jacobian. By selecting an input point and a single perturbation vector **r** , one forward mode evaluation directly yields the JVP **J∙r** with no need to compute all Jacobian columns.\n",
        "\n",
        "\n",
        "Jacobian-vector Product:\n",
        "\n",
        "$$\n",
        "J\\,\\cdot\\mathbf{r}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\displaystyle \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\displaystyle \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\displaystyle \\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\displaystyle \\frac{\\partial y_m}{\\partial x_n}\n",
        "\\end{pmatrix}\n",
        "\\;\\cdot\\;\n",
        "\\begin{pmatrix}\n",
        "r_1\\\\\n",
        "\\vdots\\\\\n",
        "r_n\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "In summary, forward mode AD is especially efficient for functions $f∶R^n→R^m$ when the number of inputs $n$ is much smaller than the number of outputs $m$. In that scenario, one forward pass suffices to compute the entire Jacobian. Conversely, for a function from $R^n$ to $R$, you need $n$ forward passes to obtain its gradient—making forward mode less practical when $n$ is large.\n",
        "\n",
        "This distinction matters in neural networks, where the model’s parameters live in $R^n$ but the loss is a single scalar in $R$. Using forward mode AD for gradient based training would be inefficient because $n$ is typically enormous.\n",
        "\n",
        "Although forward mode avoids numerical pitfalls like instability and expression swell (unlike finite differences or symbolic differentiation), it does not scale well to high dimensional inputs. That limitation is exactly why we turn to AD’s other variant, reverse mode AD for deep learning.\n",
        "\n",
        "\n",
        "### **Reverse Mode**\n",
        "\n",
        "At this stage, we introduce reverse mode AD, which mirrors forward mode in spirit but differs in technique. We start by defining adjoints $\\bar{v_i}$, each of which represents the partial derivative of a specific output $y_j$, with respect to an intermediate variable $v_i$. Formally, for a function $f∶R^n→R^m$ and indices $i=1,…,n$ and $j=1,…,m$, we set up these adjoints as follows:\n",
        "\n",
        "$$ \\bar{v_i} =  \\frac{∂y_j}{∂v_j}$$\n",
        "\n",
        "In reverse mode AD, we first execute a forward pass exactly as in ordinary code, computing all intermediate values. Unlike forward mode, we don’t calculate derivatives (adjoints) on the fly—instead, we record every operation and its inputs in a computational graph. Once the forward pass is complete, we traverse that graph backward. Using cached intermediate values and the known derivatives of each elementary operation, we apply the chain rule in reverse order to compute each adjoint $\\bar{v_i}$. This backward sweep—from final outputs back to inputs—is what gives reverse mode its name.\n",
        "\n",
        "With intuition behind reverse mode AD, let's take a look at the reverse mode evaluation trace of eq.5 using the same values for the input variables from the Forward Mode Trace.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| Forward Primal Trace          | Output           | Reverse Adjoint Trace                                                     | Output                |\n",
        "|-------------------------------|------------------|---------------------------------------------------------------------------|-----------------------|\n",
        "| $v_{-1} = x_{1}$              | $3$              | $\\bar v_{-1} = \\bar x_{1} = \\bar v_{2}\\times\\frac{1}{v_{-1}} + \\bar v_{1}\\times v_{0}$ | $-1 \\times (1/3) + 1 \\times (-4) = -4.33$ |\n",
        "| $v_{0} = x_{2}$               | $-4$             | $\\bar v_{0} = \\bar x_{2} = \\bar v_{3}\\times 1 + \\bar v_{1}\\times v_{-1}$         | $1 \\times 1 + 1 \\times 3 = 4$            |\n",
        "| $v_{1} = v_{-1} \\times v_{0}$ | $3 \\times (-4) = -12$ | $\\bar v_{1} = \\bar v_{3}\\times 1$                                         | $1 \\times 1 = 1$                       |\n",
        "| $v_{2} = \\ln(v_{-1})$         | $\\ln(3) = 1.10$  | $\\bar v_{2} = \\bar v_{4}\\times -1$                                        | $1 \\times -1 = -1$                     |\n",
        "| $v_{3} = v_{1} + v_{0}$       | $-12 + -4 = -16$ | $\\bar v_{3} = \\bar v_{1} + \\bar v_{4}\\times 1$                              | $1 \\times 1 = 1$                       |\n",
        "| $v_{4} = v_{2} - v_{3}$       | $-16 - 1.10 = 17.10$ | $\\bar v_{4} = \\bar y$                                                     | $1$                                    |\n",
        "| $y = v_{4}$                   | $-17.10$         | $\\bar y = \\bar v_{4}$                                                       | $1$                                    |                                                                                          | 1                             |\n",
        "<p style=\"text-align:center;\">Table 3: Reverse Mode Trace</p>\n",
        "</div>\n",
        "\n",
        "We begin the backward pass in reverse mode AD by seeding the output’s adjoint $\\bar{y}=\\frac{∂y}{∂y}=1$. From there, we propagate derivatives back to every variable that influenced $y$, using each operation’s local derivative. Eventually, every input $x$ that contributed to $y$ acquires its adjoint $\\bar{x}$.\n",
        "\n",
        "You may wonder why intermediates like $\\bar{v}_{-1}$ and $\\bar{v}_{0}$ each receive two contributions when their primal values feed into multiple downstream operations (for example, both $v_2$ and $v_1$ depend on them). Rather than discarding one path, we sum both partial derivatives—so each variable’s adjoint accumulates its total effect on the final output.\n",
        "\n",
        "Just as forward mode can compute Jacobians by generating one column per pass, reverse mode can generate rows of the Jacobian. If $f∶R^n→R^m$, you set **x=a** and run one reverse mode pass with $\\bar{y}=e_j$ for each output index $j$($j=1,…,m$). That pass produces $\\frac{∂y_j}{∂x_i }$ for $i=1,…,n$, i.e. one complete row of **J**. Repeating this for all m outputs costs $O(m)$ evaluations.\n",
        "\n",
        "Moreover, reverse mode naturally yields the vector–Jacobian product (VJP). Given a cotangent (row) vector $r^T$  ϵ $R^{1×m}$, a single backward pass computes $r^T$∙**J**( **J** ϵ $R^{m×n}$), an $1×n$ vector of weighted partials—describing how a perturbation in the outputs propagates back to the inputs.\n",
        "\n",
        "Vector-Jacobian Product:\n",
        "\n",
        "$$\n",
        "\\mathbf{r}^T \\cdot J\n",
        "=\n",
        "\\begin{matrix}\n",
        "(r_1 & \\cdots & r_n)^T\n",
        "\\end{matrix}\n",
        "\\cdot\n",
        "\\begin{pmatrix}\n",
        "\\displaystyle \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\displaystyle \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\displaystyle \\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\displaystyle \\frac{\\partial y_m}{\\partial x_n}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Vector-Jacobian Product (Alt. Form):\n",
        "\n",
        "$$\n",
        "J^T \\,.\\mathbf{r}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\displaystyle \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\displaystyle \\frac{\\partial y_m}{\\partial x_1} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\displaystyle \\frac{\\partial y_1}{\\partial x_n} & \\cdots & \\displaystyle \\frac{\\partial y_m}{\\partial x_n}\n",
        "\\end{pmatrix}^T\n",
        "\\cdot\n",
        "\\begin{pmatrix}\n",
        "r_1 & \\cdots & r_n\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Vector–Jacobian products (VJPs) are central to neural network optimization. We view the Jacobian **J** as the matrix of partial derivatives of a model’s outputs with respect to its inputs, and we interpret $r^T$ as the gradient of a loss function with respect to those outputs. Computing $r^T$∙**J** in this setting directly yields the gradients of the loss with respect to every input parameter—exactly what we need for training—without ever forming the full Jacobian.\n",
        "\n",
        "In reverse mode AD, one backward pass produces the gradient of a single output with respect to all inputs. To obtain gradients for $m$ outputs, you would run $m$ such passes. Since neural networks typically have $n≫m$ parameters but only one scalar loss, reverse mode is ideal: a single reverse pass computes all $n$ gradients in one shot. Although it uses more memory to store intermediate values, that cost is a small price to pay for avoiding an $O(n)$ explosion in computation time. Ultimately, reverse mode AD is the most efficient choice for gradient based learning, delivering all required derivatives in just one backward sweep.\n",
        "\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g87wIJiyWcSe",
      "metadata": {
        "id": "g87wIJiyWcSe"
      },
      "source": [
        "## Backpropagation Algorithm\n",
        "## Gradient-Based Optimization\n",
        "## give an example and describe the autodiff step-by-step workflow\n",
        "## Summary. Compare betwwen each type advancement of each types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9o5Q0ZG0KVQ2",
      "metadata": {
        "id": "9o5Q0ZG0KVQ2"
      },
      "source": [
        "# Visualization\n",
        "\n",
        "Code, implement AutoDiff, explain with an example. show backward and forward\n",
        "Ex: f(x1, x2) = ln(x1) + x1x2 - sin(x2)\n",
        "\n",
        "https://d2l.ai/chapter_preliminaries/autograd.html\n",
        "\n",
        "https://homepages.inf.ed.ac.uk/htang2/mlg2022/tutorial-3.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hk4f988CNvh7",
      "metadata": {
        "id": "hk4f988CNvh7"
      },
      "source": [
        "# Exercises (1-4)\n",
        "\n",
        "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
        "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
        "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
        "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KievrNAtN8MC",
      "metadata": {
        "id": "KievrNAtN8MC"
      },
      "source": [
        "# Exercises (5-8)\n",
        "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
        "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
        "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
        "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4144c129",
      "metadata": {
        "id": "4144c129",
        "origin_pos": 1
      },
      "source": [
        "***Reference: origin notebook of d2l as below***\n",
        "\n",
        "# Automatic Differentiation\n",
        ":label:`sec_autograd`\n",
        "\n",
        "Recall from :numref:`sec_calculus`\n",
        "that calculating derivatives is the crucial step\n",
        "in all the optimization algorithms\n",
        "that we will use to train deep networks.\n",
        "While the calculations are straightforward,\n",
        "working them out by hand can be tedious and error-prone,\n",
        "and these issues only grow\n",
        "as our models become more complex.\n",
        "\n",
        "Fortunately all modern deep learning frameworks\n",
        "take this work off our plates\n",
        "by offering *automatic differentiation*\n",
        "(often shortened to *autograd*).\n",
        "As we pass data through each successive function,\n",
        "the framework builds a *computational graph*\n",
        "that tracks how each value depends on others.\n",
        "To calculate derivatives,\n",
        "automatic differentiation\n",
        "works backwards through this graph\n",
        "applying the chain rule.\n",
        "The computational algorithm for applying the chain rule\n",
        "in this fashion is called *backpropagation*.\n",
        "\n",
        "While autograd libraries have become\n",
        "a hot concern over the past decade,\n",
        "they have a long history.\n",
        "In fact the earliest references to autograd\n",
        "date back over half of a century :cite:`Wengert.1964`.\n",
        "The core ideas behind modern backpropagation\n",
        "date to a PhD thesis from 1980 :cite:`Speelpenning.1980`\n",
        "and were further developed in the late 1980s :cite:`Griewank.1989`.\n",
        "While backpropagation has become the default method\n",
        "for computing gradients, it is not the only option.\n",
        "For instance, the Julia programming language employs\n",
        "forward propagation :cite:`Revels.Lubin.Papamarkou.2016`.\n",
        "Before exploring methods,\n",
        "let's first master the autograd package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130439cd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:08.286501Z",
          "iopub.status.busy": "2023-08-18T19:26:08.285693Z",
          "iopub.status.idle": "2023-08-18T19:26:10.052257Z",
          "shell.execute_reply": "2023-08-18T19:26:10.050994Z"
        },
        "id": "130439cd",
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ab3850",
      "metadata": {
        "id": "e2ab3850",
        "origin_pos": 6
      },
      "source": [
        "## A Simple Function\n",
        "\n",
        "Let's assume that we are interested\n",
        "in (**differentiating the function\n",
        "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to the column vector $\\mathbf{x}$.**)\n",
        "To start, we assign `x` an initial value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4253cfab",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.056833Z",
          "iopub.status.busy": "2023-08-18T19:26:10.055871Z",
          "iopub.status.idle": "2023-08-18T19:26:10.084858Z",
          "shell.execute_reply": "2023-08-18T19:26:10.083727Z"
        },
        "id": "4253cfab",
        "origin_pos": 8,
        "outputId": "a4817393-fc1c-4795-875e-4d2861054425",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(4.0)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75614b0",
      "metadata": {
        "id": "e75614b0",
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "[**Before we calculate the gradient\n",
        "of $y$ with respect to $\\mathbf{x}$,\n",
        "we need a place to store it.**]\n",
        "In general, we avoid allocating new memory\n",
        "every time we take a derivative\n",
        "because deep learning requires\n",
        "successively computing derivatives\n",
        "with respect to the same parameters\n",
        "a great many times,\n",
        "and we might risk running out of memory.\n",
        "Note that the gradient of a scalar-valued function\n",
        "with respect to a vector $\\mathbf{x}$\n",
        "is vector-valued with\n",
        "the same shape as $\\mathbf{x}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a001d1e",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.088716Z",
          "iopub.status.busy": "2023-08-18T19:26:10.087816Z",
          "iopub.status.idle": "2023-08-18T19:26:10.092878Z",
          "shell.execute_reply": "2023-08-18T19:26:10.091740Z"
        },
        "id": "2a001d1e",
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
        "x.requires_grad_(True)\n",
        "x.grad  # The gradient is None by default"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e74bc02",
      "metadata": {
        "id": "2e74bc02",
        "origin_pos": 15
      },
      "source": [
        "(**We now calculate our function of `x` and assign the result to `y`.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3bd777",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.096336Z",
          "iopub.status.busy": "2023-08-18T19:26:10.095772Z",
          "iopub.status.idle": "2023-08-18T19:26:10.105236Z",
          "shell.execute_reply": "2023-08-18T19:26:10.104075Z"
        },
        "id": "6e3bd777",
        "origin_pos": 17,
        "outputId": "d6372de1-8216-4343-88fa-ca454f727629",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3067490",
      "metadata": {
        "id": "c3067490",
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "[**We can now take the gradient of `y`\n",
        "with respect to `x`**] by calling\n",
        "its `backward` method.\n",
        "Next, we can access the gradient\n",
        "via `x`'s `grad` attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b134ae",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.108600Z",
          "iopub.status.busy": "2023-08-18T19:26:10.108011Z",
          "iopub.status.idle": "2023-08-18T19:26:10.160854Z",
          "shell.execute_reply": "2023-08-18T19:26:10.159702Z"
        },
        "id": "21b134ae",
        "origin_pos": 25,
        "outputId": "3c3b96c9-3955-4c18-f6ed-fe92feaf5957",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d1390b",
      "metadata": {
        "id": "17d1390b",
        "origin_pos": 28
      },
      "source": [
        "(**We already know that the gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.**)\n",
        "We can now verify that the automatic gradient computation\n",
        "and the expected result are identical.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5030e37d",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.164665Z",
          "iopub.status.busy": "2023-08-18T19:26:10.163930Z",
          "iopub.status.idle": "2023-08-18T19:26:10.171033Z",
          "shell.execute_reply": "2023-08-18T19:26:10.169923Z"
        },
        "id": "5030e37d",
        "origin_pos": 30,
        "outputId": "da15a8ec-929d-48c0-d80f-b3a536de4f8c",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad == 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da440e48",
      "metadata": {
        "id": "da440e48",
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "[**Now let's calculate\n",
        "another function of `x`\n",
        "and take its gradient.**]\n",
        "Note that PyTorch does not automatically\n",
        "reset the gradient buffer\n",
        "when we record a new gradient.\n",
        "Instead, the new gradient\n",
        "is added to the already-stored gradient.\n",
        "This behavior comes in handy\n",
        "when we want to optimize the sum\n",
        "of multiple objective functions.\n",
        "To reset the gradient buffer,\n",
        "we can call `x.grad.zero_()` as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add5cf4b",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "20"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.174691Z",
          "iopub.status.busy": "2023-08-18T19:26:10.173957Z",
          "iopub.status.idle": "2023-08-18T19:26:10.181847Z",
          "shell.execute_reply": "2023-08-18T19:26:10.180759Z"
        },
        "id": "add5cf4b",
        "origin_pos": 37,
        "outputId": "c3ec729a-b1fb-481f-e7f5-655516e2b346",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()  # Reset the gradient\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdd4c0c",
      "metadata": {
        "id": "8bdd4c0c",
        "origin_pos": 40
      },
      "source": [
        "## Backward for Non-Scalar Variables\n",
        "\n",
        "When `y` is a vector,\n",
        "the most natural representation\n",
        "of the derivative of  `y`\n",
        "with respect to a vector `x`\n",
        "is a matrix called the *Jacobian*\n",
        "that contains the partial derivatives\n",
        "of each component of `y`\n",
        "with respect to each component of `x`.\n",
        "Likewise, for higher-order `y` and `x`,\n",
        "the result of differentiation could be an even higher-order tensor.\n",
        "\n",
        "While Jacobians do show up in some\n",
        "advanced machine learning techniques,\n",
        "more commonly we want to sum up\n",
        "the gradients of each component of `y`\n",
        "with respect to the full vector `x`,\n",
        "yielding a vector of the same shape as `x`.\n",
        "For example, we often have a vector\n",
        "representing the value of our loss function\n",
        "calculated separately for each example among\n",
        "a *batch* of training examples.\n",
        "Here, we just want to (**sum up the gradients\n",
        "computed individually for each example**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dda7124",
      "metadata": {
        "id": "9dda7124",
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "Because deep learning frameworks vary\n",
        "in how they interpret gradients of\n",
        "non-scalar tensors,\n",
        "PyTorch takes some steps to avoid confusion.\n",
        "Invoking `backward` on a non-scalar elicits an error\n",
        "unless we tell PyTorch how to reduce the object to a scalar.\n",
        "More formally, we need to provide some vector $\\mathbf{v}$\n",
        "such that `backward` will compute\n",
        "$\\mathbf{v}^\\top \\partial_{\\mathbf{x}} \\mathbf{y}$\n",
        "rather than $\\partial_{\\mathbf{x}} \\mathbf{y}$.\n",
        "This next part may be confusing,\n",
        "but for reasons that will become clear later,\n",
        "this argument (representing $\\mathbf{v}$) is named `gradient`.\n",
        "For a more detailed description, see Yang Zhang's\n",
        "[Medium post](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1baa40bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.185096Z",
          "iopub.status.busy": "2023-08-18T19:26:10.184685Z",
          "iopub.status.idle": "2023-08-18T19:26:10.192537Z",
          "shell.execute_reply": "2023-08-18T19:26:10.191435Z"
        },
        "id": "1baa40bd",
        "origin_pos": 45,
        "outputId": "dc4b39c4-6948-4526-f167-7118248b07d2",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffbd2c9d",
      "metadata": {
        "id": "ffbd2c9d",
        "origin_pos": 48
      },
      "source": [
        "## Detaching Computation\n",
        "\n",
        "Sometimes, we wish to [**move some calculations\n",
        "outside of the recorded computational graph.**]\n",
        "For example, say that we use the input\n",
        "to create some auxiliary intermediate terms\n",
        "for which we do not want to compute a gradient.\n",
        "In this case, we need to *detach*\n",
        "the respective computational graph\n",
        "from the final result.\n",
        "The following toy example makes this clearer:\n",
        "suppose we have `z = x * y` and `y = x * x`\n",
        "but we want to focus on the *direct* influence of `x` on `z`\n",
        "rather than the influence conveyed via `y`.\n",
        "In this case, we can create a new variable `u`\n",
        "that takes the same value as `y`\n",
        "but whose *provenance* (how it was created)\n",
        "has been wiped out.\n",
        "Thus `u` has no ancestors in the graph\n",
        "and gradients do not flow through `u` to `x`.\n",
        "For example, taking the gradient of `z = x * u`\n",
        "will yield the result `u`,\n",
        "(not `3 * x * x` as you might have\n",
        "expected since `z = x * x * x`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107ac041",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "21"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.196001Z",
          "iopub.status.busy": "2023-08-18T19:26:10.195456Z",
          "iopub.status.idle": "2023-08-18T19:26:10.203246Z",
          "shell.execute_reply": "2023-08-18T19:26:10.202155Z"
        },
        "id": "107ac041",
        "origin_pos": 50,
        "outputId": "e07fe5dc-f226-4cf2-8042-4c499f7ed519",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0378e1f",
      "metadata": {
        "id": "e0378e1f",
        "origin_pos": 53
      },
      "source": [
        "Note that while this procedure\n",
        "detaches `y`'s ancestors\n",
        "from the graph leading to `z`,\n",
        "the computational graph leading to `y`\n",
        "persists and thus we can calculate\n",
        "the gradient of `y` with respect to `x`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8c674b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.206880Z",
          "iopub.status.busy": "2023-08-18T19:26:10.206001Z",
          "iopub.status.idle": "2023-08-18T19:26:10.213592Z",
          "shell.execute_reply": "2023-08-18T19:26:10.212476Z"
        },
        "id": "cb8c674b",
        "origin_pos": 55,
        "outputId": "772ee87b-f492-48b8-e329-e0ef9f98ca18",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f056ce",
      "metadata": {
        "id": "76f056ce",
        "origin_pos": 58
      },
      "source": [
        "## Gradients and Python Control Flow\n",
        "\n",
        "So far we reviewed cases where the path from input to output\n",
        "was well defined via a function such as `z = x * x * x`.\n",
        "Programming offers us a lot more freedom in how we compute results.\n",
        "For instance, we can make them depend on auxiliary variables\n",
        "or condition choices on intermediate results.\n",
        "One benefit of using automatic differentiation\n",
        "is that [**even if**] building the computational graph of\n",
        "(**a function required passing through a maze of Python control flow**)\n",
        "(e.g., conditionals, loops, and arbitrary function calls),\n",
        "(**we can still calculate the gradient of the resulting variable.**)\n",
        "To illustrate this, consider the following code snippet where\n",
        "the number of iterations of the `while` loop\n",
        "and the evaluation of the `if` statement\n",
        "both depend on the value of the input `a`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83327c2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.218214Z",
          "iopub.status.busy": "2023-08-18T19:26:10.217554Z",
          "iopub.status.idle": "2023-08-18T19:26:10.222956Z",
          "shell.execute_reply": "2023-08-18T19:26:10.221858Z"
        },
        "id": "a83327c2",
        "origin_pos": 60,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189f6785",
      "metadata": {
        "id": "189f6785",
        "origin_pos": 63
      },
      "source": [
        "Below, we call this function, passing in a random value, as input.\n",
        "Since the input is a random variable,\n",
        "we do not know what form\n",
        "the computational graph will take.\n",
        "However, whenever we execute `f(a)`\n",
        "on a specific input, we realize\n",
        "a specific computational graph\n",
        "and can subsequently run `backward`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ef0264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.227364Z",
          "iopub.status.busy": "2023-08-18T19:26:10.226919Z",
          "iopub.status.idle": "2023-08-18T19:26:10.232880Z",
          "shell.execute_reply": "2023-08-18T19:26:10.231773Z"
        },
        "id": "c5ef0264",
        "origin_pos": 65,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51065133",
      "metadata": {
        "id": "51065133",
        "origin_pos": 68
      },
      "source": [
        "Even though our function `f` is, for demonstration purposes, a bit contrived,\n",
        "its dependence on the input is quite simple:\n",
        "it is a *linear* function of `a`\n",
        "with piecewise defined scale.\n",
        "As such, `f(a) / a` is a vector of constant entries\n",
        "and, moreover, `f(a) / a` needs to match\n",
        "the gradient of `f(a)` with respect to `a`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab14ef91",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.237298Z",
          "iopub.status.busy": "2023-08-18T19:26:10.236886Z",
          "iopub.status.idle": "2023-08-18T19:26:10.243577Z",
          "shell.execute_reply": "2023-08-18T19:26:10.242480Z"
        },
        "id": "ab14ef91",
        "origin_pos": 70,
        "outputId": "4d85cfd2-c7b3-460e-bf7e-76fdf88e0f50",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.grad == d / a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a992f28c",
      "metadata": {
        "id": "a992f28c",
        "origin_pos": 73
      },
      "source": [
        "Dynamic control flow is very common in deep learning.\n",
        "For instance, when processing text, the computational graph\n",
        "depends on the length of the input.\n",
        "In these cases, automatic differentiation\n",
        "becomes vital for statistical modeling\n",
        "since it is impossible to compute the gradient *a priori*.\n",
        "\n",
        "## Discussion\n",
        "\n",
        "You have now gotten a taste of the power of automatic differentiation.\n",
        "The development of libraries for calculating derivatives\n",
        "both automatically and efficiently\n",
        "has been a massive productivity booster\n",
        "for deep learning practitioners,\n",
        "liberating them so they can focus on less menial.\n",
        "Moreover, autograd lets us design massive models\n",
        "for which pen and paper gradient computations\n",
        "would be prohibitively time consuming.\n",
        "Interestingly, while we use autograd to *optimize* models\n",
        "(in a statistical sense)\n",
        "the *optimization* of autograd libraries themselves\n",
        "(in a computational sense)\n",
        "is a rich subject\n",
        "of vital interest to framework designers.\n",
        "Here, tools from compilers and graph manipulation\n",
        "are leveraged to compute results\n",
        "in the most expedient and memory-efficient manner.\n",
        "\n",
        "For now, try to remember these basics: (i) attach gradients to those variables with respect to which we desire derivatives; (ii) record the computation of the target value; (iii) execute the backpropagation function; and  (iv) access the resulting gradient.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
        "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
        "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
        "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
        "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
        "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
        "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
        "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0ab97d",
      "metadata": {
        "id": "4c0ab97d",
        "origin_pos": 75,
        "tab": [
          "pytorch"
        ]
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/35)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:15:57) [MSC v.1916 64 bit (AMD64)]"
    },
    "required_libs": [],
    "vscode": {
      "interpreter": {
        "hash": "faff1432c64dc3cf8e578be2847b833a03fac963b7493e93320a7ab2da274b14"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
