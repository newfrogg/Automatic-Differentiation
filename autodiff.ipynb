{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newfrogg/Automatic-Differentiation/blob/main/autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Mục lớn                             | Mục nhỏ                                                         |           |\n",
        "|-------------------------------------|-----------------------------------------------------------------|-----------|\n",
        "| Introduction                        |                                                                 |  1 người  |\n",
        "|                                     | Differential Calculus                                           |  Tử Quân  |\n",
        "|                                     | Rules of Calculus                                               |  Tử Quân  |\n",
        "|                                     | Multivariate Chain Rule                                         |  Tử Quân  |\n",
        "|                                     | Geometry of Gradients and Gradient Descent                      |  Tử Quân  |\n",
        "| What Autodiff Isn’t                 |                                                                 |  3 người  |\n",
        "|                                     | Autodiff is not finite differences (numerical differentiation)  |  An Đông  |\n",
        "|                                     | Autodiff is not symbolic differentiation                        |  An Đông  |\n",
        "|                                     | What autodiff is ?                                              |  An Đông  |\n",
        "|                                     | Types of Autodiff (explain forward, backward)                   |  An Đông  |\n",
        "|                                     | Backpropagation Algorithm                                       |  Gia Hinh |\n",
        "|                                     | Gradient-Based Optimization                                     |  Gia Hinh |\n",
        "|                                     | give an example and describe the autodiff step-by-step workflow |  Gia Hinh |\n",
        "|                                     | Summary. Compare betwwen each type advancement of each types    |  Gia Hinh |\n",
        "| Visualization. Code, Implementation |                                                                 |  1 người  |\n",
        "|                                     | Code                                                            | Bảo Lương |\n",
        "| Exercise                            |                                                                 |  2 người  |\n",
        "|                                     | Q1-4                                                            |   Triết   |\n",
        "|                                     | Q5-8                                                            | Minh Quân |"
      ],
      "metadata": {
        "id": "6-FUvYwvWEJF"
      },
      "id": "6-FUvYwvWEJF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Differential Calculus\n",
        "Differential calculus is fundamentally the study of how functions behave under small changes. The key of single variable calculus is the behavior of familiar functions can be modeled by a line in a small enough range. This means that for most functions, it is reasonable to expect that as we shift the $x$ value of the function by a small amount $x + \\epsilon$, the output $f(x)$ will also be shifted by a little bit.\n",
        "\n",
        "We can consider the ratio of the change in the output of a function for a small change in the input of the function. We can write this formally as:\n",
        "\\\\[\\frac{L(x+\\epsilon) - L(x)}{(x+\\epsilon) - x} = \\frac{L(x+\\epsilon) - L(x)}{\\epsilon}.\\\\]\n",
        "\n",
        "The derivative of a function quantifies how the function's output changes in response to changes in its input and is defined by the limit:\n",
        "\\\\[\\frac{df}{dx}(x) = \\lim_{\\epsilon \\rightarrow 0}\\frac{f(x+\\epsilon) - f(x)}{\\epsilon}.\\\\]\n",
        "\n",
        "## Rules of Calculus\n",
        "### Common Derivatives\n",
        "When computing derivatives one can oftentimes use a series of rules to reduce the computation to a few core functions.\n",
        "\n",
        "*   Derivative of constants. $\\frac{d}{dx}c = 0$\n",
        "*   Derivative of linear functions. $\\frac{d}{dx}(ax) = a$\n",
        "*   Power rule. $\\frac{d}{dx}x^n = nx^{n-1}$\n",
        "*   Derivative of exponentials. $\\frac{d}{dx}e^x = e^x$\n",
        "*   Derivative of the logarithm. $\\frac{d}{dx}\\log(x) = \\frac{1}{x}$\n",
        "\n",
        "### Derivative Rules\n",
        "We can generalize the above derivatives and compute more complex derivatives by codify what happens when we take functions and combine them in various ways, most importantly: sums, products, and compositions.\n",
        "*   Sum rule $\\frac{d}{dx}\\left(g(x) + h(x)\\right) = \\frac{dg}{dx}(x) + \\frac{dh}{dx}(x)$\n",
        "*   Product rule $\\frac{d}{dx}\\left(g(x)\\cdot h(x)\\right) = g(x)\\frac{dh}{dx}(x) + \\frac{dg}{dx}(x)h(x)$\n",
        "*   Chain rule $\\frac{d}{dx}g(h(x)) = \\frac{dg}{dh}(h(x))\\cdot \\frac{dh}{dx}(x)$\n",
        "\n",
        "These rule provide us with a flexible set of tools to compute essentially any expression desired. As an example, let us find the derivative of the function $f(x) = \\log\\left(1+(x-1)^{10}\\right)$:\n",
        "\n",
        "\n",
        "\\begin{split}\\begin{aligned}\n",
        "\\frac{d}{dx}\\left[\\log\\left(1+(x-1)^{10}\\right)\\right] & = \\left(1+(x-1)^{10}\\right)^{-1}\\frac{d}{dx}\\left[1+(x-1)^{10}\\right]\\\\\n",
        "& = \\left(1+(x-1)^{10}\\right)^{-1}\\left(\\frac{d}{dx}[1] + \\frac{d}{dx}[(x-1)^{10}]\\right) \\\\\n",
        "& = \\left(1+(x-1)^{10}\\right)^{-1}\\left(0 + 10(x-1)^9\\frac{d}{dx}[x-1]\\right) \\\\\n",
        "& = 10\\left(1+(x-1)^{10}\\right)^{-1}(x-1)^9 \\\\\n",
        "& = \\frac{10(x-1)^9}{1+(x-1)^{10}}.\n",
        "\\end{aligned}\\end{split}\n",
        "\n",
        "Where each line has used the following rules:\n",
        "\n",
        "1.   The chain rule and derivative of logarithm.\n",
        "2.   The sum rule.\n",
        "3.   The derivative of constants, chain rule, and power rule.\n",
        "4.   The sum rule, derivative of linear functions, derivative of constants.\n",
        "\n",
        "### Linear Approximation\n",
        "When working with derivatives, it is often useful to geometrically interpret the approximation used above. In particular, note that the equation\n",
        " \\\\[f(x+\\epsilon) \\approx f(x) + \\epsilon \\frac{df}{dx}(x),\\\\]\n",
        "\n",
        "approximates the value of $f$ by a line which passes through the point $(x, f(x))$ and has slope $\\frac{df}{dx}(x)$. In this way we say that the derivative gives a linear approximation to the function $f$.\n",
        "\n",
        "### Higher Order Derivatives\n",
        "The derivative, $\\frac{df}{dx}$, can be viewed as a function itself, so nothing stops us from computing the derivative of $\\frac{df}{dx}$ to get $\\frac{d^2f}{dx^2} = \\frac{df}{dx}\\left(\\frac{df}{dx}\\right)$. We will call this the second derivative of $f$. This function is the rate of change of the rate of change of $f$, or in other words, how the rate of change is changing. We may apply the derivative any number of times to obtain what is called the $n$-th derivative. To keep the notation clean, we will denote the $n$-th derivative as\n",
        "\n",
        "\\\\[f^{(n)}(x) = \\frac{d^{n}f}{dx^{n}} = \\left(\\frac{d}{dx}\\right)^{n} f.\\\\]\n",
        "\n",
        "The second derivative can be interpreted as describing the way that the function $f$ curves. A positive second derivative leads to a upwards curve, while a negative second derivative means that $f$ curves downwards, and a zero second derivative means that $f$ does not curve at all.\n",
        "### Taylor Series\n",
        "The Taylor series provides a method to approximate the function $f(x)$ if we are given values for the first $n$ derivatives at a point $x_0$, i.e., $\\left\\{ f(x_0), f^{(1)}(x_0), f^{(2)}(x_0), \\ldots, f^{(n)}(x_0) \\right\\}$. The idea will be to find a degree $n$ polynomial that matches all the given derivatives at $x_0$.\n",
        "\n",
        "We can get a degree $n$ polynomial by\n",
        "\\\\[P_n(x) = \\sum_{i = 0}^{n} \\frac{f^{(i)}(x_0)}{i!}(x-x_0)^{i}.\\\\]\n",
        "where the notation\n",
        "\\\\[f^{(n)}(x) = \\frac{d^{n}f}{dx^{n}} = \\left(\\frac{d}{dx}\\right)^{n} f.\\\\]\n",
        "\n",
        "Taylor series have two primary applications:\n",
        "\n",
        "1. Theoretical applications: Often when we try to understand a too complex function, using Taylor series enables us to turn it into a polynomial that we can work with directly.\n",
        "\n",
        "2. Numerical applications: Some functions like $e^{x}$ or $\\cos(x)$ are difficult for machines to compute. They can store tables of values at a fixed precision (and this is often done), but it still leaves open questions like “What is the 1000-th digit of $\\cos(1)$?” Taylor series are often helpful to answer such questions.\n",
        "\n",
        "## Geometry of Gradients and Gradient Descent\n",
        "\n",
        "\n",
        "In multivariable calculus, the gradient of a scalar-valued function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R} $ is a vector that points in the direction of the greatest rate of increase of the function. The gradient is denoted as $\\nabla f $ and is defined as:\n",
        "\n",
        "\\\\[\\nabla f(\\mathbf{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)\\\\]\n",
        "\n",
        "Here, each component $\\frac{\\partial f}{\\partial x_i}$ represents the partial derivative of $f$ with respect to the variable $x_i$.\n",
        "\n",
        "The geometric interpretation of gradients can be explained as follows:\n",
        "\n",
        "* Direction: The gradient vector at a point $\\mathbf{x}$ points in the direction of the steepest ascent of the function $f$ at that point.\n",
        "\n",
        "* Magnitude: The length (or norm) of the gradient vector indicates the rate of increase of the function in that direction.\n",
        "\n",
        "* Orthogonality to Level Sets: The gradient at a point is perpendicular (orthogonal) to the level set (or contour line) passing through that point. A level set is the set of points where the function $f$ has a constant value.\n",
        "\n",
        "Gradient descent is an iterative optimization algorithm used to find local minima of differentiable functions. Starting from an initial point $\\mathbf{x}_0$, the algorithm updates the point iteratively using the rule:\n",
        "\n",
        "\\\\[\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\eta \\nabla f(\\mathbf{x}_k)\\\\]\n",
        "\n",
        "Here:\n",
        "\n",
        "* $\\mathbf{x}_k $: Current point in $\\mathbb{R}^n$.\n",
        "\n",
        "* $\\eta$: Learning rate or step size, a positive scalar determining the size of the step taken in the direction opposite to the gradient.\n",
        "\n",
        "* $\\nabla f(\\mathbf{x}_k)$: Gradient of the function $ f$ at point $\\mathbf{x}_k$.\n",
        "\n",
        "At each iteration, the algorithm moves the current point $\\mathbf{x}_k$ in the direction opposite to the gradient $\\nabla f(\\mathbf{x}_k)$, aiming to decrease the function's value.\n",
        "\n",
        "Imagine a three-dimensional surface representing the function $f(\\mathbf{x}) $, where the height corresponds to the function's value. The gradient at any point on this surface points in the direction of the steepest ascent. Therefore, moving in the opposite direction of the gradient leads to the steepest descent.\n",
        "\n",
        "In two dimensions, the level sets of $ f $ are contour lines. The gradient vector at any point is perpendicular to the contour line at that point, pointing towards higher values of $f$. Gradient descent moves the point perpendicularly across contour lines towards regions of lower function values.\n",
        "\n",
        "## Multivariate Chain Rule\n",
        "\n",
        "The Multivariate Chain Rule is a fundamental concept in calculus, where functions often depend on multiple variables through complex compositions. When a function depends on multiple variables, and each of those variables is itself a function of other variables, the multivariate chain rule provides a systematic way to compute derivatives. This is essential for understanding how changes in input variables affect the output.\n",
        "\n",
        "If a function $f$ depends on intermediate variables $u$ and $v$, which in turn depend on variables $a$ and $b$, the derivative of $f$ with respect to $a$ is given by:\n",
        "\n",
        "\\\\[\\frac{\\partial f}{\\partial a} = \\frac{\\partial f}{\\partial u} \\frac{\\partial u}{\\partial a} + \\frac{\\partial f}{\\partial v} \\frac{\\partial v}{\\partial a}\\\\]\n",
        "\n",
        "This formula accounts for all the paths through which $a$ influences $f$.\n",
        "\n",
        "For example, let us imagine network of functions where the functions on the right depend on those that are connected to on the left:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "To compute something like $\\frac{\\partial f}{\\partial y}$, we need to sum over all (in this case $3$) paths from $y$ to $f$ giving\n",
        "\n",
        "\\\\[\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial a} \\frac{\\partial a}{\\partial u} \\frac{\\partial u}{\\partial y} + \\frac{\\partial f}{\\partial u} \\frac{\\partial u}{\\partial y} + \\frac{\\partial f}{\\partial b} \\frac{\\partial b}{\\partial v} \\frac{\\partial v}{\\partial y}.\\\\]\n",
        "\n",
        "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html\n",
        "https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html"
      ],
      "metadata": {
        "id": "LRVgmL2uBTRS"
      },
      "id": "LRVgmL2uBTRS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What Autodiff Isn’t\n",
        "\n",
        "## Autodiff is not finite differences (numerical differentiation)\n",
        "\n",
        "## Autodiff is not symbolic differentiation\n",
        "\n",
        "## What autodiff is ? Types of Autodiff (explain forward, backward)\n",
        "f(x1,x2) = [sin(x1/x2) + x1/x2 - e^x2] * [x1/x2 + e^x2]\n",
        "\n",
        "## Types of Autodiff (explain forward, backward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aQgVWYQ1I9AG"
      },
      "id": "aQgVWYQ1I9AG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation Algorithm\n",
        "## Gradient-Based Optimization\n",
        "## give an example and describe the autodiff step-by-step workflow\n",
        "## Summary. Compare betwwen each type advancement of each types"
      ],
      "metadata": {
        "id": "g87wIJiyWcSe"
      },
      "id": "g87wIJiyWcSe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n",
        "\n",
        "Code, implement AutoDiff, explain with an example. show backward and forward\n",
        "Ex: f(x1, x2) = ln(x1) + x1x2 - sin(x2)\n",
        "\n",
        "https://d2l.ai/chapter_preliminaries/autograd.html\n",
        "\n",
        "https://homepages.inf.ed.ac.uk/htang2/mlg2022/tutorial-3.pdf\n"
      ],
      "metadata": {
        "id": "9o5Q0ZG0KVQ2"
      },
      "id": "9o5Q0ZG0KVQ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises (1-4)\n",
        "\n",
        "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
        "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
        "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
        "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
        "\n"
      ],
      "metadata": {
        "id": "hk4f988CNvh7"
      },
      "id": "hk4f988CNvh7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises (5-8)\n",
        "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
        "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
        "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
        "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved."
      ],
      "metadata": {
        "id": "KievrNAtN8MC"
      },
      "id": "KievrNAtN8MC"
    },
    {
      "cell_type": "markdown",
      "id": "4144c129",
      "metadata": {
        "origin_pos": 1,
        "id": "4144c129"
      },
      "source": [
        "***Reference: origin notebook of d2l as below***\n",
        "\n",
        "# Automatic Differentiation\n",
        ":label:`sec_autograd`\n",
        "\n",
        "Recall from :numref:`sec_calculus`\n",
        "that calculating derivatives is the crucial step\n",
        "in all the optimization algorithms\n",
        "that we will use to train deep networks.\n",
        "While the calculations are straightforward,\n",
        "working them out by hand can be tedious and error-prone,\n",
        "and these issues only grow\n",
        "as our models become more complex.\n",
        "\n",
        "Fortunately all modern deep learning frameworks\n",
        "take this work off our plates\n",
        "by offering *automatic differentiation*\n",
        "(often shortened to *autograd*).\n",
        "As we pass data through each successive function,\n",
        "the framework builds a *computational graph*\n",
        "that tracks how each value depends on others.\n",
        "To calculate derivatives,\n",
        "automatic differentiation\n",
        "works backwards through this graph\n",
        "applying the chain rule.\n",
        "The computational algorithm for applying the chain rule\n",
        "in this fashion is called *backpropagation*.\n",
        "\n",
        "While autograd libraries have become\n",
        "a hot concern over the past decade,\n",
        "they have a long history.\n",
        "In fact the earliest references to autograd\n",
        "date back over half of a century :cite:`Wengert.1964`.\n",
        "The core ideas behind modern backpropagation\n",
        "date to a PhD thesis from 1980 :cite:`Speelpenning.1980`\n",
        "and were further developed in the late 1980s :cite:`Griewank.1989`.\n",
        "While backpropagation has become the default method\n",
        "for computing gradients, it is not the only option.\n",
        "For instance, the Julia programming language employs\n",
        "forward propagation :cite:`Revels.Lubin.Papamarkou.2016`.\n",
        "Before exploring methods,\n",
        "let's first master the autograd package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130439cd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:08.286501Z",
          "iopub.status.busy": "2023-08-18T19:26:08.285693Z",
          "iopub.status.idle": "2023-08-18T19:26:10.052257Z",
          "shell.execute_reply": "2023-08-18T19:26:10.050994Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "130439cd"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2ab3850",
      "metadata": {
        "origin_pos": 6,
        "id": "e2ab3850"
      },
      "source": [
        "## A Simple Function\n",
        "\n",
        "Let's assume that we are interested\n",
        "in (**differentiating the function\n",
        "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to the column vector $\\mathbf{x}$.**)\n",
        "To start, we assign `x` an initial value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4253cfab",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.056833Z",
          "iopub.status.busy": "2023-08-18T19:26:10.055871Z",
          "iopub.status.idle": "2023-08-18T19:26:10.084858Z",
          "shell.execute_reply": "2023-08-18T19:26:10.083727Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "4253cfab",
        "outputId": "a4817393-fc1c-4795-875e-4d2861054425"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.arange(4.0)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75614b0",
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "e75614b0"
      },
      "source": [
        "[**Before we calculate the gradient\n",
        "of $y$ with respect to $\\mathbf{x}$,\n",
        "we need a place to store it.**]\n",
        "In general, we avoid allocating new memory\n",
        "every time we take a derivative\n",
        "because deep learning requires\n",
        "successively computing derivatives\n",
        "with respect to the same parameters\n",
        "a great many times,\n",
        "and we might risk running out of memory.\n",
        "Note that the gradient of a scalar-valued function\n",
        "with respect to a vector $\\mathbf{x}$\n",
        "is vector-valued with\n",
        "the same shape as $\\mathbf{x}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a001d1e",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.088716Z",
          "iopub.status.busy": "2023-08-18T19:26:10.087816Z",
          "iopub.status.idle": "2023-08-18T19:26:10.092878Z",
          "shell.execute_reply": "2023-08-18T19:26:10.091740Z"
        },
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "id": "2a001d1e"
      },
      "outputs": [],
      "source": [
        "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
        "x.requires_grad_(True)\n",
        "x.grad  # The gradient is None by default"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e74bc02",
      "metadata": {
        "origin_pos": 15,
        "id": "2e74bc02"
      },
      "source": [
        "(**We now calculate our function of `x` and assign the result to `y`.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3bd777",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.096336Z",
          "iopub.status.busy": "2023-08-18T19:26:10.095772Z",
          "iopub.status.idle": "2023-08-18T19:26:10.105236Z",
          "shell.execute_reply": "2023-08-18T19:26:10.104075Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "6e3bd777",
        "outputId": "d6372de1-8216-4343-88fa-ca454f727629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3067490",
      "metadata": {
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "c3067490"
      },
      "source": [
        "[**We can now take the gradient of `y`\n",
        "with respect to `x`**] by calling\n",
        "its `backward` method.\n",
        "Next, we can access the gradient\n",
        "via `x`'s `grad` attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b134ae",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.108600Z",
          "iopub.status.busy": "2023-08-18T19:26:10.108011Z",
          "iopub.status.idle": "2023-08-18T19:26:10.160854Z",
          "shell.execute_reply": "2023-08-18T19:26:10.159702Z"
        },
        "origin_pos": 25,
        "tab": [
          "pytorch"
        ],
        "id": "21b134ae",
        "outputId": "3c3b96c9-3955-4c18-f6ed-fe92feaf5957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d1390b",
      "metadata": {
        "origin_pos": 28,
        "id": "17d1390b"
      },
      "source": [
        "(**We already know that the gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.**)\n",
        "We can now verify that the automatic gradient computation\n",
        "and the expected result are identical.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5030e37d",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.164665Z",
          "iopub.status.busy": "2023-08-18T19:26:10.163930Z",
          "iopub.status.idle": "2023-08-18T19:26:10.171033Z",
          "shell.execute_reply": "2023-08-18T19:26:10.169923Z"
        },
        "origin_pos": 30,
        "tab": [
          "pytorch"
        ],
        "id": "5030e37d",
        "outputId": "da15a8ec-929d-48c0-d80f-b3a536de4f8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad == 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da440e48",
      "metadata": {
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ],
        "id": "da440e48"
      },
      "source": [
        "[**Now let's calculate\n",
        "another function of `x`\n",
        "and take its gradient.**]\n",
        "Note that PyTorch does not automatically\n",
        "reset the gradient buffer\n",
        "when we record a new gradient.\n",
        "Instead, the new gradient\n",
        "is added to the already-stored gradient.\n",
        "This behavior comes in handy\n",
        "when we want to optimize the sum\n",
        "of multiple objective functions.\n",
        "To reset the gradient buffer,\n",
        "we can call `x.grad.zero_()` as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add5cf4b",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "20"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.174691Z",
          "iopub.status.busy": "2023-08-18T19:26:10.173957Z",
          "iopub.status.idle": "2023-08-18T19:26:10.181847Z",
          "shell.execute_reply": "2023-08-18T19:26:10.180759Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "add5cf4b",
        "outputId": "c3ec729a-b1fb-481f-e7f5-655516e2b346"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()  # Reset the gradient\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdd4c0c",
      "metadata": {
        "origin_pos": 40,
        "id": "8bdd4c0c"
      },
      "source": [
        "## Backward for Non-Scalar Variables\n",
        "\n",
        "When `y` is a vector,\n",
        "the most natural representation\n",
        "of the derivative of  `y`\n",
        "with respect to a vector `x`\n",
        "is a matrix called the *Jacobian*\n",
        "that contains the partial derivatives\n",
        "of each component of `y`\n",
        "with respect to each component of `x`.\n",
        "Likewise, for higher-order `y` and `x`,\n",
        "the result of differentiation could be an even higher-order tensor.\n",
        "\n",
        "While Jacobians do show up in some\n",
        "advanced machine learning techniques,\n",
        "more commonly we want to sum up\n",
        "the gradients of each component of `y`\n",
        "with respect to the full vector `x`,\n",
        "yielding a vector of the same shape as `x`.\n",
        "For example, we often have a vector\n",
        "representing the value of our loss function\n",
        "calculated separately for each example among\n",
        "a *batch* of training examples.\n",
        "Here, we just want to (**sum up the gradients\n",
        "computed individually for each example**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dda7124",
      "metadata": {
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "9dda7124"
      },
      "source": [
        "Because deep learning frameworks vary\n",
        "in how they interpret gradients of\n",
        "non-scalar tensors,\n",
        "PyTorch takes some steps to avoid confusion.\n",
        "Invoking `backward` on a non-scalar elicits an error\n",
        "unless we tell PyTorch how to reduce the object to a scalar.\n",
        "More formally, we need to provide some vector $\\mathbf{v}$\n",
        "such that `backward` will compute\n",
        "$\\mathbf{v}^\\top \\partial_{\\mathbf{x}} \\mathbf{y}$\n",
        "rather than $\\partial_{\\mathbf{x}} \\mathbf{y}$.\n",
        "This next part may be confusing,\n",
        "but for reasons that will become clear later,\n",
        "this argument (representing $\\mathbf{v}$) is named `gradient`.\n",
        "For a more detailed description, see Yang Zhang's\n",
        "[Medium post](https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1baa40bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.185096Z",
          "iopub.status.busy": "2023-08-18T19:26:10.184685Z",
          "iopub.status.idle": "2023-08-18T19:26:10.192537Z",
          "shell.execute_reply": "2023-08-18T19:26:10.191435Z"
        },
        "origin_pos": 45,
        "tab": [
          "pytorch"
        ],
        "id": "1baa40bd",
        "outputId": "dc4b39c4-6948-4526-f167-7118248b07d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffbd2c9d",
      "metadata": {
        "origin_pos": 48,
        "id": "ffbd2c9d"
      },
      "source": [
        "## Detaching Computation\n",
        "\n",
        "Sometimes, we wish to [**move some calculations\n",
        "outside of the recorded computational graph.**]\n",
        "For example, say that we use the input\n",
        "to create some auxiliary intermediate terms\n",
        "for which we do not want to compute a gradient.\n",
        "In this case, we need to *detach*\n",
        "the respective computational graph\n",
        "from the final result.\n",
        "The following toy example makes this clearer:\n",
        "suppose we have `z = x * y` and `y = x * x`\n",
        "but we want to focus on the *direct* influence of `x` on `z`\n",
        "rather than the influence conveyed via `y`.\n",
        "In this case, we can create a new variable `u`\n",
        "that takes the same value as `y`\n",
        "but whose *provenance* (how it was created)\n",
        "has been wiped out.\n",
        "Thus `u` has no ancestors in the graph\n",
        "and gradients do not flow through `u` to `x`.\n",
        "For example, taking the gradient of `z = x * u`\n",
        "will yield the result `u`,\n",
        "(not `3 * x * x` as you might have\n",
        "expected since `z = x * x * x`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107ac041",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "21"
        },
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.196001Z",
          "iopub.status.busy": "2023-08-18T19:26:10.195456Z",
          "iopub.status.idle": "2023-08-18T19:26:10.203246Z",
          "shell.execute_reply": "2023-08-18T19:26:10.202155Z"
        },
        "origin_pos": 50,
        "tab": [
          "pytorch"
        ],
        "id": "107ac041",
        "outputId": "e07fe5dc-f226-4cf2-8042-4c499f7ed519"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0378e1f",
      "metadata": {
        "origin_pos": 53,
        "id": "e0378e1f"
      },
      "source": [
        "Note that while this procedure\n",
        "detaches `y`'s ancestors\n",
        "from the graph leading to `z`,\n",
        "the computational graph leading to `y`\n",
        "persists and thus we can calculate\n",
        "the gradient of `y` with respect to `x`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8c674b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.206880Z",
          "iopub.status.busy": "2023-08-18T19:26:10.206001Z",
          "iopub.status.idle": "2023-08-18T19:26:10.213592Z",
          "shell.execute_reply": "2023-08-18T19:26:10.212476Z"
        },
        "origin_pos": 55,
        "tab": [
          "pytorch"
        ],
        "id": "cb8c674b",
        "outputId": "772ee87b-f492-48b8-e329-e0ef9f98ca18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f056ce",
      "metadata": {
        "origin_pos": 58,
        "id": "76f056ce"
      },
      "source": [
        "## Gradients and Python Control Flow\n",
        "\n",
        "So far we reviewed cases where the path from input to output\n",
        "was well defined via a function such as `z = x * x * x`.\n",
        "Programming offers us a lot more freedom in how we compute results.\n",
        "For instance, we can make them depend on auxiliary variables\n",
        "or condition choices on intermediate results.\n",
        "One benefit of using automatic differentiation\n",
        "is that [**even if**] building the computational graph of\n",
        "(**a function required passing through a maze of Python control flow**)\n",
        "(e.g., conditionals, loops, and arbitrary function calls),\n",
        "(**we can still calculate the gradient of the resulting variable.**)\n",
        "To illustrate this, consider the following code snippet where\n",
        "the number of iterations of the `while` loop\n",
        "and the evaluation of the `if` statement\n",
        "both depend on the value of the input `a`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83327c2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.218214Z",
          "iopub.status.busy": "2023-08-18T19:26:10.217554Z",
          "iopub.status.idle": "2023-08-18T19:26:10.222956Z",
          "shell.execute_reply": "2023-08-18T19:26:10.221858Z"
        },
        "origin_pos": 60,
        "tab": [
          "pytorch"
        ],
        "id": "a83327c2"
      },
      "outputs": [],
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189f6785",
      "metadata": {
        "origin_pos": 63,
        "id": "189f6785"
      },
      "source": [
        "Below, we call this function, passing in a random value, as input.\n",
        "Since the input is a random variable,\n",
        "we do not know what form\n",
        "the computational graph will take.\n",
        "However, whenever we execute `f(a)`\n",
        "on a specific input, we realize\n",
        "a specific computational graph\n",
        "and can subsequently run `backward`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ef0264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.227364Z",
          "iopub.status.busy": "2023-08-18T19:26:10.226919Z",
          "iopub.status.idle": "2023-08-18T19:26:10.232880Z",
          "shell.execute_reply": "2023-08-18T19:26:10.231773Z"
        },
        "origin_pos": 65,
        "tab": [
          "pytorch"
        ],
        "id": "c5ef0264"
      },
      "outputs": [],
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51065133",
      "metadata": {
        "origin_pos": 68,
        "id": "51065133"
      },
      "source": [
        "Even though our function `f` is, for demonstration purposes, a bit contrived,\n",
        "its dependence on the input is quite simple:\n",
        "it is a *linear* function of `a`\n",
        "with piecewise defined scale.\n",
        "As such, `f(a) / a` is a vector of constant entries\n",
        "and, moreover, `f(a) / a` needs to match\n",
        "the gradient of `f(a)` with respect to `a`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab14ef91",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:26:10.237298Z",
          "iopub.status.busy": "2023-08-18T19:26:10.236886Z",
          "iopub.status.idle": "2023-08-18T19:26:10.243577Z",
          "shell.execute_reply": "2023-08-18T19:26:10.242480Z"
        },
        "origin_pos": 70,
        "tab": [
          "pytorch"
        ],
        "id": "ab14ef91",
        "outputId": "4d85cfd2-c7b3-460e-bf7e-76fdf88e0f50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.grad == d / a"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a992f28c",
      "metadata": {
        "origin_pos": 73,
        "id": "a992f28c"
      },
      "source": [
        "Dynamic control flow is very common in deep learning.\n",
        "For instance, when processing text, the computational graph\n",
        "depends on the length of the input.\n",
        "In these cases, automatic differentiation\n",
        "becomes vital for statistical modeling\n",
        "since it is impossible to compute the gradient *a priori*.\n",
        "\n",
        "## Discussion\n",
        "\n",
        "You have now gotten a taste of the power of automatic differentiation.\n",
        "The development of libraries for calculating derivatives\n",
        "both automatically and efficiently\n",
        "has been a massive productivity booster\n",
        "for deep learning practitioners,\n",
        "liberating them so they can focus on less menial.\n",
        "Moreover, autograd lets us design massive models\n",
        "for which pen and paper gradient computations\n",
        "would be prohibitively time consuming.\n",
        "Interestingly, while we use autograd to *optimize* models\n",
        "(in a statistical sense)\n",
        "the *optimization* of autograd libraries themselves\n",
        "(in a computational sense)\n",
        "is a rich subject\n",
        "of vital interest to framework designers.\n",
        "Here, tools from compilers and graph manipulation\n",
        "are leveraged to compute results\n",
        "in the most expedient and memory-efficient manner.\n",
        "\n",
        "For now, try to remember these basics: (i) attach gradients to those variables with respect to which we desire derivatives; (ii) record the computation of the target value; (iii) execute the backpropagation function; and  (iv) access the resulting gradient.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Why is the second derivative much more expensive to compute than the first derivative?\n",
        "1. After running the function for backpropagation, immediately run it again and see what happens. Investigate.\n",
        "1. In the control flow example where we calculate the derivative of `d` with respect to `a`, what would happen if we changed the variable `a` to a random vector or a matrix? At this point, the result of the calculation `f(a)` is no longer a scalar. What happens to the result? How do we analyze this?\n",
        "1. Let $f(x) = \\sin(x)$. Plot the graph of $f$ and of its derivative $f'$. Do not exploit the fact that $f'(x) = \\cos(x)$ but rather use automatic differentiation to get the result.\n",
        "1. Let $f(x) = ((\\log x^2) \\cdot \\sin x) + x^{-1}$. Write out a dependency graph tracing results from $x$ to $f(x)$.\n",
        "1. Use the chain rule to compute the derivative $\\frac{df}{dx}$ of the aforementioned function, placing each term on the dependency graph that you constructed previously.\n",
        "1. Given the graph and the intermediate derivative results, you have a number of options when computing the gradient. Evaluate the result once starting from $x$ to $f$ and once from $f$ tracing back to $x$. The path from $x$ to $f$ is commonly known as *forward differentiation*, whereas the path from $f$ to $x$ is known as backward differentiation.\n",
        "1. When might you want to use forward, and when backward, differentiation? Hint: consider the amount of intermediate data needed, the ability to parallelize steps, and the size of matrices and vectors involved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0ab97d",
      "metadata": {
        "origin_pos": 75,
        "tab": [
          "pytorch"
        ],
        "id": "4c0ab97d"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/35)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
